{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])"},"docs":[{"title":"About the Report","text":"<p></p> <p>Welcome to the online version of a report titled, 'Trustworthy Assurance of Digital Mental Healthcare'. Please use the links below to navigate through the report's chapters, or access information about funding and acknowledgements below.</p> <p>This report was authored and produced by Dr Christopher Burr and Rosamund Powell (Public Policy Programme, The Alan Turing Institute).</p> <p>A PDF version of this report can be accessed here.</p> <p>Illustrations by Jonny Lighthands: www.jonnylighthands.co.uk</p> <p>Graphic Design of the PDF report by Nerea G\u00f3mez: www.nereagomez.com</p>","location":"dmh-report/about/"},{"title":"Table of Contents","text":"<ul> <li> <p> Foreword</p>  <p>A foreword written by Cath Biddle (Mind).</p> <p> Go to chapter</p> </li> <li> <p> Executive Summary</p>  <p>An executive summary covering the main findings and recommendations from the report.</p> <p> Go to chapter</p> </li> <li> <p> Chapter 1\u2014Introduction</p>  <p>Introduction to the report.</p> <p> Go to chapter</p> </li> <li> <p> Chapter 2\u2014Presenting Trustworthy Assurance</p>  <p>This chapter introduces argument-based assurance for readers who may be unfamiliar with the methodology.</p> <p> Go to chapter</p> </li> <li> <p> Chapter 3\u2014Applying Trustworthy Assurance</p>  <p>This chapter covers the research, findings, and recommendations from our sub-project with UK Universities and students.</p> <p> Go to chapter</p> </li> <li> <p> Chapter 4\u2014Co-Designing Trustworthy Assurance</p>  <p>This chapter discusses our engagement with regulators, developers, researchers, and users of digital mental health technology.</p> <p> Go to chapter</p> </li> <li> <p> Chapter 5\u2014Developing Trustworthy Assurance</p>  <p>This chapter presents two argument patterns on the topics of 'fairness' and 'explainability', informed by our workshops.</p> <p> Go to chapter</p> </li> <li> <p> Conclusion</p>  <p>The conclusion to our report with next steps and open questions.</p> <p> Go to chapter</p> </li> <li> <p> Appendix 1: Project Methodology</p>  <p>An appendix outlining our project's methodology.</p> <p> Go to chapter</p> </li> <li> <p> Appendix 2: Examples of DMHTs</p>  <p>A series of illustrative examples of digital mental health technologies.</p> <p> Go to chapter \u2192</p> </li> </ul>","location":"dmh-report/about/#table-of-contents"},{"title":"Additional Information","text":"","location":"dmh-report/about/#additional-information"},{"title":"Funding","text":"<p>Research and production for this report was undertaken at the Alan Turing Institute and supported by funding from the UKRI\u2019s Trustworthy Autonomous Hub, which was awarded to Dr Christopher Burr (Grant number: TAS_PP_00040).</p>  <p></p> <p></p>","location":"dmh-report/about/#funding"},{"title":"Acknowledgements","text":"<p>Talking about mental health can be very difficult. It was important that we enabled and supported participants to share their perspectives and attitudes anonymously during the project\u2019s engagement events, and so we would like to begin by expressing our gratitude to those who took part in our events, or supported specific activities.</p> <p>The research team would like to acknowledge the following groups and individuals:</p> <ul> <li>We are grateful for the candid and thoughtful discussions of the users of digital mental health technologies, who took part in our workshops facilitated by the McPin foundation.</li> <li>The following individuals who generously gave their time to review a draft version of this report (alphabetical order): Cath Biddle, Claudia Corradi, Ibrahim Habli, Zoe Porter, and Mat Rawsthorne. We would also like to thank two anonymous reviewers.</li> <li>Members of the wider project team who offered helpful advice and guidance: Dr Kate Devlin, Professor David Leslie, and Morgan Briggs</li> <li>Our collaborators at University of York's Assuring Autonomy International Programme, including Dr Ibrahim Habli and Dr Zoe Porter who have been instrumental in ongoing discussions about argument-based assurance. And, also our thanks to all participants and the organisers of the UKRI TAS Health and Social Care Workshop for their valuable feedback and queries.</li> <li>Our collaborators at the McPin Foundation, including Dr Dan Robotham and Roya Camvar who were instrumental in facilitating conversations with individuals who have lived experiences relating to the use of digital mental health technologies.</li> <li>Colleagues at the Alan Turing Institute, who have provided direct and indirect input into this research project: Oana Romocea, Eirini Koutsuoroupa, Pauline Kinniburgh, Dr Shyam Krishna, and Dr Michael Katell.</li> <li>With special thanks to all participants in our research engagements throughout this project, including participants with lived experiences of digital mental health technologies, students and administrators at UK universities and, finally, policymakers, researchers and developers working within the sector.</li> </ul>","location":"dmh-report/about/#acknowledgements"},{"title":"Appendix: Project Methodology","text":"<p>This appendix offers additional information about the structure and methodology for our workshops. First, we begin by providing an overview of the three case studies that were used across the workshops. Next, we detail the general methodology for the workshops split across the following four sections:</p> <ol> <li>University administrators</li> <li>University students</li> <li>Regulators, developers, and researchers</li> <li>Users of digital mental health technology</li> </ol>","location":"dmh-report/appendix-1/","tags":["methodology","workshops"]},{"title":"Case Study Information","text":"<p>No one ever said ethics was easy. Moral deliberation requires deep and wide-ranging consideration of issues and challenges such as conflicting values, resource limitations, and the needs and interests of diverse people and groups. One way to facilitate ethical deliberation, therefore, is through the use of illustrative case studies, which hold certain details fixed to support and guide exploratory reflection.</p> <p>We prepared four case studies for this project, all of which were related to hypothetical projects (though based on real-world examples) involving the development or use of a digital mental health technology.</p>  <p>Case Studies</p> <p>Our case studies are free to download from our online repository. They are released under a Creative Commons Attribution-ShareAlike 4.0 International (CC BY-SA 4.0) license. They can be freely shared and adapted subject to the following licensing restrictions. We would also encourage anyone who uses the case studies to provide feedback on where they were helpful or could be improved.</p>","location":"dmh-report/appendix-1/#case-study-information","tags":["methodology","workshops"]},{"title":"Risk assessment and peer-to-peer support","text":"<p>A platform for users to discuss their mental health with others in an anonymous environment, but where a machine-learning algorithm is identifying markers of risk and alerting trained professionals if positive instances are detected.</p> <p></p>","location":"dmh-report/appendix-1/#risk-assessment-and-peer-to-peer-support","tags":["methodology","workshops"]},{"title":"App limits","text":"<p>An automated service on a smartphone that learns to detect signs of problematic usage of specific apps (e.g. gambling, social media) and prevents users from accessing these apps for a specified period.</p> <p></p>","location":"dmh-report/appendix-1/#app-limits","tags":["methodology","workshops"]},{"title":"Clinical decision support system","text":"<p>An AI system that supports a psychiatrist with the assessment and diagnosis of a patient by analysing the patient's speech and making recommendations for follow-up questions or possible follow-up actions.</p> <p></p>","location":"dmh-report/appendix-1/#clinical-decision-support-system","tags":["methodology","workshops"]},{"title":"Virtual reality for therapeutic support","text":"<p>A virtual reality (VR) system that immerses patients in a virtual environment that is designed to expose them to challenging situations in a way that is monitored and controlled (e.g. social encounters for individuals suffering from social anxiety).</p> <p></p> <p>For each case study, the following information was presented:</p> <ul> <li>Overview: a summary of the case study with relevant information about the system and use context</li> <li>Key consideration: a question that was designed to elicit reflection on ethically salient features of the case study</li> <li>Deliberative prompts: additional questions to help structure group discussion</li> <li>Datasheet: a table of information about the data available to the project team (e.g. input data, training data) and the techniques used in the project (e.g. natural language processing, artificial neural network)</li> <li>Affected users, groups, and stakeholders: a list of those who are likely to be impacted by the system or who may impact the system's use and adoption.</li> </ul> <p>For all the case studies, this information was presented as a starting point but the participants were given sufficient flexibility to build upon the case studies where necessary. For instance, if the participants thought that a property of the use context was relevant to their deliberation, but it was not explicitly stated in the document, they were encouraged to include it their discussion.</p> <p>With this preliminary information specified, we now turn to our analysis of the workshops.</p>","location":"dmh-report/appendix-1/#virtual-reality-for-therapeutic-support","tags":["methodology","workshops"]},{"title":"Engagements: Structure and Methodology","text":"<p>Note</p> <p>The following applies to all of our engagements:</p> <ul> <li>An internal and independent ethics review process was followed to evaluate the design of our engagements.</li> <li>Participants were sent an informed consent statement and briefing document prior to their participation. These documents provided information about the purpose and nature of the project, how their data and involvement would support the project, and also specified that all data would be a) securely stored in an anonymised format, and b) deleted following completion of the project.</li> <li>The same four case studies (see above) were presented to the participants for use in the workshop activities.</li> <li>No information about the SAFE-D principles were presented to any of the groups prior to any activities where it was important to have feedback that was not primed. However, for some groups where knowledge of ethics or practical decision-making could not be assumed, a general introduction was provided to support their participation (e.g. users).</li> </ul>  <p>The following sections provide details of these workshop groups:</p> <ol> <li>University administrators</li> <li>University students</li> <li>Regulators and Policy-Makers, Developers, and Researchers</li> <li>Users with lived experience of DMHTs</li> </ol>","location":"dmh-report/appendix-1/#engagements-structure-and-methodology","tags":["methodology","workshops"]},{"title":"University administrators","text":"","location":"dmh-report/appendix-1/#university-administrators","tags":["methodology","workshops"]},{"title":"Recruitment and participant details","text":"<p>Interviews with administrators across 10 UK universities were conducted between January and March 2022, each lasting one hour and facilitated by Dr Christopher Burr and Rosamund Powell. In all instances, interviews were conducted remotely via Zoom due to the geographic diversity of participants. Participants were selected from the top 20 UK universities according to all metrics, based upon the Times Higher Education Survey 2021.1</p> <p>Relevant representatives were identified and invited to interview. Individuals invited to interview worked within Student Services departments with the majority serving as Director of Student Welfare or Wellbeing. Some instead served as Head of Disability, Deputy Director of Wellbeing or Head of Student Services.</p>","location":"dmh-report/appendix-1/#recruitment-and-participant-details","tags":["methodology","workshops"]},{"title":"Interview structure","text":"<p>In advance of interviews, all participants were sent a consent form, a list of questions and briefing document on the digital mental healthcare landscape. At the start of each interview, one of the facilitators began by introducing the project and the project members on the call. Following this, verbal consent was requested, and participants were given the opportunity to ask questions on the informed consent form that had been pre-circulated. The informed consent statement explained that their answers would be fully anonymised to allow the participants to feel comfortable expressing candid opinions and beliefs about potentially sensitive topics.</p> <p>Once consent was obtained, the interview recording began. A semi-structured format was selected for several reasons:</p> <ul> <li>It was suitable for an exploratory stage interview</li> <li>It encourages exploration of tangential issues that participants feel are relevant</li> <li>It is more likely to promote a relaxed interview and hones feedback.</li> </ul> <p>The interviews were split into three main sections. Section 1 focused on attitudes to the advantages and disadvantages of digital mental health technologies and current procurement practices at UK universities. Section 2 focused on how current procurement processes align with duty of care. Section 3 focused on collecting feedback on the ethical assurance methodology.</p> <p>At the end of interviews, the recording was stopped, and the participant was asked if they had any further questions for the interviewers. The participants were thanked for their time and told they would receive a copy of this report.</p>","location":"dmh-report/appendix-1/#interview-structure","tags":["methodology","workshops"]},{"title":"Analysis","text":"<p>To facilitate qualitative analysis, automated transcriptions were verified and corrected by members of the Turing project team before the original recording was deleted. Two project team members then analysed these transcriptions independently to identify salient themes, grouped into two sections:</p> <ol> <li>Contextual challenges to the ethical deployment of digital mental healthcare</li> <li>Administrator feedback on trustworthy assurance</li> </ol> <p>Once themes were identified, themes were discussed and key conclusions were summarised and quote from interviews extracted. This preliminary analysis was then set aside so that student workshops could be completed.</p>","location":"dmh-report/appendix-1/#analysis","tags":["methodology","workshops"]},{"title":"University students","text":"","location":"dmh-report/appendix-1/#university-students","tags":["methodology","workshops"]},{"title":"Recruitment and participant details","text":"<p>Workshops with students from UK universities were conducted between February and March 2022. Two six-hour workshops were completed and facilitated by the project team (see above). In all instances, workshops were conducted remotely via Zoom to support geographic diversity. Participants were selected from across all UK universities.</p> <p>An open call for applications was published by the Turing, inviting any students currently enrolled in an undergraduate or postgraduate course. In order to apply, students completed an application form asking a series of optional EDI questions alongside the below three project questions:</p> <ol> <li>Why are you interested in participating in this workshop?</li> <li>How do you understand the aims of this research in your own words?</li> <li>Do you have any prior understanding about the ethics of digital mental healthcare? If so, please provide details.</li> </ol> <p>Participants were selected from a total of 45 applications and a total of 25 students joined the final workshop sessions.</p>","location":"dmh-report/appendix-1/#recruitment-and-participant-details_1","tags":["methodology","workshops"]},{"title":"Workshop design","text":"<p>In advance of interviews, all participants were sent a consent form and briefing document on the digital mental healthcare landscape. At the start of each workshop, one of the facilitators began by introducing the project and the project members on the call. Following this, verbal consent was requested, and participants were given the opportunity to ask questions on the informed consent form that had been pre-circulated. The informed consent statement explained that their answers would be fully anonymised, in order to allow the participants to feel comfortable expressing their opinions and beliefs about potentially sensitive topics.</p> <p>Once consent was obtained, the recording began.</p> <p>The workshops were split into two sections. Section 1 focused on attitudes to the advantages and disadvantages of digital mental health technologies and on identifying which values and principles mattered to students. Section 2 then saw students evaluate two illustrative case studies (see above) designed to identify possible ethical issues which might arise if they were deployed in a university context.</p> <p>During the workshop sessions, participants were also asked to complete an online survey seeking individual feedback on the ethics of digital mental healthcare in a university context. At the end of workshops, the recording was stopped, and the participants were asked if they had any further questions for the facilitators. The participants were thanked for their time and told they would receive a copy of this report.</p>","location":"dmh-report/appendix-1/#workshop-design","tags":["methodology","workshops"]},{"title":"Analysis","text":"<p>To facilitate qualitative analysis, automated transcriptions were verified and corrected by members of the Turing project team before the original recording was deleted. This transcription was also accompanied by notes taken by members of the Turing team during the workshops to help identify key themes. In addition, the completed surveys were analysed. Finally, the notes taken online during the workshop activities were analysed as participants had been encouraged to take structured notes on the case studies during the workshop sessions.</p> <p>Each of these elements were taken into account in identifying key themes. Once themes were identified, key conclusions were summarised and quotes from workshops extracted. This preliminary analysis was then set aside, to be combined with pre-existing analysis form administrator interviews.</p>","location":"dmh-report/appendix-1/#analysis_1","tags":["methodology","workshops"]},{"title":"Final Analysis","text":"<p>For students and administrators, key themes which fell into the category of \u201ccontextual challenges to the deployment of digital mental health technologies\u201d were compared to identify cross-cutting themes. During analysis, researchers were careful to maintain differentiation between administrator and student perspectives such that agreements and disagreements could be identified. For all of the six themes identified (see Chapter 3), student and administrator feedback was relevant and so all themes were cross-cutting. However, in many cases, students and administrators contributed to the development of these themes in contrasting ways.</p> <p>In addition to contextual challenges, the final report identifies a series of methodological challenges. These are largely drawn from administrator interviews where more time was dedicated to the evaluation of trustworthy assurance. Nevertheless, where relevant, student perspectives are used to supplement this analysis.</p>","location":"dmh-report/appendix-1/#final-analysis","tags":["methodology","workshops"]},{"title":"Regulators and Policy-makers, Developers, and Researchers","text":"","location":"dmh-report/appendix-1/#regulators-and-policy-makers-developers-and-researchers","tags":["methodology","workshops"]},{"title":"Recruitment and participant details","text":"<p>Unlike the previous workshops, 39 participants from these three groups were sent specific invitations based on their roles and responsibilities (30 attended). While this introduces a source of selection bias, it also allowed the project team to utilise existing relationships to ensure senior decision-makers and researchers from across the UK government and civil service, third-sector, development community, and academia were included.</p> <p>A decision was made early in the project to bring these different stakeholder groups together, rather than holding separate workshops for each group. This was to allow the participants to have an opportunity to discuss and identify shared values and differences in perspectives and approaches out in the open. This limited the amount of comparative analysis we could perform, but a separate survey was undertaken to explore these differences specifically. However, due to the limited sample size we accepted that there would be a limit to how much we could generalise from our findings.</p>","location":"dmh-report/appendix-1/#recruitment-and-participant-details_2","tags":["methodology","workshops"]},{"title":"Workshop design","text":"<p>Two workshops were held with the participants in May 2022. The first workshop focused on broader ethical, social, and legal issues, and the second focused on the Trustworthy Assurance methodology. Because of the longer time dedicated to these groups, several presentations were given by the project team, including two on the Trustworthy Assurance methodology, which was a key focus for the groups. The first workshop was split into three sections. Section 1 focused on exploring the values and principles that were important to the participants. Section 2 identified and explored salient issues and concepts in the four case studies. And, section 3 introduced participants to the trustworthy assurance methodology.</p> <p>The second workshop provided a refresher to the trustworthy assurance methodology and then involved the design and production of a hypothetical assurance case for one of the four case studies, which the participants had voted for in the previous workshop. The participants were used our online platform to put the assurance cases together, focusing on a key ethical goal that they had selected from a set of options that they had put together.</p>","location":"dmh-report/appendix-1/#workshop-design_1","tags":["methodology","workshops"]},{"title":"Analysis","text":"<p>To facilitate qualitative and thematic analysis, automated transcriptions were verified and corrected by members of the project team before the original recording was deleted. This transcription was also accompanied by notes taken by members of the project team during the workshops to help identify key themes. In addition, the completed surveys were analysed and unedited results are provided in Chapter 4.</p> <p>Each of these elements were taken into account in identifying key themes, with a specific emphasis on their relation to the Trustworthy Assurance methodology. Therefore, unlike the research conducted with UK Universities, for this work we were looking to identify themes that could support the operationalisation of ethical principles as key components in an assurance case.</p>","location":"dmh-report/appendix-1/#analysis_2","tags":["methodology","workshops"]},{"title":"Users of digital mental health technology","text":"","location":"dmh-report/appendix-1/#users-of-digital-mental-health-technology","tags":["methodology","workshops"]},{"title":"Recruitment and participant details","text":"<p>Based on feedback from our independent and internal ethics review process, we decided to carry out these workshops with the support of the McPin foundation\u2014a mental health research charity that provide advice and support on research strategies to involve participation and expertise from individuals with lived experience of mental health issues.</p> <p>Representatives of McPin handled recruitment, such that all contact with the individuals prior to the workshops and after the workshops were organised by a single organisation. Participants were required to be over 18 years old and to have either a) direct lived experience of using DMHTs or b) direct experience of caring for someone who made use of DMHTs. No other constraints were set, and no information about the specific reason for use was requested (e.g. CBT app for anxiety).</p> <p>Two workshops were offered to participants and both were held during July 2022\u2014one in person at the Alan Turing Institute and one online using Zoom. For the in-person workshop, 10 participants were selected from around Greater London due to the need to travel to the Alan Turing Institute\u2014travel expenses were reimbursed. For the online workshop, 10 participants were selected from across the UK. All participants were reimbursed for their time.</p>","location":"dmh-report/appendix-1/#recruitment-and-participant-details_3","tags":["methodology","workshops"]},{"title":"Workshop design","text":"<p>All workshops were facilitated by two members of staff from the McPin foundation. The project team supported the McPin facilitators and also gave two presentations\u2014our slides can be downloaded here. However, in these workshops the project team took a less hands-on role than the other engagements.</p> <p>The workshops with users focused on the moral attitudes and perspectives of the participants and only indirectly explored trustworthy assurance. While they were informed about the trustworthy assurance methodology, so they could understand why they were carrying out the respective activities, it was not emphasised in the workshops. This choice was influenced a) by the feedback from the students gathered during the sub-project (see Chapter 2), and b) preliminary discussions with our workshop facilitators\u2014McPin\u2014about the accessibility of the material within the time constraints.</p> <p>Two activities were designed for these workshops (using breakout sessions to allow wider contribution from the participants):</p> <ul> <li>The first activity was a guided discussion of the following three questions:</li> <li>What is Digital Mental Healthcare?</li> <li>What are some positive use cases for digital mental health technologies?</li> <li>What are some negative use cases for digital mental health technologies?</li> <li>The second activity involved a structured task where the participants evaluated several statements, which were designed as possible assurance claims from one of the hypothetical developers in our case studies (download list of claims and questions).</li> </ul>","location":"dmh-report/appendix-1/#workshop-design_2","tags":["methodology","workshops"]},{"title":"Analysis","text":"<p>To facilitate qualitative and thematic analysis, minutes and notes were provided by the McPin Foundation facilitators and supplemented by notes taken by the Turing project team during the meeting. No recordings were taken or transcribed to help create a more comfortable environment for the participants. The Turing team used the minutes, notes, and the original documents from the workshop activities to identify general themes see Chapter 4 and extract the core attributes that were used to operationalise the ethical goals associated with the argument patterns presented in Chapter 5</p>   <ol> <li> <p>Desk research was used to determine different methods for filtering universities. For example, based upon the best and worst student satisfaction. In the end, the top 20 Universities, according to Times HE Survey, were selected because of the broad range of digital offerings available at these institutions. Admittedly, this decision introduced some selection bias into our engagements, which is why we present our findings as limited exploratory analysis and recommend further themes for ongoing research and analysis.\u00a0\u21a9</p> </li> </ol>","location":"dmh-report/appendix-1/#analysis_3","tags":["methodology","workshops"]},{"title":"Appendix 2: Examples of Digital Mental Health Technologies","text":"<p></p>  <p>Work in Progress</p> <p>This page is a work-in-progress. Additional examples will be added as they are produced, but the goal is not to develop a comprehensive list nor carry out a systematic review of the examples according to a specific methodology. Rather, it is to provide an illustrative set of examples for those who are new to this field.</p>","location":"dmh-report/appendix-2/","tags":["examples","digital-mental-health-technologies"]},{"title":"Smartphone Apps","text":"<p>There are countless smartphone apps available to consumers. Some are free to download, others require a one-off purchase, and others require ongoing subscriptions. In some instances, employers or other organisations (e.g. Universities) may provide access to individuals through group subscriptions or via formal healthcare services. As a category, <code>smartphone app</code> does not provide any indication of the type of functionality of the underlying services. Apps can range from simple meditation apps that could be beneficial for a wide-range of users (e.g. dealing with stress, anxiety, or productivity) to AI chatbots that target users with specific mental health issues. </p> <p>A prominent concern in this area is the lack of enforceable regulation that prevents developers from exploiting social media advertising networks to target users with mental health issues (e.g. depression, anxiety) while skirting around medical device regulation by referring to their service as a \"well-being tool\".</p> <p>Furthermore, as a recent study by Mozilla highlights, the quality of data privacy and protection practices varies widely, with some developers being very transparent about their data collection, analysis, and storage, while others skirt the boundaries of what is legal.</p>  <p>Privacy Not Included</p> <p>The 'Privacy Not Included' study, carried out by Mozilla, is a consumer-facing guide on the apps, services, and internet-connected devices that do and do not respect user rights to data privacy and protection. They have a whole section on mental health apps, which we highly recommend exploring. </p> <p>Their methodology is also available here: https://foundation.mozilla.org/en/privacynotincluded/about/methodology/</p>  <p>A recent standard produced by the International Standards Organisation (ISO) demonstrates one means for bringing more structure to health and wellness apps through the standardisation of reporting labels (see Figure 1)1. Labels such as these could represent a significant type of evidence to ground assurance cases, subject to the formation of consensus around such best practices.</p> <p> Figure 1. A label showing dimensions of quality and reliabiluty for health and wellness apps. (PD CEN ISO/TS 82304-2:2021)</p>  <ul> <li> <p> Wysa Chatbot</p>  <p>According to the develeoper's website, \"Wysa AI Coach is an artificial intelligence-based 'emotionally intelligent' service which responds to the emotions you express and uses evidence-based cognitive-behavioral techniques (CBT), DBT, meditation, breathing, yoga, motivational interviewing and micro-actions to help you build mental resilience skills and feel better.\"</p> <p>\u2705 ML/AI: this app is explicit in its use of AI for enabling automated conversations with users.</p> <p> A screenshot of the Wysa AI coach app. Reprinted from the Wysa Press Kit.</p> <p> Visit Developer's Site</p> </li> <li> <p> Headspace</p>  <p>Headspace\u2019s core products are mobile and web-based apps that focus on mindfulness, meditation, sleep, exercise, and focus content (e.g. guided exercises). The company publishes research to support its claims about efficacy and outcomes.</p> <p>\u2705 ML/AI: the use of ML or AI is not a prominent feature in the app, but the organisation does make use of ML to tailor recommendations to users, and has published plans to make use of additional data.</p> <p> A screenshot of the Headspace app. Reprinted from the Headspace Press Kit.</p> <p> Visit Developer's Site</p> </li> </ul>","location":"dmh-report/appendix-2/#smartphone-apps","tags":["examples","digital-mental-health-technologies"]},{"title":"Virtual or Augmented Reality (VR/AR)","text":"<p>The use of VR/AR technologies to support mental health are primarily used in research projects at present, while the efficiacy of such tools is investigated. A wide variety of research projects are exploring how these DMHTs could be used to support mental health outcomes related to psychosis, social anxiety, and chronic pain, among others.</p>  <ul> <li> <p> Virtual Body and Anxiety Reduction</p>  <p>A research team at the Department of Psychiatry (University of Oxford) have explored how the observation of a virtual body double engaged in social interaction may reduced persecutory thoughts associated with anxiety. The study demonstrates limited but positive evidence for a decreased anxiety.</p> <p>\u2705 ML/AI: automated techniques are used to control the behaviours of the artificial agents that the virtual body double interacts with, although pre-recorded dialogue is also used.</p> <p> An image of the body doubles in VR space. Reprinted from Gorisse et al. (2021) </p> <p> Read Publication</p> </li> <li> <p> VR and Chronic Pain</p>  <p>Although broader than mental health, chronic pain has stong bi-directional associations with mental health issues, such as depression. Many studies have explored whether VR can help reduce the experience of chronic pain. For instance, allowing users to explore movements in a virtual space that would typically cause an experience of pain were they carried out for real. A recent scoping review of 44 studies concludes that VR can have short-term analgesic effects in acute pain settings, but that long-term efficacy for chronic pain conditions has not been established. </p> <p>\u274c ML/AI: not applicable</p> <p> Read Publication</p> </li> </ul>","location":"dmh-report/appendix-2/#virtual-or-augmented-reality-vrar","tags":["examples","digital-mental-health-technologies"]},{"title":"Decision Support Tools","text":"<p>By 'decision support tools' we mean DMHTs that are designed to be used to augment or support the decision-making of healthcare professionals.</p>  <ul> <li> <p> IESO Digital Health</p>  <p>IESO Digital Health is one of the leading providers of online mental health services for the NHS, as part of the Improving Access to Psychological Therapies (IAPT) programme. The company have developed an AI-powered decision support tool, which \"automatically annotates therapist utterances in real-time\" to support CBT. The tool can be used for several purposes, including \"to monitor the delivery of crucial elements in live therapy\" and to remind the therapist \"to set an agenda or to set homework when they have not done so.\"</p> <p>\u2705 ML/AI: the tool uses a deep learning method known as bidirectional long short-term memory.</p> <p> An image of an example therapy session with automated labels applied in real time. Reprinted from Cummins et al. (2019)</p> <p> Visit Developer's Site</p> </li> </ul>","location":"dmh-report/appendix-2/#decision-support-tools","tags":["examples","digital-mental-health-technologies"]},{"title":"Neurotechnologies","text":"<p>Neurotechnologies are are another class of technologies that are broader than mental health specifically, and also include wearables such as headbands as well as  Brain-Computer Interfaces (BCIs). They have received widespread attention from the bioethics community.2</p>  <ul> <li> <p> EEG Headbands</p>  <p>The FocusCalm is an EEG headband by BrainCo\u2014a company that grew out of the Harvard Innovation Lab. The device is currently marketed as a DMHT to measure brain activity and offer real-time feedback to focus and relax your mind. However, an earlier version of the product was tested as a tool for improving the attention of Chinese school students during lessons, before it was halted following complaints from parents.</p> <p>\u2705 ML/AI: the company claims to use ML algorithms to help interpret EEG frequency bands.</p> <p> An image of a Chinese pupil wearing an EEG headband.</p> <p> Visit Developer's Site</p> </li> </ul>    <ol> <li> <p>Reprinted from PD CEN ISO/TS 82304-2:2021, Health Software\u2014 Health and wellness apps. Quality and reliability. Permission to reproduce extracts from British Standards is granted by BSI Standards Limited (BSI). No other use of this material is permitted.  British Standards can be obtained from BSI Knowledge knowledge.bsigroup.com\u00a0\u21a9</p> </li> <li> <p>For example, see these resources and articles from Nuffield Council on Bioethics: https://www.nuffieldbioethics.org/topics/data-and-technology/neurotechnology \u21a9</p> </li> </ol>","location":"dmh-report/appendix-2/#neurotechnologies","tags":["examples","digital-mental-health-technologies"]},{"title":"Introduction","text":"<p></p>  <p>About this Chapter</p> <p>This introductory chapter provides contextual information for the report. However, it can be treated as an optional chapter for those readers who only want to engage with the trustworthy assurance methodology (Chapter 2), findings and analysis from our workshops (Chapters 3 and 4), or positive proposals (Chapter 5).</p>   <p>Do I contradict myself? Very well then I contradict myself, (I am large, I contain multitudes.) --Walt Whitman, Song of Myself</p>  <p>Whitman's ode to self-knowledge and understanding contains many poetically-phrased truths. However, the one expressed in the above line is an understatement. If we were to identify and rank the most complex phenomena in the universe, our large and multitudinous minds would sit somewhere near the top of the list!</p> <p>Even the most stubborn among us must acknowledge that part of this complexity stems from a capacity for our minds to operate as a network of often contradictory beliefs, attitudes, and opinions\u2014a network that exists within and among a larger social network of similarly fallible individuals. It would be understandable, therefore, given this reflection, if we came to the conclusion that our minds were never supposed to be understood fully and we just accepted, as Whitman did, that our mental lives are fundamentally contradictory and diverse, and sometimes none the worse for it.</p> <p>For many people, a prescription of stoic acceptance in the face of overwhelming complexity would be welcomed. But for others, their minds are not just built on top of permissible and tolerable contradictions, they also operate in a manner that prevents them from living a fully self-determined and flourishing life.</p> <p>In the last decade or so, a wide range of digital and data-driven technologies have emerged that promise to improve both our knowledge and understanding of our complex minds and its capabilities, as well as enhance our overall well-being. This fact is unsurprising. Our species has used technology to learn about and restructure both our external and internal worlds for hundreds of thousands of years. And during our time on this planet, technology has both enhanced and diminished our knowledge, understanding, and individual and social welfare. So, why has so much attention been paid in recent years to a recurring cycle of technological innovation?</p>","location":"dmh-report/chapter-1/","tags":["culture-of-distrust","landscape","digital-mental-healthcare"]},{"title":"Laying the Foundations","text":"<p>Towards the start of the 21st Century, a convergence of several social and technical factors gave rise, first, to an interest in the big data revolution, and, second, to a renewed interest in Machine Learning (ML) and Artificial Intelligence (AI). Let's look at each of these briefly, as they help establish important and explanatory context for this report.</p> <p>The big data revolution occurred as a result of increased and widespread use of Internet of Things (IoT) or mobile devices (i.e. the sources of data); availability of affordable cloud computing infrastructure (i.e. for extracting, loading, and transforming the data); and ongoing development of open-source frameworks and software libraries for more efficient and distributed data storage and analysis (e.g. Apache Hadoop, Python), among other factors.</p> <p>To help differentiate big data from ordinary data collection, analysis, and use, many have pointed to the five V's of big data:</p> <ul> <li>Volume: the amount of data being extracted</li> <li>Variety: the types of data being extracted</li> <li>Velocity: the speed at which data is extracted</li> <li>Value: the socioeconomic benefit of data</li> <li>Veracity: the accuracy of data</li> </ul> <p>In the context of digital mental healthcare, all of these are noteworthy, but three stand out against the backdrop of this report's opening remarks regarding the complex phenomena of interest (our minds):</p> <ol> <li>How accurate are the data we are now collecting, analysing, and using?</li> <li>Given the variety of minds that populate this plant, how representative are the types of data?</li> <li>How much of a gap is there between the socioeconomic value of data and the value to the individual who is represented by the data? Or, to put it more bluntly, who benefits from the data?</li> </ol> <p>We (the authors) have heard and discussed many variants of these questions over the course of this project. For example, concerns about accuracy and variety are deeply connected to considerations around the regulatory assessment of clinical efficacy and safety for novel data-driven medical devices.1 But more than this, accuracy and variety also underpin broader ethical concerns about existing barriers to enabling a fairer and more accessible healthcare system (e.g. the digital divide that systematically excludes certain people and groups from benefits associated with digital technologies). However, although data are important, addressing these questions is just one part of the puzzle.</p> <p>Turning to the technologies themselves\u2014another significant piece\u2014we can similarly identify several explanatory factors behind the recent surge of interest in machine learning algorithms (ML) and artificial intelligence (AI). Developments in this domain build on top of the aforementioned factors behind the big data revolution\u2014ML and AI are, after all, sometimes referred to as 'data-driven technologies':</p> <ul> <li>Theoretical advances in machine learning for robotics and intelligent software agents (e.g. DeepMind's Alpha Go)</li> <li>Improved application of deep neural networks to well-defined tasks such as medical imaging or speech detection</li> <li>Hardware improvements in specialised computer processor architectures to allow for more efficient and effective edge computing</li> </ul> <p>All of these developments are important, but again there are three aspects that stand out as significant:</p> <ol> <li>The ability for ML/AI systems to operate autonomously</li> <li>The ability for ML/AI systems to learn from their environments</li> <li>The ability for ML/AI systems to adapt to and affect their environments</li> </ol> <p>As we will see throughout this report, these features of ML algorithms and AI systems create possible risks and benefits to the realisation of ethical goals associated with digital mental healthcare. For instance, the ability to respect a patient's right to autonomous decision-making. Furthermore, these issues intersect with the issues raised by the previous three questions pertaining to data (e.g. the ability to operate autonomously in complex environments with insufficiently accurate data).</p> <p>These topics already paint a very complex picture, but there is also a further level of complexity involved with understanding the dynamic feedback loops that emerge in mental healthcare when autonomous and adaptive systems are used to complement existing therapeutic interventions, many of which are already poorly understood (e.g. Selective Serotonin Reuptake Inhibitors). This complexity can cause issues for our existing research, development, and regulatory frameworks, such as when performing clinical trials (e.g. how should we control for the effects of adaptive and personalised technologies?).</p> <p>Collectively, these six points about Big Data and ML/AI help to establish the background and context for this report, and also help us gain some conceptual clarity when attempting to address the uncertainty around trustworthy DMHTs. Some of this uncertainty stems from the technologies themselves (as noted above). But other key aspects of this uncertainty arise because a) the concept \u2018trustworthy digital mental health technology\u2019 is a poorly defined term2 that captures a vast and heterogeneous class of tools and services, and b) our relationships to and interactions with the technologies are also varied. It is not just the technologies that are complex after all. As eloquently captured by Whitman at the start, we\u2014the individual members of the class, 'humanity'\u2014are large and contain multitudes.</p> <p>These many layers of complexity coalesce into a particularly thorny problem. If we cannot trust the technologies themselves, we will not be able to trust the information gained from them about our own minds. But if we do not understand our minds, we may be unable to fully determine and address the cause of our trust or distrust. And, as we have just noted, understanding the social environment is also a vital part of addressing this fundamentally sociotechnical problem. As you can probably guess by know, adding another element to our picture is not likely to reduce its complexity.</p>","location":"dmh-report/chapter-1/#laying-the-foundations","tags":["culture-of-distrust","landscape","digital-mental-healthcare"]},{"title":"The Current (Socioeconomic) Landscape of Digital Mental Healthcare","text":"<p>To explore and understand the socioeconomic landscape of digital mental healthcare, let us start by addressing two outstanding questions:</p> <ol> <li>What is meant by 'digital mental healthcare technology'?</li> <li>Why and how are 'digital mental healthcare technologies' being used?</li> </ol>","location":"dmh-report/chapter-1/#the-current-socioeconomic-landscape-of-digital-mental-healthcare","tags":["culture-of-distrust","landscape","digital-mental-healthcare"]},{"title":"What are digital mental health technologies?","text":"<p>This is not an easy question to answer because of the multifaceted ways that the term 'digital mental health technology' could be employed.</p> <p>We can attempt to answer the question by drawing distinctions between, say, the use of digital technologies within formal healthcare settings (e.g. NHS) and those outside. However, as we have explored in previous research3, this boundary is vaguely drawn at best, and unhelpful at worst, when it comes to understanding DMHTs and the ethical issues surrounding their design and use.</p> <p>One reason for this is that outside of formal healthcare systems, DMHTs have been employed in social domains and contexts as diverse as financial services, education (e.g. schools and universities), and employment. And, furthermore, DMHTs have been used within such domains for myriad purposes ranging from vulnerability assessment through to proactive intervention3. There is also the use of DMHTs by social media platforms and charities to consider, which tends to cross many of these boundaries, especially those between work and home life, making it difficult to draw useful distinctions unless a narrow focus is defined in advance (e.g. studying the use of NLP used for risk assessment of adolescents on social media)4.</p> <p>What about focusing on the technologies themselves? Again, this is not an easy feat. Some organisations, such as the Nuffield Council on Bioethics have focused on emerging technologies to narrow their scope 5. In doing so, they have identified specific ethical challenges associated with the following class of technologies:</p> <ul> <li>Smartphone apps and chatbots</li> <li>Predictive analytics (e.g. based on digital phenotyping)</li> <li>Consumer neurotechnology (e.g. portable electroencephalography devices)</li> <li>Immersive technology (e.g. virtual reality)</li> </ul> <p>But none of these categories are suitable for building a definition of DMHTs writ large. For instance, let's look at so-called \"digital phenotyping\"\u2014an increasingly popular area of digital mental healthcare\u2014, defined as follows:</p>  <p>We also introduce the term \u201cdigital phenotyping\u201d to refer to the \u201cmoment-by-moment quantification of the individual-level human phenotype in-situ using data from smartphones and other personal digital devices.\u201d6</p>  <p>Putting aside the previously raised data challenges associated with the \"quantification of the individual-level human phenotype\" (e.g. how to construct accurate scales that apply to all varied people while retaining value), there is also wide variation between 'smartphones' and 'other personal digital devices', such as wearables. For instance, some smartphone apps may use advanced forms of machine learning algorithms or AI to infer novel attributes regarding a user's mental health. And, others may offer nothing more than a simple interface and database for users to record how they are feeling at a particular time or on a particular day. Furthermore, some wearables may store and process sensitive information locally on a user's device, whereas others may store data on the cloud and share health-related information with a vast number of organisations across jurisdictions with varying levels of data protection.</p> <p>What about if we move to a lower level of abstraction, such as the algorithmic technique used by the digital technology? Here too we would find difficulties with delineating the meaning of the term \u2018digital mental health technology\u2019. For example, the use of unsupervised machine learning by trusted clinical researchers may be justified if used responsibly as a form of exploratory research or hypothesis generation. But, if a complex version of the technique were deployed by a local council to determine how best to spend limited resources (e.g. resulting in clusters that were not clearly interpretable by humans), the potential lack of transparency could undermine efforts to remain accountable to their residents.</p> <p>Regardless of the level of abstraction we adopt, there will always be some difficulty with clearly defining this nebulous term. Therefore, while it may seem unsatisfying to a reader who wishes to know precisely what the term 'digital mental health technology' comprises, for our present purposes the following (loose and permissive) definition shall suffice:</p>  <p>The term 'digital mental health technology' refers to any digital technology that has been designed, developed, and deployed with the goal of improving or otherwise impacting some mental health outcome for an individual or group of people.</p>  <p>In particular, this report will pay close attention to those technologies that are data-driven and/or use some form of machine learning or AI, given the considerations outlined in the opening sections.</p> <p>We acknowledge that many will find this definition too permissive, but this report is not concerned with developing a robust philosophical definition or a taxonomy that can be used to delineate the precise nature of DMHTs. Rather, it is focused on the defence of a methodology to help make DMHTs more trustworthy and ethical. Therefore, the broader the class that can be drawn the better it will be for our goals, because more technologies will fall within its scope7. And, insofar as there are legal considerations that demand precise definitions, these issues will be addressed along different lines (e.g. operationalising standards of assessment for equitable treatment).</p>  <p>Examples</p> <p>A set of illustrative examples of DMHTs can be found on this page.</p>  <p>Let us now turn to the second question.</p>","location":"dmh-report/chapter-1/#what-are-digital-mental-health-technologies","tags":["culture-of-distrust","landscape","digital-mental-healthcare"]},{"title":"Why and how are digital mental healthcare technologies being used?","text":"<p>The following statistics offer a partial and fragmented perspective to help frame this question, focusing on the UK specifically:</p> <ul> <li>Over 60% of children and young people with diagnosed mental health conditions do not receive NHS care.8</li> <li>Rates of probable mental health disorders in children and young people (aged 6 to 16 years) have risen from 11.6% in 2017 to 17.4% in 2021.9</li> <li>Approximately two-thirds of people who die by suicide are not in contact with NHS mental health services.10</li> <li>In the first 3 months of 2021, 1 in 5 adults in Britain experienced some form of depression (over double the pre-pandemic figures).11</li> <li>During the pandemic both males and females saw an increase in anxiety and a reduction in 'life satisfaction'\u2014a subjective measure of well-being that asks individuals to evaluate their life as a whole, rather than time-specific emotions. However, females experienced lower life satisfaction and happiness than males.11</li> </ul> <p>When we consider these figures, combined with a reflection of the impact wrought by the COVID-19 pandemic on an already over-burdened mental health sector, we can begin to understand why many organisations across the public, private, and third sectors are deploying digital technologies to augment and complement their services, and why many users in turn have engaged.</p> <p>But if we are to implement digital technologies in an ethical and trustworthy manner, there are several considerations that need to be addressed. </p> <p>The first two relate to choice and access, as outlined in a briefing note from the Nuffield Council on Bioethics5 (emphasis ours):</p> <ol> <li>\"Many people affected by mental health problems do not have access to or are reluctant to use mental healthcare technologies. If these are to become widely adopted in the future, there should be choice about using them.\"</li> <li>\"Technology solutions should not divert resources from other important forms of mental healthcare and support and should be used as an addition to what is already available, rather than a replacement.\"</li> </ol> <p>The latter conclusion is echoed here because it is a theme that emerged frequently in our own project among diverse stakeholder groups. That is, DMHTs should augment and support, but never replace human decision-making or human-centred services. And, the former conclusion is also important because it captures something salient about trust. </p> <p>For some potential users, such as elderly patients, a lack of access can be due to their needs not being sufficiently considered when designing the service or technological interface.12 This form of inaccessibility is sometimes overlooked due to an emphasis on other economic barriers (e.g. digital poverty). However, even users that a) have access to the services (in both senses of the term \u2018access), and b) potentially benefit from use of the respective technology, may still have legitimate reasons for not wishing to use the service due to a distrust (or, \u201creluctance\u201d to use the same term from the above quotes) in the service or the organisation responsible for designing, developing, and deploying it. In some cases, this distrust arises due to legitimate concerns about violations of data privacy or mishandling of sensitive information by commercial organisations.</p> <p>However, the unethical behaviour and transgressions of law by commercial organisations such as Facebook13 can have a wider impact beyond their own disastrous public relations. They can also contribute to a growing culture of distrust in the ecosystem more broadly, affecting the public and third sectors, as members of the public may be unable to separate the differing ethical, social, or legal norms that govern each sector or domain.4 This is understandable from the perspective of the user, as the norms that regulate and govern the public, private, and third sectors are complex and deeply interwoven. But it is still characteristic of an unethical and irresponsible approach to research and innovation, and one that is unlikely to build trust.</p>","location":"dmh-report/chapter-1/#why-and-how-are-digital-mental-healthcare-technologies-being-used","tags":["culture-of-distrust","landscape","digital-mental-healthcare"]},{"title":"A Culture of Distrust","text":"<p>In the context of the law, it is well known that states and public sector organisations are beholden to wide-ranging legal duties, both positive and negative, such as those set out in human rights law or in national legislation (e.g. the public sector equality duty created by the UK's Equality Act 2010).</p> <p>Commercial organisations are not obligated to observe all of the same principles or rules as public sector organisations, but are nevertheless required to comply with myriad information governance standards, legislation designed to protect environmental sustainability and public health, and a whole host of other corporate or fiduciary duties14.</p> <p>Third sector organisations, such as charities or volunteer groups, may have less restrictive legislation governing their conduct, but are still expected to adhere to necessary transparency and accountability standards over matters such as the organisation and incorporation of managing trusts.</p> <p>Such legal requirements create an interlocking foundation upon which public perceptions and attitudes towards trust can be based, but are often difficult to separate and pick apart. And, even where one is able to do so, legal requirements typically set only the minimal standards expected of organisations. To put it simply, and sidestep a vast amount of important jurisprudence, just because something is legal does not guarantee it is ethical or socially acceptable.</p> <p>On top of the norms that fall within the scope of the law, modern institutions and organisations are also expected to observe and comply with an expansive and shifting set of ethical and social norms. For example, while underpinned by legal mechanisms, matters of social justice and fairness go beyond the legal requirements to ensure non-discrimination (e.g. poverty, a risk factor associated with worse mental health outcomes, is not a protected characteristic15 as set out in the Equality Act 2010)16. Moreover, legal texts often leave wide scope for actions that may be sufficient to discharge duties corresponding to individual rights17, but are seen by many as, at best, failing to observe the spirit of the law, and at worst, morally impermissible (e.g. privacy policies).</p> <p>A particularly well known illustration of this problem is the EU's General Data Protection Regulation (GDPR) and ePrivacy Directive. The GDPR (and directive) resulted in widespread changes to the operation of cookies, including a requirement to receive users\u2019 consent before any cookies were used, except those strictly necessary. However, as almost everyone will know from first-hand experience, the manner in which some organisations secure consent can range from the entirely user-friendly, to the intentionally frustrating use of dark patterns18 or hours long process of flipping hundreds of opt-out toggle buttons. Here, the expectations that society have regarding what is both legally and morally permissible clearly differ substantially from what is desirable from the perspective of the organisation and the law.</p> <p>But these expectations also differ depending on whether the organisation is part of the public, private, or third sector, and what role they play within each sector. And, furthermore, expectations are not equally shared across a vast and homogenous \u201cpublic\u201d. Quite the opposite in fact.</p> <p>Consider, for example, the range of attitudes that members of the public may have towards a private company extracting economic value from their data collection activities. Depending on key details about the informational content of the data, attitudes could range from shareholder praise for savvy corporate governance, through to begrudging toleration by consumers, and up to the vehement and vocal criticism by privacy activists or employee campaign groups.</p> <p>And to add one final layer of complexity, to really drive home Whitman's point from the start of this chapter, the scope and distribution of this variation may increase as we expand our field of consideration to the public and third sectors. Now, the same data extraction could be seen as deeply unethical or impermissible by those who were in favour of it originally. In terms of underlying values, therefore, pluralism and variation should be expected when considering the attitudes of the publics (reiterating the emphasis on the plural).</p> <p>By now, you may be feeling as though stoic acceptance in the face of overwhelming complexity is inevitable. What other options are there? You may be thinking, for instance, that there are simply too many factors for any one person or organisation to consider when researching, developing, or regulating DMHTs.</p> <p>However, this report (and our project more generally) aims to challenge this attitude while fully acknowledging the overwhelming complexity involved. Our approach and methodology in many respects embodies the principle that light is the best disinfectant. We can frame our approach as a set of recommendations that build on the two earlier conclusions from the Nuffield Council on Bioethics.</p> Original Conclusions from Nuffield Council on BioethicsAdditional Recommendations   <ol> <li>Many people affected by mental health problems do not have access to or are reluctant to use mental healthcare technologies. If these are to become widely adopted in the future, there should be choice about using them.</li> <li>Technology solutions should not divert resources from other important forms of mental healthcare and support and should be used as an addition to what is already available, rather than a replacement.</li> </ol>   <ol> <li>Organisations that choose to use DMHTs should consider broader ethical goals, in addition to traditional goals such as \u2018safety\u2019 and \u2018efficacy\u2019, to help create a more ethical, responsible, and trustworthy ecosystem of digital mental healthcare.19</li> <li>Organisations should provide transparent and evidence-based assurance about how these ethical goals have been operationalised and secured during the design, development, and deployment of DMHTs.</li> </ol>    <p>With these points in mind, we can now turn to the project itself and introduce the notion of trustworthy assurance\u2014a methodology that can help address all of the above recommendations (and more to come).</p>","location":"dmh-report/chapter-1/#a-culture-of-distrust","tags":["culture-of-distrust","landscape","digital-mental-healthcare"]},{"title":"About the Project","text":"<p>Assurance is a process of establishing trust.</p> <p>Whether we trust someone or some object depends, in part, on the evidence we have to help us evaluate whether there are good grounds for placing trust. In other words, what is the evidence of their trustworthiness?</p> <p>When it comes to trust, we do not expect the same level of evidence when assessing the trustworthiness of different people, objects, or systems. A trustworthy doctor, for example, is not assessed by the same standards as a trustworthy friend. And, similarly, the trustworthiness of an AI chatbot used in customer services is not (and ought not) be evaluated by the same measures as an AI chatbot used to support people with their mental health.</p> <p>In short, when we speak of 'trustworthy assurance' we are creating room for a wide variety of associated goals and standards, to accommodate the complexity alluded to in the previous sections. These can, of course include goals and standards related to \u2018safety\u2019 or \u2018clinical efficacy\u2019, which carry their own ethical significance. However, for present purposes we are primarily interested in those goals that are directly framed in terms of ethical principles (e.g. fairness).</p> <p>Our project focused directly on a methodology for making the assessment, communication, and realisation of these goals more robust and transparent. The methodology is known as \u2018argument-based assurance\u2019 (ABA) and we can define this methodology as follows:</p>  <p>Argument-based assurance is a process of using structured argumentation to provide assurance to another party (or parties) that a particular claim (or set of related claims) about a property of a system is warranted given the available evidence.22</p>  <p>We offer a simplified introduction to ABA in the following chapter. But various types of ABA are already widely used in safety critical domains, and have also been used in the context of healthcare20. Typically, the purpose of ABA is to assess and communicate the safety of a system within a particular environment. Our project was concerned with the question of whether a revised and extended version of the methodology could be used for a broader set of ethical goals, such as fairness or explainability (see Chapter 5).</p> <p>There is an immediate question that ought to be addressed here:</p>  <p>Question</p> <p>How should ethical goals be determined and operationalised in the context of the design, development, and deployment of DMHTs?</p>  <p>Our approach in this project to determining and operationalising the relevant ethical goals was participatory in nature, and was driven by three primary objectives:</p> <ol> <li>To explore whether and how the methodology of ABA could be extended to address ethical issues in the context of digital mental healthcare.</li> <li>To evaluate how an extension of the methodology could support stakeholder co-design and engagement, in order to build a more trustworthy and responsible ecosystem of digital mental healthcare.</li> <li>To lay the theoretical and practical foundations for scaling the trustworthy assurance methodology to new domains, while integrating wider regulatory guidance (e.g. technical standards).</li> </ol> <p>To realise these objectives, several workshops were organised and run over the course of the project with a diverse set of participants. Broadly, we categorised these stakeholder groups as follows:</p> <ul> <li>University students</li> <li>University administrators</li> <li>Policy-makers and regulators in healthcare</li> <li>Developers of DMHTs</li> <li>Researchers working in disciplines adjacent to digital mental healthcare</li> <li>Users with lived experience of DMHTs</li> </ul> <p>Workshops and interviews were held with representatives from each of these stakeholder groups, where tailored activities were run to both understand their attitudes towards DMHTs, but also to a) help us evaluate methodological questions related to trustworthy assurance and b) identify which ethical values and principles matter most to them in the present context.</p> <p>Chapters 3 and 4 present our findings, analysis, and recommendations from the engagements. Here, we shall just speak to the procedural matter of operationalising ethical principles through processes of stakeholder participation and engagement.</p>","location":"dmh-report/chapter-1/#about-the-project","tags":["culture-of-distrust","landscape","digital-mental-healthcare"]},{"title":"SAFE-D Principles","text":"<p>In previous work, we have defended an ethical framework for evaluating the harms and benefits of data-driven technologies, which has already been revised, tested, and validated with a wide-variety of stakeholders21.</p> <p>We refer to this framework as the SAFE-D framework, because it establishes five principles that form the acronym SAFE-D (or, \u2018safety\u2019, which is another important component of trustworthy AI):</p> <ul> <li>Sustainability</li> <li>Accountability</li> <li>Fairness</li> <li>Explainability</li> <li>Data (Quality, Integrity, Protection and Privacy)</li> </ul> <p>Each of the SAFE-D principles has a subset of core attributes that help to specify and operationalise the principles throughout a project's lifecycle using a series of processes and activities (see next chapter for full details).</p> <p>In other words, while the principles themselves act as starting points for context-specific reflection and deliberation with affected stakeholders, it is the core attributes that serve as practical guardrails throughout a project\u2019s lifecycle. For instance, the principle of 'explainability', which emphasises core attributes such as transparency, interpretability, and accessibility of an automated system, has a particular ethical significance when utilised in a domain such as digital mental healthcare. That is, ensuring digital mental healthcare technologies and services are explainable is a key part of respecting a patient's right to informed and autonomous decision-making. This right cannot be upheld and respected without ensuring sufficiently transparent, interpretable, and accessible forms of information about how a digital technology operates (e.g. how an algorithmic system reaches a decision). How organisations achieve this goal is something this project and report addresses directly.</p> <p>While the SAFE-D principles have been designed and refined over multiple years (in a domain-general context) their relevance in digital mental healthcare had, hitherto, not been evaluated. Therefore, part of this project involved the following:</p> <ol> <li>Understanding which, if any, of the SAFE-D principles were significant to different groups of stakeholders, and whether specific core attributes could be identified and developed in conjunction with stakeholders.</li> <li>Identifying if there were any gaps or omissions in the SAFE-D framework.</li> <li>Determining whether any of the revised and domain-specific principles or attributes could serve as top-level goals or property claims in trustworthy assurance cases (see Chapter 2).</li> </ol> <p>Our findings and analysis that address these specific research questions comprise the majority of Chapters 3 and 4. Among other findings and recommendations, these sections show there is strong evidence to suggest that the methodology of trustworthy assurance will lead to positive impacts in digital mental healthcare, and help foster a more responsible ecosystem of research and innovation.</p> <p>Before we discuss these findings and analysis though, it is necessary to introduce and explain the methodology of trustworthy assurance, which is the topic of the next chapter.</p>   <ol> <li> <p>See for example, the work programme being conducted by the Medicines &amp; Healthcare Regulatory Agency (MHRA) on Software and AI as a Medical Device, the Evidence standards framework (ESF) for digital health technologies from the National Institute for Health and Care Excellence (NICE), and the proposed work from the Multi-agency advisory service (MAAS) for artificial intelligence (AI) and data-driven technologies, which is being funded by the NHS AI Lab.\u00a0\u21a9</p> </li> <li> <p>For the purpose of this report we use the term 'trust' to refer to those characteristics of a person's beliefs or attitudes that are directed towards an object, person, or proposition (among other things), whereas 'trustworthiness' refers to the perceived property or attribute which an individual uses to determine whether to place trust (e.g. whether to trust a news article based on its quoted sources).\u00a0\u21a9</p> </li> <li> <p>Burr, C., J. Morley, M. Taddeo, &amp; L. Floridi. (2020). Digital Psychiatry: Risks and Opportunities for Public Health and Wellbeing. IEEE Transactions on Technology and Society, 1(1), 21\u201333. https://doi.org/10.1109/TTS.2020.2977059 \u21a9\u21a9</p> </li> <li> <p>We explored this point more fully in a separate article: Burr, C. (2022). Charities are contributing to growing mistrust of mental-health text support\u2014Here\u2019s why. The Conversation. Retrieved 29 July 2022, from http://theconversation.com/charities-are-contributing-to-growing-mistrust-of-mental-health-text-support-heres-why-179056 \u21a9\u21a9</p> </li> <li> <p>Nuffield Council on Bioethics (2022) The role of technology in mental healthcare. https://www.nuffieldbioethics.org/assets/pdfs/The-role-of-technology-in-mental-healthcare.pdf \u21a9\u21a9</p> </li> <li> <p>Torous et al. (2016) New Tools for New Research in Psychiatry: A Scalable and Customizable Platform to Empower Data Driven Smartphone Research. JMIR Ment Health. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4873624/ \u21a9</p> </li> <li> <p>For instance, if we draw the class as comprising digital technologies for \"health and well-being\" we will capture technologies as diverse as ML algorithms used to identify associations between genetic factors and mental health outcomes, to mobile apps that use natural language processing techniques to help users better understand their feelings through a smart diary.\u00a0\u21a9</p> </li> <li> <p>https://www.england.nhs.uk/mental-health/cyp/ \u21a9</p> </li> <li> <p>https://digital.nhs.uk/data-and-information/publications/statistical/mental-health-of-children-and-young-people-in-england/2021-follow-up-to-the-2017-survey \u21a9</p> </li> <li> <p>https://www.ons.gov.uk/peoplepopulationandcommunity/birthsdeathsandmarriages/deaths/bulletins/suicidesintheunitedkingdom/2020registrations \u21a9</p> </li> <li> <p>https://www.ons.gov.uk/peoplepopulationandcommunity/wellbeing/articles/coronavirusanddepressioninadultsgreatbritain/januarytomarch2021 \u21a9\u21a9</p> </li> <li> <p>See references and discussion on 'universal design' in Burr, C., Taddeo, M., &amp; Floridi, L. (2020). The Ethics of Digital Well-Being: A Thematic Review. Science and Engineering Ethics. https://doi.org/10.1007/s11948-020-00175-8 \u21a9</p> </li> <li> <p>For a timeline that conveys a shocking pattern of behaviour at Facebook, which is hard to treat as anything other than a flagrant disregard for user\u2019s data privacy, see Facebook data privacy scandal: A cheat sheet.\u00a0\u21a9</p> </li> <li> <p>It is important to note that private organisations are still bound by the legal duties of non-discrimination, even where they fall outside the wider scope of the public sector equality duty. For further advice and guidance on these topics, see this recent publication by the Equality and Human Rights Commission on Artificial Intelligence in Public Services.\u00a0\u21a9</p> </li> <li> <p>The Equality Act 2010 sets out the following protected characteristics:</p> <ul> <li>Age</li> <li>Disability</li> <li>Gender reassignment</li> <li>Marriage and civil partnership</li> <li>Pregnancy and maternity</li> <li>Race</li> <li>Religion or belief</li> <li>Sex</li> <li>Sexual orientation</li> </ul> <p>They are protected in the sense that the law is designed to protect individuals from unfair treatment or discrimination on the basis of these characteristics.\u00a0\u21a9</p> </li> <li> <p>Our thanks to a reviewer for bringing the following example to our attention of a local council who unanimously voted to make 'care experience' a protected characteristic within its constituency: https://www.cypnow.co.uk/news/article/cumberland-council-votes-to-make-care-experience-a-protected-characteristic.\u00a0\u21a9</p> </li> <li> <p>For instance, Article 8 of the EU Charter of Fundamental Rights (On the Protection of personal data) states, \"1. Everyone has the right to the protection of personal data concerning him or her. 2. Such data must be processed fairly for specified purposes and on the basis of the consent of the person concerned or some other legitimate basis laid down by law. Everyone has the right of access to data which has been collected concerning him or her, and the right to have it rectified. 3. Compliance with these rules shall be subject to control by an independent authority.\" Provisions 1 and 2 help to delineate the remit of an individual's right, whereas provision 3 establishes a corresponding duty on member states that helps ensure the aforementioned rights are guaranteed and protected.\u00a0\u21a9</p> </li> <li> <p>Dark patterns are design elements of an user interface that have been intentionally chosen to manipulate or deceive users into taking actions or making sub-conscious choices, which they would be unlikely to do when conscious of the outcome (e.g. purchasing a more expensive product, agreeing to invasive privacy policies).\u00a0\u21a9</p> </li> <li> <p>To emphasise the consistency of this recommendation with the conclusions from the Nuffield Council report we should also acknowledge that this \u2018digital ecosystem\u2019 must be complementary with, supportive of, and enhance more traditional healthcare services (qua conclusion 2).\u00a0\u21a9</p> </li> <li> <p>See an older, but still important report from Health Foundation as well as a more recent proposal: Habli et al. (2020). Enhancing COVID-19 decision making by creating an assurance case for epidemiological models. BMJ Health &amp; Care Informatics, 27(3), e100165. https://doi.org/10.1136/bmjhci-2020-100165 \u21a9</p> </li> <li> <p>The most recent and comprehensive account of this framework can be found in the following proposal to the Council of Europe: Leslie, D., Burr, C., Aitken, M., Katell, M., Briggs, M., &amp; Rincon, C. (2022). Human rights, democracy, and the rule of law assurance framework for AI systems: A proposal. https://rm.coe.int/huderaf-coe-final-1-2752-6741-5300-v-1/1680a3f688.\u00a0\u21a9</p> </li> <li> <p>Christopher Burr and David Leslie. Ethical assurance: a practical approach to the responsible design, development, and deployment of data-driven technologies. AI and Ethics, June 2022. URL: https://link.springer.com/10.1007/s43681-022-00178-0 (visited on 2022-08-01), doi:10.1007/s43681-022-00178-0.\u00a0\u21a9</p> </li> </ol>","location":"dmh-report/chapter-1/#safe-d-principles","tags":["culture-of-distrust","landscape","digital-mental-healthcare"]},{"title":"Presenting Trustworthy Assurance\u2014A Framework and Methodology","text":"<p></p>  <p>Chapter Overview</p> <p>This section introduces a framework and methodology for enabling a more trustworthy ecosystem of digital mental healthcare through the responsible and ethical design, development, and deployment of digital technologies. The section also serves as an introduction for the analysis and recommendations for the following sections.</p> <p>First, we introduce a model of a typical research or innovation lifecycle for a data science or AI project that includes activities of project design, model development, and system deployment. </p> <p>Second, we discuss the methodology of trustworthy assurance that is at the centre of our project. We provide a simple overview of the relevant procedures, focusing on the structure, elements, and purpose of an assurance case.</p> <p>Finally, we look at argument patterns: reusable templates that can be developed to ensure a more consistent approach to trustworthy design, development, and deployment of digital mental healthcare.</p>","location":"dmh-report/chapter-2/","tags":["lifecycle","assurance","SAFE-D","argument-patterns"]},{"title":"Designing, Developing, and Deploying Trustworthy Digital Mental Health Technologies","text":"<p>Designing, developing, and deploying an AI system is not a one-person task!1 The stages and activities that comprise a typical AI project lifecycle involve a wide-ranging set of skills and capabilities. These skills are encapsulated within a variety of roles, including 'project commissioner', 'product manager', 'data protection officer', 'data scientist', 'system architect' and 'software engineer'. And, these roles are interwoven such that they create an irreducible and collective responsibility that spans the entire project lifecycle, and may span multiple teams and organisations.</p> <p>Figure 2.1 presents a simplified model of a typical research or innovation lifecycle for a data science or AI project lifecycle, to help gain a purchase on these interweaving roles, skills, and responsibilities2.</p> <p>The model represents three over-arching stages of <code>(project) design</code>, <code>(model) development</code>, and <code>(system) deployment</code>. For each stage, there are corresponding activities, detailed in Table 2.1. The project lifecycle is depicted as a circular process to highlight the fact that responsibility is ongoing and does not end once a system has been implemented or put into deployment. Rather, responsible (and trustworthy) approaches to research and innovation require consideration of how a technological system may need to be monitored and updated once in production, and removed and replaced once it reaches the end of its lifecycle. </p> <p> Figure 2.1: a model of a typical project lifecycle for a data-driven technology, detailing the overarching stages of design, development, and deployment and their constitutive activities (reprinted from Burr and Leslie, 2022)</p> <p>Table 2.1: Project Lifecycle Activities</p>    Stage Description     Project Planning Preliminary activities designed to help scope out the aims, objectives, and processes involved with the project, including potential risks and benefits.   Problem Formulation The formulation of a clear statement about the over-arching problem the system or project addresses (e.g., a research statement or system specification) and a lower-level description of the computational procedure that instantiates it.   Data Extraction or Procurement The design of an experimental method or decisions about data gathering and collection, based on the planning and problem formulation from the previous steps.   Data Analysis Stages of exploratory and confirmatory data analysis designed to help researchers or developers identify relevant associations between input variables and target variables.   Preprocessing and Feature Engineering A process of cleaning, normalising, and refactoring data into the features that will be used in model training and testing, as well as the features that may be used in the final system.   Model Selection and Training The selection of a particular algorithm (or multiple algorithms) for training the model.   Model Testing and Validation Testing the model against a variety of metrics, which may include those that assess how accurate a model is for different sub-groups of a population. This is important where issues of fairness or equality may arise.   Model Documentation A process of documenting both the formal and non-formal properties of both the model and the processes by which it was developed (e.g., source of data, algorithms used, evaluation metrics).   System Implementation The process of implementing the technological system into its intended environment or target domain to enable and structure interaction with the underlying model(s) (e.g. a recommender system that suggests possible treatment options for patients based on input data.   User Training Training for those individuals or groups who are either required to operate a data-driven system (perhaps in a safety critical context) or who are likely to use the system (e.g. healthcare professionals, medical researchers).   System Use and Monitoring Ongoing monitoring and feedback from the system, either automated or probed, to ensure that issues such as model drift have not affected performance or resulted in harms to individuals or groups.   Model Updating or Deprovisioning An algorithmic model that that adapts its behaviour over time or context may require updating.3 Where no further updating can be carried out, and this results in a system being removed from production (i.e. deprovisioned), a new system may be required. This restarts the project lifecycle.    <p>To see how this model can help us understand the interwoven nature of responsibility, consider the following example. An organisation wants to implement a speech recognition algorithm within a service they are developing for online counselling. However, there is no one in the organisation with the relevant expertise to collect data and train a model from scratch. Therefore, they choose to procure a pre-trained model from another company. This means that a significant portion of the project lifecycle\u2014from <code>Data Extraction or Procurement</code> to <code>Model Documentation</code>\u2014will have been carried out by a separate organisation.</p> <p>Although the specifics of the relationship between the two organisations will complicate forms of responsibility, such as legal duties or obligations, this need not concern us here. Instead, we can focus on how the initial organisation who has chosen to undertake the project (e.g. the product owner or commissioner) can use the project lifecycle model to a) identify and analyse their own responsibility and how it intersects with the responsibilities of others, and b) how this necessitates a process of trustworthy communication and assurance.</p>","location":"dmh-report/chapter-2/#designing-developing-and-deploying-trustworthy-digital-mental-health-technologies","tags":["lifecycle","assurance","SAFE-D","argument-patterns"]},{"title":"Reflective and Anticipatory Deliberation","text":"<p>At the start of a project, while activities such as planning and initial evaluation of feasibility are being conducted, the project lifecycle model can be used to structure reflective and anticipatory processes of deliberation among the project team. For instance, the team could use the model to identify and evaluate potential actions and decisions that are likely to emerge during specific activities, such as which data types may be required and whether stakeholders or users will consent to these data being collected and analysed (a reflective and anticipatory exercise). As this example suggests, the project team may carry out the preliminary deliberation, but additional stakeholders will need to be engaged to thoroughly evaluate the ethical, legal, and social permissibility and acceptability of the project.</p> <p>Consider another example. A team of developers working for a commercial organisation have identified a risk associated with an AI system they have developed, which they claim is able to detect emotions. They have been approached by a healthcare provider who wish to procure and implement their system into a video consultation service to help their counsellors better understand the emotional and behavioural responses of their patients during an initial assessment. However, the developers did not evaluate their model (during <code>Model Testing or Validation</code>) using a dataset that is representative of the patient population intended by the healthcare provider that has approached them (i.e. individuals that are likely to be suffering from a mental health issue). Therefore, the developers are unable to make any claims about the generalisability of their model to this new population. Moreover, neither the developers nor the healthcare provider have engaged the relevant stakeholder groups during <code>Project Planning</code> to determine if this would be an acceptable use for their system. As such, additional activities would need to be carried out to determine the full scope of the risks and possible harms that could arise from the use of this technology. This would likely require the procuring organisation (i.e. the healthcare provider) to set clear requirements for what forms of evidence would be required from the developers (e.g., at <code>Model Documentation</code>), and to determine clear boundaries and thresholds for whether the project should proceed.</p> <p>As this example illustrates, the project lifecycle structure can help support forms of reflective and anticipatory deliberation that help instantiate a responsible ecosystem of research and innovation. And, it can also help identify points in the lifecycle where structured and transparent communication between teams and organisations may be crucial.</p> <p>In addition, there is a further purpose for the project lifecycle model that will become clearer in the next sections: the identification of actions and decisions that generate forms of evidence that provide justificatory support for trustworthy assurance.</p>","location":"dmh-report/chapter-2/#reflective-and-anticipatory-deliberation","tags":["lifecycle","assurance","SAFE-D","argument-patterns"]},{"title":"What is Trustworthy Assurance?","text":"<p>Trustworthy assurance is a procedure for developing a structured argument, which provides reviewable (and contestable) assurance that a set of claims about the ethical properties of a data-driven technology are warranted given the available evidence.</p>  <p>This definition captures three important and interlocking components of trustworthy assurance:</p> <ol> <li>A structured argument comprising linked claims and evidence that collectively justify a top-level goal</li> <li>A procedure for developing an assurance case, which represents the argument either formally and/or visually</li> <li>Agreed upon standards for reviewing and evaluating the argument</li> </ol> <p> Figure 2.2: a schematic showing the three interlocking components that support trustworthy assurance.</p>","location":"dmh-report/chapter-2/#what-is-trustworthy-assurance","tags":["lifecycle","assurance","SAFE-D","argument-patterns"]},{"title":"Argument","text":"<p>Trustworthy assurance is a form of argument-based assurance. It uses a structured type of documented argumentation, known as an assurance case, as the primary means for providing assurance that a goal has been obtained, based on the claims and evidence presented.</p> <p>There are three basic elements of an argument:</p> <ol> <li>A claim about the goal to be established (supported by descriptions of the system and the context in which the system is intended to operate)</li> <li>A set of property claims about the project or system that collectively specify and operationalise the goal</li> <li>A set of evidential claims that jointly establish the validity of the property claims</li> </ol>  <p>Box 2.1 Elements of a Trustworthy Assurance Case</p> <p>The following table provides summary information about the typical elements of a trustworthy assurance case.  NB: the colours do not mean anything. They are purely used as a visual aid to help differentiate the elements.</p>    Element Name Description Icon     Goal Claim A claim about an ethical goal of the DMHT, which the assurance case attempts to justify has been established on the basis of the evidence and argument provided.    System Description A short description about the DMHT, including any central algorithmic techniques.    Context Description A short description about the intended context of use for the DMHT, including the users of the system (e.g. healthcare professionals).    Property Claim A claim about how the ethical goal has been implemented or opertaionalised, which references a property of the system or project, e.g. an action that was undertaken during the model's development.    Evidential Claim A specific claim about some evidence, which serves to establish the validity of the higher level property claim.    Evidential Artefact A description of the evidence referred to by the above evidential claim, including a link to the relevant document where available.      <p>The following figure constitutes a simple (but incomplete) argument, showing the relationship between the three central elements.</p> <p></p> <p>Figure 2.3: a portion of a simplified assurance case on respect for privacy</p> <p>Here, the goal that is being established relates to the project team's ambition to 'respect user privacy'. They argue that this is achieved by adherence to data minimisation principles\u2014a claim about a property of the project's governance and the system's design. Evidence of this adherence is also provided.</p> <p>While useful as an illustration, this example is too simple to constitute a full-fledged assurance case because it reduces the concept of 'respect for user privacy' to a single principle (i.e., data minimisation). Although this claim may be relevant, on its own it is insufficient. We can, for example, consider an app that collects no personal data but still violates reasonable expectations of privacy by routinely notifying and disturbing users.</p> <p>A more detailed procedure is required, therefore, to help project teams identify the set of property claims that a) specify and operationalise the top-level goal and b) collectively justify the goal.</p>  <p>Box 2.2 Argument-Based Assurance Notation</p> <p>Argument-based assurance (ABA) was defined in the previous chapter as follows:</p>  <p>A process of using structured argumentation to provide assurance to another party (or parties) that a particular claim (or set of related claims) about a property of a system is warranted given the available evidence. </p>  <p>ABA is widely used in safety-critical domains or industries where manufacturing, development, and maintenance processes are required to comply with strict regulatory requirements and legislation, while also supporting industry-recognised best practices 16. Because of these requirements, there are many formal standards that can be used to better govern the process of constructing an assurance case. </p> <p>A popular standard is known as 'Goal Structuring Notation' (GSN)\u2014originally developed in the 1990s at the University of York to assist the production, maintenance, and reuse of safety and assurance cases in safety critical industries such as traffic management and nuclear power 17. There are many similarities between GSN's assurance cases and Trustworthy Assurance, as the latter was directly inspired by the former. For example, as the name implies, GSN structures an assurance case towards a particular goal, and best practices associated with the standard prescribe methods for minimising possible harms are proportionate to the risks presented by the technology or system (e.g. minimisation of safety risks to levels that are as low as reasonably practicable). However, GSN also has additional elements (e.g. solutions and strategies) and relationships between elements that we do not include in the current presentation of Trustworthy Assurance4.</p> <p>It should also be noted that our use of colours for the various elements should not be seen as signifying any meaning within a formal context. This choice was made solely for ease of comprehension for our stakeholders who were unfamiliar with the method. As we discuss in the Conclusion, our future ambitions are to explore how GSN can be used to anchor Trustworthy Assurance in a more formal notation or schema. However, for this project we chose to sidestep many of the formal considerations that arise in the GSN standard (or any other formal standards5) due to the likely barriers to comprehension that existed for our stakeholders. </p> <p>Further Resources</p> <p>The following resources provide good overviews and clear introductions for the reader who is interested in further exploring argument-based assurance:</p> <ul> <li>Kelly, T. (1998) Arguing Safety \u2013 A Systematic Approach to Managing Safety Cases (PhD Thesis). Available: https://www-users.cs.york.ac.uk/~tpk/tpkthesis.pdf</li> <li>The Assurance Case Working Group. (2021). GSN Community Standard Version 3. Available: https://scsc.uk/r141C:1?t=1</li> <li>Hawkins, R., Paterson, C., Picardi, C., Jia, Y., Calinescu, R., &amp; Habli, I. (2021). Guidance on the Assurance of Machine Learning in Autonomous Systems (AMLAS). University of York. https://www.york.ac.uk/media/assuring-autonomy/documents/AMLASv1.1.pdf</li> <li>Laher, S., Brackstone, C., Reis, S., Nguyen, A., White, S., &amp; Habli, I. (2022). Review of the Assurance of Machine Learning for use in Autonomous Systems (AMLAS) Methodology for Application in Healthcare. 32. https://arxiv.org/ftp/arxiv/papers/2209/2209.00421.pdf</li> <li>Sujan, M. A., Habli, I., Kelly, T. P., Pozzi, S., &amp; Johnson, C. W. (2016). Should healthcare providers do safety cases? Lessons from a cross-industry review of safety case practices. Safety Science, 84, 181\u2013189. https://doi.org/10.1016/j.ssci.2015.12.021</li> </ul>","location":"dmh-report/chapter-2/#argument","tags":["lifecycle","assurance","SAFE-D","argument-patterns"]},{"title":"Procedure","text":"<p>The procedure advocated for trustworthy assurance is the anticipatory and deliberative exercise introduced above, which incorporates inclusive and accessible forms of stakeholder engagement.</p> <p>By using the project lifecycle model as a scaffold for anticipatory reflection and stakeholder-informed deliberation, project teams are able to answer the following questions:</p> <ol> <li>Which claims (that may emerge from participatory deliberation) are necessary and sufficient to specify and justify the top-level goal?</li> <li>How do these claims relate to one another?</li> <li>What evidence is required to demonstrate the validity of the claims being made?</li> </ol> <p>Consider the following example. A company wishes to develop an assurance case that shows how their system, which uses a ML algorithm to predict whether users of an online betting platform are \"problem gamblers\", can generate results that are explainable to their users. They decide this is an important ethical goal for an assurance case, because they want to be able to provide accessible forms of communication to any user that they contact on the basis of their algorithmically-generated prediction.</p> <p>They start by formulating the following goal statement, which sets out an ambitious objective to achieve:</p>  <p>\"An explanation of how our system predicts whether a user is a \"problem gambler\" can be provided to all users of our platform.\"</p>  <p>Next, they consider which potential actions or decisions taken throughout the stages of the project lifecycle could be relevant to the specification and justification of this goal. For example, they flag that results from a series of planned workshops to be carried out with representative users during their <code>Project Planning</code> activities may be relevant. </p> <p>Following the delivery of these workshops, it turns out that plain language explanations are preferable to detailed explanations of the algorithmic techniques, but that users were more trusting of these explanations when they knew they had been independently validated by a professional auditor. This result influences which machine learning algorithm the team go on to select during the <code>Model Selection and Training</code> activities, and which features they report on during <code>Model Documentation</code> for the independent audit. </p> <p>Once the team have reflected on all the stages of the project lifecycle, and carried out the corresponding activities, they recognise that there are two broad sets of claims. One set of claims are about the design choices made during the project, which support accessible explanations for users. The second set are about the interpretability of the system, which are relevant for professional auditing and assessment of the system. Categorising the claims in this manner helps the company determine what evidence will be needed for each claim, and how best to structure the argument. At this point, most of the evidence has already been generated as a byproduct of the team's work, so this stage is primarily a matter of collection, curation, and communication (through documenting an assurance case).</p> <p>The following figure summarises the points made in the above example.</p> <p></p> <p>Figure 2.4: a portion of the assurance case for this hypothetical project.</p> <p>The final component of trustworthy assurance relates to standards.</p>","location":"dmh-report/chapter-2/#procedure","tags":["lifecycle","assurance","SAFE-D","argument-patterns"]},{"title":"Standards","text":"<p>Standards support the development and refinement of best practices and codes of conduct. There are standards for measurement (e.g., universal scales), procedures (e.g., manufacturing), and assessments (e.g., risk and impact assessment), and much more. Here, we are interested in standards as they pertain to evidence and claims. </p> <p>Evidential standards can refer to both the identification and evaluation of evidence. </p> <p>Standards for identifying evidence are common in areas such as law where rules exist to determine what constitutes relevant, material, and admissible evidence.</p> <p>Similarly, standards for evaluating the quality of evidence are well-established in domains such as scientific research, where various procedures or methods are held to produce reliable forms of evidence (e.g. peer review or randomised controlled trials), and in risk assessment and management (e.g. standardised guidelines on risk management for systems and software engineering).6</p> <p>As distinct communities of practice develop and emerge within digital mental healthcare, we would expect standards and best practices to evolve to help with both the identification and evaluation of evidence. Subsequently, this would help support the development of trustworthy assurance in domains such as digital mental healthcare because specific types of claims or evidence would be recognised as more reliable forms of evidence-based assurance.</p> <p>It should be noted, however, that regulators and developers are not starting with a blank slate. There are many relevant standards that exist today, and new standards are emerging to support the procedure of constructing a trustworthy assurance case.7 We will consider some of these standards in Chapter 3.</p>","location":"dmh-report/chapter-2/#standards","tags":["lifecycle","assurance","SAFE-D","argument-patterns"]},{"title":"Argument Patterns","text":"","location":"dmh-report/chapter-2/#argument-patterns","tags":["lifecycle","assurance","SAFE-D","argument-patterns"]},{"title":"Claims as Reasons","text":"<p>Trustworthy assurance is a process of giving and justifying claims about choices made during the design, development, and deployment of DMHTs. These claims can be viewed as a series of reasons for why a particular decision was made.</p> <p>To see why, let\u2019s assume that an organisation is in the process of procuring an AI-enabled chatbot to provide therapeutic support to service members returning from deployment.8 As this technology is new and relatively untested, the organisation has a series of questions for the developers.</p> <ol> <li>\"Why should we license this digital system instead of investing in traditional forms of talk therapy?\"</li> <li>\"Why have you chosen a female avatar as your virtual assistant?\"</li> <li>\"How did you measure and validate the clinical efficacy of the system for different subgroups to ensure that it is fair?\"</li> </ol> <p>In answering these three questions, the developers would be giving reasons (supported by evidence) for their actions\u2014reasons that would need to be accepted by the procuring organisation to be relevant and justifiable. This perspective on claims emphasises one of their most vital roles as publicly contestable reasons. That is, whether a claim or set of interrelated claims are valid in the context of an assurance case is, in part, conditional on whether they are accepted as reasonable justifications by those who are tasked with evaluating the assurance case9.</p> <p>Let\u2019s look at another example. Consider the following section of an assurance case for the aforementioned chatbot, which the developers have produced for the procuring organisation:</p> <p></p> <p>Figure 2.5: a portion of an assurance case for a chatbot10</p> <p>As we have discussed, the organisation responsible for evaluating the trustworthiness of the AI system has to determine whether the evidential claim (EC1) is a reasonable choice to justify its parent claim (C2). They may, for instance, argue that EC1 is a reasonable claim, but nevertheless argue that it is insufficient on its own to justify the claim that the chatbot is an \"accessible\" alternative to human-led therapy. Alternatively, they may claim that it is not reasonable on the grounds that the ratings given by the service members are not relevant to establishing whether the chatbot is an \u201caccessible form of therapy\u201d but merely that the avatar is \u201cfriendly and realistic\u201d. This example highlights a potential challenge associated with the development of assurance cases: determining what constitutes relevant, sufficient, and reasonable claims.</p> <p>In the context of safety assurance, a large body of guidance has been established to help developers assess what claims they will need to establish and justify, and a key part of this guidance is the development of argument patterns.</p>","location":"dmh-report/chapter-2/#claims-as-reasons","tags":["lifecycle","assurance","SAFE-D","argument-patterns"]},{"title":"What are argument patterns?","text":"<p>Argument patterns are starting templates for building assurance cases. They identify the types of claims (or, the sets of reasons) that need to be established to justify the associated top-level normative goal. Figure 2.6 shows an example argument pattern for the technical goal of interpretability.11</p> <p> Figure 2.6: a pattern for an interpretability case (reprinted from Ward and Habli, 2020)</p> <p>The pattern depicted in Figure 2.6 shows a template for an assurance case that serves to justify the following top-level goal:</p>  <p>\"The {ML Model} is sufficiently {interpretable} in the intended {context}\".</p>  <p>Here, the curly brackets serve as placeholders for specific variables that are properly established when a full assurance case is developed. A notable contribution of this pattern is the identification of three essential aspects of interpretability:</p> <ol> <li>Right Method: The right interpretability methods are implemented, i.e. the correct information is faithfully being explained.</li> <li>Right Context:<ul> <li>Time: Interpretations produced at the appropriate times.</li> <li>Setting: Interpretations are available in the right setting.</li> <li>Audience: Interpretations produced for the right audience.</li> </ul> </li> <li>Right Format: The interpretability methods are presented in the right format for the audience.</li> </ol> <p>These three essential aspects subsequently serve to delineate the more detailed argument and evidential claims at the lowest levels. </p> <p>Argument patterns, such as the one above, are helpful for the following reasons:</p> <ul> <li>They provide a consistent and systematic approach for the reflective and deliberative activities carried out across a project\u2019s lifecycle. </li> <li>They speed up the process of developing assurance cases.</li> <li>They provide reusable structures that, if used widely throughout a domain, could establish best practices.</li> </ul> <p>But where do they come from?</p>","location":"dmh-report/chapter-2/#what-are-argument-patterns","tags":["lifecycle","assurance","SAFE-D","argument-patterns"]},{"title":"Generalisable Patterns","text":"<p>In the case of the argument pattern from Figure 2.6, this pattern was proposed by the authors as a means to address a gap in the safety assurance of ML systems. As experts in their domain, and as a peer-reviewed contribution, this is a valid source for an argument pattern.</p> <p>However, an alternative (though not entirely disconnected) means for achieving generalisable structures and patterns is through participatory engagement from stakeholders and affected users, perhaps building sample assurance cases and then extracting common themes. This is the method that we have explored in the current project on Digital Mental Healthcare and subsequently propose as a procedure for Trustworthy Assurance.</p> <p>Much like ML algorithms, humans have remarkably effective (but biased) pattern recognition capabilities, some of which underpin our assessment and internalisation of ethical, legal, and social norms. As an example, James W. Nickel says of human rights:</p>  <p>\"We can think of the emergence of a human right as the coming together of the recognition of a problem; the belief that the problem, is very severe; and optimism about the possibility of addressing it through social and political action at national and international levels.\" 12</p>  <p>Similarly, we can think of ethical and social norms as the shared recognition and subsequent externalisation of beliefs and attitudes towards events as diverse as acceptable etiquette during a dinner party through to permissible forms of punishment for various transgressions.</p> <p>This understanding of the emergence of norms is crucial to ensuring the relevance, sufficiency, and reasonableness of evidence, and the legitimacy of corresponding trustworthy argument patterns.</p> <p>In terms of the emergence of argument patterns, the three elements that we have explored already are, again, important: the top-level normative goal, the property claims, and the evidential support. Let's take each of these in turn.</p> <p>The phrase 'trustworthy assurance' creates a wide scope for top-level goals that may be deemed relevant to establishing trust (e.g. sustainable digital platforms, accountable methods of data governance, fair classifiers, and explainable decision support systems). As trustworthy assurance cases are developed for data-driven technologies, it is likely that we will see certain goals emphasised (and re-emphasised) over others. In turn, these normative goals will orient other projects and help cultivate best practices. In related work, we have proposed a series of ethical principles that have been developed to provide actionable insights and safeguards on responsible research and innovation in data science and AI.13 They are known as the SAFE-D principles:</p> <ul> <li>Sustainability</li> <li>Accountability</li> <li>Fairness</li> <li>Explainability</li> <li>Data (Quality, Integrity, Protection and Privacy)</li> </ul> <p>These principles have been refined and validated in a wide range of domains, and were originally based on a broad understanding of the typical harms and benefits associated with data-driven technology (e.g. starting from the felt injustices or needs of users and stakeholders, and developing principles to reflect these challenges). Therefore, unlike alternative frameworks they are tailored to the specific needs and challenges of responsible, trustworthy, and ethical data science and AI, rather than, say, importing or revising existing frameworks such as biomedical ethics.14</p> <p>However, the SAFE-D principles were designed to be domain-neutral starting points. That is, we did not presume that these principles would capture the ethical, social, and legal values that are dominant in digital mental healthcare. Instead, the present project undertook a process of exploratory engagement and participatory design to explore which ethical values and principles were relevant to the specific context of trustworthy digital mental healthcare, and whether specific SAFE-D principles captured these. We will return to this point in Chapter 4 where we analyse our findings from the project's workshops.</p> <p>Turning now to the property claims and supporting evidence, as assurance cases are communicated for specific goals we will likely see sets of property claims and supporting evidence used more than others as justifiable and accepted reasons for establishing the respective goal. For instance, as developers focus on goals like 'accountability', core attributes of the system and project are likely to be emphasised as relevant targets (e.g. constructing traceable data pipelines, establishing mechanisms to support auditing processes, ensuring accessible documentation).</p> <p>Returning to the SAFE-D principles once more, we have previously developed a set of core attributes for each of the principles, which a) identify the types of properties that need to be established in a project or a system to ensure the relevant goal is obtained and b) the stages of the project lifecycle where actions can be taken to implement the respective property. Table 2.2 shows an example of the core attributes for 'sustainability'.</p> <p>Table 2.2: a summary of the core attributes for the principle 'sustainability'</p>    Core Attribute Description     Safety Safety is core to sustainability but goes beyond the mere operational safety of the system. It also includes an understanding of the long-term use context and impact of the system, and the resources needed to ensure the system continues to operate safely over time within its environment (i.e. is sustainable). For instance, safety may depend upon sufficient change monitoring processes that establish whether there has been any substantive drift in the underlying data distributions or social operating environment. Or, it could also involve engaging and involving users and stakeholders in the design and assessment of AI systems that could impact their human rights and fundamental freedoms.   Security Security encompasses the protection of several operational dimensions of an AI system when confronted with possible adversarial attack. A secure system is capable of maintaining the integrity of its constitutive information. This includes protecting its architecture from the unauthorised modification or damage of any of its component parts. A secure system also remains continuously functional and accessible to its authorised users and keeps confidential and private information secure even under hostile or adversarial conditions.   Robustness The objective of robustness can be thought of as the goal that an AI system functions reliably and accurately under harsh or uncertain conditions. These conditions may include adversarial intervention, implementer error, or skewed goal-execution by an automated learner (in reinforcement learning applications). The measure of robustness is, therefore, the strength of a system\u2019s functional integrity and the soundness of its operation in response to difficult conditions, adversarial attacks, perturbations, data poisoning, or undesirable reinforcement learning behaviour.   Reliability The objective of reliability is that an AI system behaves exactly as its designers intended and anticipated. A reliable system adheres to the specifications it was programmed to carry out. Reliability is therefore a measure of consistency and can establish confidence in the safety of a system based upon the dependability with which it conforms to its intended functionality.   Accuracy and Performance The accuracy of a model is the proportion of examples for which it generates a correct output. This performance measure is also sometimes characterised conversely as an error rate or the fraction of cases for which the model produces an incorrect output. Specifying a reasonable performance level for the system may also require refining or exchanging the measure of accuracy. For instance, if certain errors are more significant or costly than others, a metric for total cost can be integrated into the model so that the cost of one class of errors can be weighed against that of another.    <p>Again, we are not proposing that these principles and core attributes should be adopted in digital mental healthcare as the respective goals, claims, and evidence. However, they could provide a starting point for the refinement of domain-specific principles while argument patterns emerge and become crystallised.15</p> <p>Figure 2.7 offers a simple graphic to help visualise this process as it relates to trustworthy assurance.</p> <p> Figure 2.7: process of consensus formation for ethical principles as constraints on trustworthy assurance</p> <p>These preliminary remarks about trustworthy assurance serve as a foundation for understanding and contextualising our project's research and the recommendations we derive from our findings.</p>   <ol> <li> <p>Rather, it \"takes a village to raise an algorithm\" as noted in this blog post. Thanks to one of our reviewers for bringing this post to our attention.\u00a0\u21a9</p> </li> <li> <p>To be clear, we are referring here to role and task responsibilities first and foremost, which overlap with but are conceptually separate from moral responsibility.\u00a0\u21a9</p> </li> <li> <p>Updating can also occur in cases where no adaptive behaviour is present for reasons such as performance improvements or bug fixes.\u00a0\u21a9</p> </li> <li> <p>See Part 1 of the GSN Standard, Version 3 for a complete overview of the notation.\u00a0\u21a9</p> </li> <li> <p>Another alternative to GSN is the Claims, Arguments and Evidence notation developed by Adelard.\u00a0\u21a9</p> </li> <li> <p>For example, guidance such as ISO/IEC/IEEE 16085, 'Systems and software engineering - Life cycle management - Risk Management'\u00a0\u21a9</p> </li> <li> <p>The AI Standards Hub, for example, serves an observatory of relevant standards for AI technologies (see https://aistandardshub.org).\u00a0\u21a9</p> </li> <li> <p>For an example of this type of technology, see  Lucas et al. (2017). Reporting Mental Health Symptoms: Breaking Down Barriers to Care with Virtual Human Interviewers. Frontiers in Robotics and AI. https://www.frontiersin.org/articles/10.3389/frobt.2017.00051 \u21a9</p> </li> <li> <p>See Kirsch, A. (2017). Explain to whom? Putting the User in the Center of Explainable AI. Proceedings of the First International Workshop on Comprehensibility and Explanation in AI and ML 2017 Co-Located with 16th International Conference of the Italian Association for Artificial Intelligence (AI*IA 2017). https://hal.archives-ouvertes.fr/hal-01845135 \u21a9</p> </li> <li> <p>Recall, that evidential claims are required to link property claims to their supporting evidential artefacts, and evidential claims, therefore, can also serve as reasons.\u00a0\u21a9</p> </li> <li> <p>Ward, F. R., &amp; Habli, I. (2020). An Assurance Case Pattern for the Interpretability of Machine Learning in Safety-Critical Systems. https://doi.org/10.1007/978-3-030-55583-2_30 \u21a9</p> </li> <li> <p>Nickel, J. W. (2007) Making Sense of Human Rights (2nd Edition). Blackwell Publishing.\u00a0\u21a9</p> </li> <li> <p>The most recent and comprehensive account of this framework can be found in the following proposal to the Council of Europe: Leslie, D., Burr, C., Aitken, M., Katell, M., Briggs, M., &amp; Rincon, C. (2022). Human rights, democracy, and the rule of law assurance framework for AI systems: A proposal. https://rm.coe.int/huderaf-coe-final-1-2752-6741-5300-v-1/1680a3f688.\u00a0\u21a9</p> </li> <li> <p>For a contrasting approach, see Floridi, L., &amp; Cowls, J. (2019). A Unified Framework of Five Principles for AI in Society. Harvard Data Science Review. https://doi.org/10.1162/99608f92.8cd550d1 \u21a9</p> </li> <li> <p>This process is (loosely) derived from the idea of 'reflective equilibrium', made famous by the political philosopher John Rawls. In short, the phrase 'reflective equilibrium' refers to a state of coherence among moral beliefs and attitudes, which emerges over time as a result of public deliberation and consensus building activities that focus on the relevant moral beliefs and attitudes (e.g. notions of 'justice').\u00a0\u21a9</p> </li> <li> <p>Richard Hawkins, Colin Paterson, Chiara Picardi, Yan Jia, Radu Calinescu, and Ibrahim Habli. Guidance on the Assurance of Machine Learning in Autonomous Systems (AMLAS). Technical Report, University of York, Assuring Autonomy International Programme, March 2021. URL: https://www.york.ac.uk/media/assuring-autonomy/documents/AMLASv1.1.pdf (visited on 2022-09-08).\u00a0\u21a9</p> </li> <li> <p>The Assurance Case Working Group. GSN Community Standard Version 3. Technical Report, The Assurance Case Working Group, May 2021. URL: https://scsc.uk/r141C:1?t=1.\u00a0\u21a9</p> </li> </ol>","location":"dmh-report/chapter-2/#generalisable-patterns","tags":["lifecycle","assurance","SAFE-D","argument-patterns"]},{"title":"Applying Trustworthy Assurance\u2014Digital Mental Healthcare at UK Universities","text":"<p></p>  <p>Chapter Overview</p> <p>This section is the first of two sections that present findings from research conducted with stakeholders and/or affected users. Specifically, this section presents findings from research conducted on the application of trustworthy assurance to the procurement of DMHTs for use in the higher education (HE) sector.</p> <p>First, we provide an overview of the digital mental healthcare landscape at UK universities, detailing the pressures faced by university teams and the range of services currently on offer across the country.  </p> <p>Second, we present findings from a series of participatory engagements conducted with students and administrators at universities across the UK. We outline a series of contextual challenges to the ethical deployment of digital mental healthcare in higher education before exploring how the methodology of trustworthy assurance might be introduced in this sector to help tackle these challenges. For each challenge identified, future recommendations for the sector are provided.</p> <p>Broader lessons for the ethics of digital mental healthcare are also fed forward into Chapter 4 where further recommendations for policy-makers and developers are given.</p>","location":"dmh-report/chapter-3/","tags":["university","duty-of-care","students","higher-education"]},{"title":"The University Context","text":"<p>Prior to the onset of the Covid-19 pandemic, the suggestion that student mental health was in crisis across UK universities was already prominent.1 Media attention intensified around 2017, for instance, in response to a cluster of high-profile suicides.2 Since then, concern has only grown further, and focus has turned to whether the crisis has worsened due to the increased social isolation brought about by Covid-19 and remote learning.3 </p> <p>While media reports have been criticised for their simplistic focus on suicide figures as a metric for student wellbeing, research by The Office for Students has found that lengthy waiting times for counselling and a rise in help-seeking behaviour have both put an increased strain on services. All this has occurred during a period where the overall student population has grown.4</p> <p>Research continues to point to significant challenges facing university mental health services given factors such as the fivefold increase in students disclosing mental health conditions between 2007 and 20175 and the lack of capacity to address student concerns quickly. For instance, NUS research found that only one in six students received professional support within one week of reaching out.6</p> <p>In this high-pressure context, systemic changes have been proposed. Key policy documents, from the University Mental Health Charter to Universities UK Step Change Framework and IPPR\u2019s report on student mental health, have all called for a \u201cwhole university approach\u201d, for better cohesion between university departments, and for a greater focus on positive wellbeing. In addition to structural shifts, these recommendations have increasingly referenced \u2018digital interventions\u2019 as a key tool within the student mental health offering.7 </p> <p>Digital interventions are frequently seen as a \u201cnatural step\u201d for student mental health services as they expand beyond the traditional counselling model,8 while youth mental health is seen as a prime target for AI with the NHS AI strategy noting their first task with regard to mental wellbeing \u201cis to look into children and young people\u2019s mental health\u201d using AI-driven solutions.9 </p> <p>Amid these policy recommendations for modernisation, digital mental healthcare offerings designed for the general population have proliferated with The Office for Students suggesting 43,000 wellness and medical apps are now available for smartphone use.10</p> <p>It is no surprise, therefore, that these new tools have appealed to universities facing rising service demand alongside tight budgets, and that digital mental healthcare adoption has advanced at pace across UK universities.</p> <p>One way in which the growth in digital mental healthcare has been amplified is through universities\u2019 informal recommendations of external apps. For example, among the 24 Russel Group Universities in the UK, 15 recommend students try Headspace, eight point in the direction of Calm, and nine recommend SAM (Self-help App for the Mind), to name just a few of the apps listed on university webpages, typically under the heading of \u2018self-help resources\u2019.11 </p> <p>Another way in which digital mental healthcare has infiltrated the university sector is through the formal procurement by universities of digital mental healthcare tools designed with the specific needs of student populations in mind. Kotouza et al. have mapped these shifts and found that, as of 2021, \u201cmore than half of UK universities\u201d have a contract with at least one from SilverCloud and Togetherall while \u201cFika won over 35 UK university contracts between 2019 and 2020\u201d.12 </p> <p></p> <p>This rapid adoption of DMHTs may be seen as a perfect solution for the university sector, especially when the claims made by these digital service providers are taken at face value. For example, a co-founder of Fika has claimed that \u201cmental fitness approaches like Fika\u2019s present a new, far more sustainable solution to the natin\u2019s mental health needs\u201d,13  while Kooth describe their platform as a \u201csafe and confidential space to share experiences and gain support\u201d.14 </p> <p>However, it is important that universities are given the tools required to assess these claims themselves, and to do so using systematic, reliable, and well-validated procedures and standards. The University Mental Health Charter, for instance, emphasises that the \u201cneed for quality assurance extends to other interventions, such as the provision of digitally based services\u201d. Further guidance is required for universities to be able to carry out such quality assurance, and in particular for university teams to effectively assess these digital tools on the basis of their ethical implications. Here, we present findings to inform universities in doing such due diligence.</p>","location":"dmh-report/chapter-3/#the-university-context","tags":["university","duty-of-care","students","higher-education"]},{"title":"Workshop Information","text":"<p>To assess current challenges to the ethical and trustworthy deployment of digital mental technologies in a university context, we ran a series of participatory engagements to seek the feedback of university students and mental health teams.</p> <p>The findings presented below are based upon a series of participatory engagement events conducted by The Alan Turing Institute between January and March 2022. These consisted of semi-structured interviews with university administrators leading mental health teams at 10 universities across the UK (henceforth \u2018administrators\u2019) alongside research workshops attended by 25 students enrolled in undergraduate and postgraduate courses at UK universities (henceforth \u2018students\u2019).</p> <p>Table 3.1\u2014summary information about the two sets out workshops.</p>    Groups Purpose of workshop Type of Engagement Main activities     Administrators <ul><li> To identify which data-driven technologies or services are used by higher education institutions to support or deliver on their duty of care for students.</li> <li> To understand what metrics or properties are used to evaluate a data-driven technology or service prior to its procurement and deployment.</li> <li> To determine whether the administrators and students share the same goals and values when evaluating a service.</li> <li> To test the trustworthy assurance method and how it may be applied to specific case studies. </li></ul> Semi-Structured Interviews <ol><li> Exploring ethical principles and sharing current procurement approaches. <li> understanding duty of care. </li> <li> Assessing the trustworthy assurance methodology </li></li></ol>   Students <ul><li> To determine whether the administrators and students share the same goals and values when evaluating a service. </li> <li> To explore a set of illustrative case studies that were designed to support the development of trustworthy assurance cases and identify significant ethical principles and values. </li></ul> Group Workshops <ol><li> Which values and principles matter most to you? </li> <li> Evaluating case studies </li> </ol>","location":"dmh-report/chapter-3/#workshop-information","tags":["university","duty-of-care","students","higher-education"]},{"title":"Analysis and Contextual Challenges","text":"<p>Key findings from these engagements conducted with university administrators and students are presented as follows. First, we explore contextual challenges to the ethical deployment of DMHTs at universities, as raised by students and administrators. Second, we present findings on how the implementation of trustworthy assurance to this sector may resolve challenges. In each case, distinct perspectives from administrators as compared to students will be highlighted and conclusions for the future of ethical digital mental healthcare procurement summarised.</p> <p>As discussed in the Introduction, digital technologies raise a distinct set of ethical issues when deployed in a mental health context. For example, it is necessary to consider their impact on the therapeutic relationship and the privacy implications of using sensitive health data.15  Many of these ethical issues continue to be relevant at UK universities. However, the challenges posed by emerging DMHTs are specific to the context in which they are deployed. Therefore, this sub-project set out to determine HE sector-specific challenges to the trustworthy and ethical procurement and assurance of DMHTs.</p> <p>Contextual challenges, ranging from obstacles to transparency and accessibility to external pressures and institutional constraints, are summarised to inform key conclusions for the HE sector. </p>","location":"dmh-report/chapter-3/#analysis-and-contextual-challenges","tags":["university","duty-of-care","students","higher-education"]},{"title":"Challenge 1: Duty of care: a legal or ethical goal?","text":"<p>To determine which digital technologies to procure and provide, universities must first have a clear understanding of the overarching goal of their service. Defining this top-level goal also constitutes the first stage in developing a trustworthy assurance case. </p> <p>In a university context, defining this overarching goal introduces its own set of challenges. Universities are understood to have a duty of care to their students\u2019 mental health. This has been defined by AMOSSHE\u2014the Association of Managers Of Student Services in Higher Education\u2014as the duty of the institution to \u201cto deliver its educational and pastoral services to the standard of the ordinarily competent institution, and, in carrying out its services and functions, to act reasonably to protect the health, safety and welfare of its students.\u201d16 Therefore, as part of this sub-project we explored whether 'duty of care' could serve as a top-level normative goal for a trustworthy assurance case\u2014going beyond the SAFE-D principles.</p> <p>All digital technologies must support the university in delivering upon this duty. But, to assess whether a particular procurement choice is in-keeping with a university\u2019s responsibility, more detail is needed to translate duty of care into a fully specified goal.</p> <p>For university administrators, our engagement found that legalistic understandings of duty of care were common as references to the avoidance of negligence and to ensuring institutional competence and compliance were frequent. This is unsurprising, and as one noted that, \u201cif the worst outcome happens and you\u2019re in a coroner\u2019s court\u201d, you must be able to show that due diligence has been done and procedure has been followed. Moreover, students also made frequent references to the importance of legal mechanisms in holding institutions to account.</p>  <p>Administrator Perspective</p> <p>\u201cWe take the duty of care very seriously. And I think it is not the easiest thing to articulate. We talk regularly with our legal team about duty of care. If we are writing documents which reference the duty of care, we always talk with them about that.\u201d</p>   <p>Student Perspective</p> <p>\u201cIn terms of duty of care, that brings in things like the equalities act and various educational laws and policies so that universities aren\u2019t discriminating against students based on access or based on protected characteristics, and they are following all of the laws.\u201d </p>  <p>However, despite these legalistic framings, it is clear that the law does not provide a complete guide to action. AMOSSHE\u2019s account on duty of care clarifies these challenges, noting first that \u201cstudent law is still evolving\u201d, thus creating \u201cdifficulty in providing a legal definition of an institution\u2019s duty of care\u201d, and second, that \u201cthere is a balance between what the university should do as a legal minimum and what they could do based on a university\u2019s perceived moral obligation\u201d. These uncertainties can cause confusion for administrators.17</p>  <p>Administrator Perspective</p> <p>\u201cI think again it is such a problematic area for higher education institutions. I\u2019m not sure there is any clarity within the sector about that duty of care.\u201d</p>  <p>Ambiguities with regard to the nature and scope of a university\u2019s duty of care can be observed when universities must balance tensions between the preservation of student autonomy and the management of risk through forced intervention. Administrators raise the difficulty of these decisions on whether to intervene, illustrating uncertainty around duty of care.</p>  <p>Administrator Perspective</p> <p>\u201cThey are 18, they are adults. If they don\u2019t want our interventions, there is very little we can do. So, for us, it is balancing that fine line all the time\u201d</p> <p>\u201cI think we are at a very interesting period of somebody\u2019s life. The complications are, obviously, over-18 somebody will be in 99.9% of cases an adult. So, understanding the duty of care you have to\u2026 people as children or minors is very different from this new stage. But there does seem to by some muddying of the waters with this and it comes up a lot.\u201d</p>  <p>The everyday practices of a university mental health team must, therefore, go beyond the pursuit of compliance to grapple with ethical decisions on when specific interventions may or may not be appropriate. In short, legal understandings of duty of care must be supplemented by ethical values which guide the actions of teams in their selection of services.</p> <p>For administrators, the ethical values raised as key to duty of care include: \u2018inclusivity\u2019, \u2018putting the student first\u2019, \u2018consent\u2019, \u2018avoiding negative impact on the institution\u2019, \u2018best interest of every student\u2019, \u2018evidence-based decisions\u2019, \u2018benefit to all student groups\u2019 and \u2018value for money\u2019. For students, key ethical values raised include: \u2018transparency\u2019, \u2018inclusivity\u2019, \u2018anonymity\u2019, \u2018autonomy\u2019, \u2018privacy\u2019 and \u2018trust\u2019.</p> <p>While these ethical goals do show significant overlap, it is also clear to administrators that perspective is crucial to defining duty of care. First, different stakeholders define the goal of student mental health services differently. Second, perspectives on duty of care have shifted over time. Finally, the very introduction of digital services can influence who is seen to take responsibility for mental ill-health and so have an impact on how duty of care is understood.</p>  <p>Administrator Perspective</p> <p>\u201cParents think we\u2019ve got more responsibility than I think. Probably students sometimes think we\u2019ve got less responsibility\u201d</p> <p>\u201cI think we are heading towards in loco parentis. Towards having that responsibility that schools have. I think that\u2019s where the regulation is heading, if you look at things like the Student Mental Health Charter. All of the things are pushing us more and more that way.\u201d</p>   <p>Student Perspective</p> <p>\u201cDigitisation of mental health services may further the idea that mental health is an individual\u2019s issue and responsibility, rather than addressing the collective mental health and structural causal issues around wellbeing.\u201d</p>  <p>Continued work is needed to understand if and how duty of care can be translated into a fully specified goal for a university mental health team and how this goal can inform specific actions during the procurement of DMHTs. A participatory approach, considering the views of all impacted stakeholders, will be essential to clarifying the nature and scope of this responsibility.</p>  <p>Conclusions and Recommendations</p> <ul> <li>Universities should reflect on their duty of care in both legal and ethical terms.</li> <li>Shifting understandings and perspectival differences mean that a participatory approach to defining a top-level goal is crucial.</li> </ul>","location":"dmh-report/chapter-3/#challenge-1-duty-of-care-a-legal-or-ethical-goal","tags":["university","duty-of-care","students","higher-education"]},{"title":"Challenge 2: Organisational constraints: defining roles and responsibilities","text":"<p>Ambiguities surrounding the relative roles and responsibilities of different teams and institutions to protect student mental health have also at times hampered effective decision-making with regard to digital mental healthcare. Descriptions of the decision-making rationale provided by administrators suggest a procurement process which is frequently reactive rather than proactive, with a tendency to stick to the status quo or shift tack dramatically during a crisis.</p>  <p>Administrator Perspective</p> <p>\u201cThere was a general panic early March of \"oh my god, what is going to happen?\"\u201d</p> <p>\u201cQuick leaps. It wasn\u2019t particularly an evolution. We\u2019d been considering it [Togetherall] ourselves for a couple of years so it was on the horizon but that [the pandemic] was the thing that really pushed us to do that\u201d</p> <p>\u201cWith TogetherAll, a lot of universities have been offering it a long time, and there was a clinical trial, but anecdotally when you go on the mail base and talk to other heads of services and ask if their students are using it, they find students just aren\u2019t really using it. That\u2019s what we find. Paying a lot of money and people aren\u2019t using it. But they don\u2019t want to look bad or roll it back.\u201d</p>  <p>Reactive decisions are in part a consequence of the lack of clarity surrounding who is responsible for decisions on the procurement of DMHTs. First, there are significant ambiguities surrounding the relationship between university mental health services and the NHS. During engagements, administrators revealed that responsibility for procurement of digital services such as Togetherall resided in most instances with the university, but in certain cases with the NHS. In particular, administrators suggested that at times of crisis, where one institution was struggling to cope, the other had stepped in to fill a gap through purchasing a digital service. This dual responsibility can result in inconsistencies in who takes procurement decisions and in the long run could create a vacuum of responsibility if it is unclear where leadership lies.</p>  <p>Administrator Perspective</p> <p>\u201cThat is the million-dollar question of where our services stop and the NHS starts and the piece around filling that gap because there is definitely a gap between.\u201d</p>  <p>In addition, within the university, shifts towards a \u2018whole university approach\u2019 have occurred and were described by administrators. However, while greater involvement of departments such as financial services, accommodation, and examinations is taking place, less thought has been given to where responsibility for digital solutions may lie within this matrix and what capacity building is needed to develop the technical skills required to review digital technologies, monitor student use, and horizon scan for new directions. While procurement processes at many universities remain thorough in many respects, this results at times in decisions taken by clinical or legal teams, without dedicated attention to technical and ethical questions. In describing their procurement teams\u2019 top priorities when reviewing digital technologies, administrators raised both clinical evidence and value for money. Other concerns specific to data-driven technologies and their ethical implications did not feature as prominently, indicating potential gaps in data literacy.</p>  <p>Administrator Perspective</p> <p>\u201cWhen I pull them [mental health team] in to look at new apps and things, what you find is they are trained practitioners, they are counsellors, but their digital skills are not up there.\"</p> <p>\u201cI remember BigWhiteWall came around and did a slick presentation and then it was around the table who thinks this is a good idea. Rights, yeah, let's go for it.\u201d</p>  <p>Finally, as roles and responsibilities are clarified, a clear space for student involvement must be carved out. This was a priority raised by both students and administrators during engagements, but practical challenges must be addressed to involve all students and resolve possible tensions between student perspectives and clinical evidence. Additional challenges to student involvement in the procurement of digital technologies, such as the high entry costs sometimes associated with the evaluation of AI-driven tools, must also be addressed through education and collaboration between students and experts.</p>  <p>Administrator Perspective</p> <p>\u201cUniversities like to claim they are giving the students what they want but a lot of the time they are not really consulting students. We often have to convince HR and other systems to talk to students\u201d</p> <p>\u201cUniversities are often torn between do I go with the student voice or hard research. I will tell you which one they go with, student voice. The student voice doesn\u2019t have to be many, they just have to be loud. We are not talking about thousands of students having an impact, we are talking about vocal students or the students union\u201d</p>   <p>Student Perspective</p> <p>\u201cCreating dedicated spaces for students to voice their ideas and needs combined with evidence that these were somewhat taken on board. Utilising relationships with students and personal tutors/student support. Strong links with student societies and other student intermediaries.\u201d</p> <p>\u201cStudents need to be involved from the initial stages, so as to undercut serious biases working their way into the system. Furthermore, the biases within university cohorts should be examined \u2013 university intake of working class and state school students, POC, etc. is often not representative of the wider population, and technologies must make sure they are accessible to them as well, so that they are lasting into the future\u201d. </p>   <p>Conclusions and Recommendations</p> <ul> <li>Clarity is needed on the relative responsibilities of the NHS and university teams during the procurement of DMHTs.</li> <li>While university teams evolve to focus on interdepartmental cooperation, greater attention must be paid to assigning responsibility for digital offerings and ensuring technical skills have been developed.</li> <li>Engagements with administrators suggest that compliance with data protection laws is well established while there remains a gap in organisational readiness with regard to other methods for reviewing algorithmic techniques. Cross-domain communication may be essential for these review processes, potentially including IT and research departments. Consequently, designating responsibility for digital technologies should be considered as part of the shift to a \u2019whole university approach\u2019.</li> <li>Clarity on roles and responsibilities can also help designate responsibility for horizon scanning and proactive research rather than reactive decision-making.</li> <li>Meaningful student involvement is necessary and capacity building may be needed to facilitate meaningful contributions on technical topics.</li> </ul>","location":"dmh-report/chapter-3/#challenge-2-organisational-constraints-defining-roles-and-responsibilities","tags":["university","duty-of-care","students","higher-education"]},{"title":"Challenge 3: External Pressures","text":"<p>Greater clarity on responsibility for digital technologies within university mental health teams will only go part of the way to resolving current challenges. Significant pressures placed on mental health teams are outside of their control. In particular, concerns were raised throughout interviews and workshops surrounding the potential privatisation of student mental health.</p>  <p>Administrator Perspective</p> <p>\u201cWhile we are higher education institutions, we are businesses, and there is a conflict of interest with a business decision versus a clinically-driven, sound-evidence decision about what works and what doesn\u2019t.\u201d</p>   <p>Student Perspective</p> <p>\u201cFor-profits don\u2019t tend to do things out of the kindness of their own hearts. So how much are universities paying for this? Will the students have to pay? Will it be full of ads? Where is the profit coming from?\u201d</p>  <p>The relationships between universities and developers do suggest for-profit companies are having a significant influence on the procurement of DMHTs. The constant flood of digital offerings creates an environment where mental health teams struggle to ensure services are driven by student demand. Additionally, the nature of pitches received from developers often fail to provide the evidence or information necessary to evaluate services. Finally, the volume of products on the market has led to a dichotomy whereby administrators either spend significant time filtering through proposals or ignore them altogether.</p> <p></p>  <p>Administrator Perspective</p> <p>\u201cWhat I tend to get is lots of emails offering me a magic wand.\u201d</p> <p>\u201cThese apps are coming at you from every direction, and I must get at least three emails a week.\u201d</p> <p>\u201cThey [providers] are not telling me what I need to know.\u201d</p> <p>\u201cI am absolutely inundated with emails every day from providers of these things and I just ignore them.\u201d</p>  <p>These pressures which prevent services from being driven by student need are not only coming from private sector developers but from within the university sector itself. First, pressures from within an institution can lead to the adoption of a digital service on the basis of reputational concerns. Second, network pressures from comparisons with other universities can drive groupthink with regard to DMHTs.</p>  <p>Administrator Perspective</p> <p>\u201cI think it is about defensive practice as well. We just reviewed before the pandemic, we renewed BigWhiteWall, and I was quite keen to ditch and shift and look at alternatives. My boss at the time said I think we need to stay with it because it is a defensive action. If we get an FOI, it\u2019s good to say all students get offered this. It\u2019s worth 15k was the line I got, don\u2019t care whether it works or not.\u201d</p> <p>\u201cIf you want to question decisions, you\u2019re told \u2018all these other Russell Group universities are offering BigWhiteWall or free subscriptions to X, Y, X\u2019 so it seems that decisions are made more out of panic or what other institutions are doing because we need to look just as good.\u201d</p>   <p>Conclusions and Recommendations</p> <ul> <li>Universities must work together to leverage collective influence on developers such that developers provide them with more detailed information on the ethical implications of new technologies. The Trustworthy Assurance methodology provides one way of doing this (See Methodological Challenges).</li> <li>Networks such as AMOSSHE, the UK\u2019s Student Services Organisation, can play an important role in stepping forward to provide evidence-based guidance on these topics and help to prevent decisions being taken on the basis of reputational comparison. |</li> </ul>","location":"dmh-report/chapter-3/#challenge-3-external-pressures","tags":["university","duty-of-care","students","higher-education"]},{"title":"Challenge 4: Algorithmic aversion: diverted resources and prioritising human services","text":"<p>In a university context, where resources are constrained and digital solutions have been described as offering value for money, it is unsurprising that fears have been raised over technologies replacing in-person services, despite this going against the desires of both administrators and students. Determining when digital solutions are not appropriate, desirable, or morally permissible, therefore, constitutes a key challenge for the higher education sector. Administrators note that despite their intention to supplement human services with digital, there have been times where more could be done to avoid the diversion of resources to digital technologies.</p>  <p>Student Perspective</p> <p>\u201cWe all know that universities are oversubscribed and when you are offered something like this, you feel as if you are at the bottom of the list, and your mental health and what you say is all part of criteria as to whether you are \u201cbad enough\u201d or \u201cill enough\u201d for support. I think this is really damaging to someone\u2019s mental health and can worsen their state of mind.\u201d</p>   <p>Administrator Perspective</p> <p>\u201cWhat I never want is that we see digital services as an alternative to coming in and seeing our team\u201d</p> <p>\u201cWe could probably be doing better to supplement and complement rather than replace\u201d</p>  <p>Appeals to the importance of prioritising in-person services are significant. However, further specificity is required to determine how this can be achieved in a system where resources are constrained, and administrators inevitably face tough choices on where to direct funds. A more detailed understanding of students\u2019 and administrators\u2019 motivations for algorithmic aversion is necessary, therefore, to determine when and how digital technologies should be delivered. For administrators, concerns over clinical efficacy and the management of risks on platforms, as well as a lack of demand from students, formed the primary motivations for algorithmic aversion. In contrast, students focused on concerns around the dehumanisation of mental health, the lack of empathy offered by technology and the potential exacerbation of social isolation by digital offerings. Further research into these and further motivations for students\u2019 and administrators\u2019 algorithmic aversion is essential if student mental health services are to effectively deploy digital technologies.</p>  <p>Administrator Perspective</p> <p>\u201cWhat we want to know is has it been evaluated, what\u2019s the research that sits behind this and they [my team] have about zero tolerance of anything that just looks nice or doesn\u2019t have that really robust background\u201d</p> <p>\u201cStudents don\u2019t want more apps and more digital interventions. They want a small group of evidence-based digital interventions that focus on positive outcomes, and they want to see staff face-to-face.\u201d</p>   <p>Student Perspective</p> <p>\u201cThe dehumanisation of mental health. If students and staff come to see mental health as something solved by apps, the very complex nature of mental health can be undermined. We cannot automate mental health and wellbeing.\u201d</p> <p>\u201cAs someone who has gone to therapy and other support groups, it is very beneficial to do the work and even asking for help and seeking out support and turning up to someone in person is very nerve-wracking and that part of the process is so beneficial to mental health as it helps you face some anxieties\u201d</p> <p>\u201cThere is a certain level of isolation and problematic self-sufficiency that could be encouraged by directing users towards certain technologies.\u201d</p>  <p>Taking these motivations into account can help to reveal a more complex picture where students and administrators oppose digital technologies for specific reasons and in specific circumstances. This detail is necessary to guide administrators in choosing when and how to deploy digital technologies, and when to stick with in-person services.</p>  <p>Administrator Perspective</p> <p>\u201cI think there\u2019s slight value around students that have got really low-level concerns. So, sleep, procrastination, and all those things. I think some services offer great self-diagnosis. And then of course around how to do some of those things which actually, yeah, there is probably a benefits of being able to do that on your phone in the warmth of your own flat or whatever, without having to come in and see one of my advisors who might tell you exactly the same thing. So those are the two areas I see it. I don\u2019t ever want us to get to a point where we see it as a solution to replace the ability to come in and see somebody, particularly for more risky students or students that are struggling.\u201d</p>   <p>Conclusions and Recommendations</p> <ul> <li>In an environment where resources are constrained, careful consideration is needed over whether the decision to procure a technology is justified. Where resources are limited, justification of expenditure must go beyond claims that digital services do not replace in-person support to make transparent where funds come from and why the benefits of a digital service are seen to outweigh other options. This may also present a valuable opportunity to reassess the efficacy of traditional in-person mental health services.</li> <li>Administrators\u2019 and students\u2019 reasons for preferring in-person services appear to differ. Consultation of multiple student and staff groups is, therefore, essential to determining when and why digital solutions may be inappropriate.|</li> </ul>","location":"dmh-report/chapter-3/#challenge-4-algorithmic-aversion-diverted-resources-and-prioritising-human-services","tags":["university","duty-of-care","students","higher-education"]},{"title":"Challenge 5: Accessibility and fairness","text":"<p>In light of these varied motivations for aversion to digital solutions, it is clear that a more thorough understanding of different students\u2019 needs is essential. During engagements, key concerns were raised surrounding accessibility and fairness in digital mental healthcare, as many participants proposed a more tailored approach was needed. While increased accessibility of mental health services was described by both students and administrators as a primary advantage of digital technologies, a one size fits all approach has meant that access to effective care has not been improved for all groups.</p>  <p>Administrator Perspective</p> <p>\u201cThere are certain student groups who would not like to go to one-to-one therapy where something online or an app or self-help, something more empowering that they can do in their own time. Something like that suits certain student groups. Broadly, with the little bits of research we have done, before the pandemic, it suits groups with social anxiety.\u201d</p> <p>\u201cIt is assumed all students want the same thing, so we are just going to give them big white wall. Even if it is not being supported in any other way. It is just assumed students will have the motivation to use this app. But not all students are appropriate for self-help. You find even outside of technology, not everyone is suited to CBT.\u201d</p>   <p>Student Perspective</p> <p>\u201cIt [technology] can be more accessible, particularly for those with anxiety or mobility issues.\u201d</p> <p>\u201cProviding accessible tools for \u2018basic\u2019 needs (e.g. productivity tools, which are not too concerning and can be widely used).\u201d</p>  <p>These limitations of digital technologies\u2019 ability to improve accessibility need not be a reason against their deployment. In some instances, the ability to reach different groups, such as those who feel the stigma of reaching out in person, can be seen as a positive. However, to ensure accessibility is prioritised across the student population, a greater awareness of who is (and who is not) served by digital technologies is required, alongside an in-depth understanding of which student needs are currently unmet by in-person services. During engagements, some administrators suggested males were engaging more with digital technologies, some pointed to females, others to differences in engagement across academic disciplines and finally some noticed no patterns of engagement at all. Further research into this is needed for universities to move away from a one-size-fits all approach and design solutions which explicitly address the needs of all student groups.</p>  <p>Administrator Perspective</p> <p>\u201cActually, if there was more work involving different student groups and not assuming every student is a first-year middle class white academic undergraduate and there are other groups that can benefit as well, that\u2019s where we can really learn.\u201d</p>  <p>For students, concerns about fairness and health equity were significant. First, concerns were raised around digital poverty and the importance of challenging assumptions frequently made about student populations (e.g., that all students have easy access to technology). Second, students expressed worries about biases within these technologies and how algorithmic bias can impact their experiences. Here, students advocated for specific consideration of minority student groups with the possible designation of \u2018safe spaces\u2019 on such platforms based on protected characteristics or allowing students to actively choose who they were speaking to in order to avoid potential bullying or harassment.</p>  <p>Student Perspective</p> <p>\u201cDigital poverty is a thing. Just because you are at university, that doesn\u2019t mean you have a smartphone, a laptop, or even a private space where you can access the platform\u201d</p> <p>I hate to be devil\u2019s advocate, but I don\u2019t think everybody has access to devices. I do get the point around accessibility, but I think companies or universities also need to be offering the support which will lead to them accessing these services and closing the digital poverty gap.\u201d</p> <p>\u201cInformation can be taken out of context and the cultural nuances with people from a range of backgrounds can be missed\u201d</p>  <p>During discussions on the accessibility and fairness of digital technologies, proposals were made by students for accessibility to be understood in more subtle ways whereby digital solutions are not simply rolled out across the student population, but instead tailored to students with specific needs. It was also proposed that humans should mediate access to technology where possible so that a student can be matched with a service that helps them at the level they are ready for. Finally, empowerment of student choice was prioritised by participants in order to ensure students have control over the care they receive.</p>  <p>Conclusions and Recommendations</p> <ul> <li>Greater integration with research departments is needed to ensure evidence is available on who is best served by specific mental health services. Universities can draw on their own research resources in order to take a deeper look at who is and is not benefiting from digital interventions.</li> <li>Resource allocation within student services should take account of issues such as digital poverty, especially in light of the rise in remote learning following the pandemic.</li> <li>Algorithmic bias is a pervasive issue beyond the mental health sector and one which administrators must be aware of so that thorough questions can be asked of service providers in advance of service roll out.18</li> <li>A balance must be struck between empowering students to make their own choices without placing the burden of responsibility for mental health on them.</li> </ul>","location":"dmh-report/chapter-3/#challenge-5-accessibility-and-fairness","tags":["university","duty-of-care","students","higher-education"]},{"title":"Challenge 6: Transparency and communication","text":"<p>Both students and administrators raised the importance of transparent and accessible communication about what services are available to students. Both groups also recognised that there is a lack of cut-through in current communication strategies leading to students being unaware of which services are on offer at their institutions.</p>  <p>Administrator Perspective</p> <p>\u201c[Communications] can be quite difficult and especially around this sort of issue, around support. Through the pandemic, the availability of support has probably increased accessibility because a lot of it\u2019s been online. And yet you still come across students who say, \u201cthere is no support, I don\u2019t know where to find it\u201d. And you kind of think, well there was a student newsletter, there\u2019s all these social media posts across multiple channels, the student union that talked their language are talking about this stuff and TikTok is messaging about this, but still you haven\u2019t picked it up? So, it is quite a challenging environment.\u201d</p>   <p>Student Perspective</p> <p>\u201cI do not have adequate awareness of what is available\u201d</p> <p>\u201cI think they should promote them more so I would actually know what is available\u201d</p> <p>\u201cI know that I get it a lot at the bottom of an email or a newsletter that I don\u2019t tend to read and I know that lots of people don\u2019t read\u201d</p>  <p>However, among students, the need for more awareness on what services are available was not the first priority regarding transparency and communication. Rather than focus simply on advertising services to students through multiple channels, students emphasised the importance of universities communicating the limitations and personal costs associated with these technologies to them, such as the loss of privacy or limited clinical efficacy. Such transparency is also emphasised as important to facilitating informed consent. </p>  <p>Student Perspective</p> <p>\u201cThere is an asymmetry of knowledge as users don\u2019t know/consent to their data being used for these purposes. The business model is not communicated to the user and is exploitative\u201d</p> <p>\u201cAll these policies will say things about sharing information or data with \u2018trusted individuals\u2019, \u2018trusted organisations\u2019, \u2018trusted researchers\u2019, but there is nothing about what makes these organisations trusted. And you can\u2019t expect somebody in crisis to actually pour through all the terms and conditions.\u201d</p>   <p>Conclusions and Recommendations</p> <ul> <li>To ensure students are aware of the services available, communications should be delivered by as many stakeholders as possible to include academic staff, student unions and student societies. These communications should also incorporate considerations around accessibility (e.g. alternative formats).</li> <li>University mental health teams should be careful to communicate honestly with students about both benefits and risks of technologies, including transparency communications about efficacy and data sharing.</li> <li>While in-depth and legally binding privacy policies are essential, an accessible breakdown of key information (e.g. through FAQs or video messages) is crucial to \u2018informed\u2019 consent.</li> </ul>","location":"dmh-report/chapter-3/#challenge-6-transparency-and-communication","tags":["university","duty-of-care","students","higher-education"]},{"title":"Methodological Challenges","text":"<p>Overall, feedback from administrators and students suggests there is a long way to go to ensure responsible digital mental healthcare innovation and procurement across the UK HE sector. Progress can be made by following key recommendations set out here for universities. However, significant challenges will remain so long as changes do not take place elsewhere in the digital mental healthcare ecosystem. Systemic changes from developers, policy-makers, and university leadership teams are required to leverage the collective power required to foster a responsible innovation landscape.</p> <p>The trustworthy assurance methodology provides one route through which such a culture of responsible innovation and transparent communication could be facilitated so that university stakeholders are given all necessary information to review new mental health tools offered to them. Due to the potential of this methodology in this sector we asked administrators to provide feedback on its deployment. Crucially, feedback indicated that this methodology would fit within current procurement practices, provide structure for the critical assessment of developers\u2019 claims, and encourage more transparent communication among university staff about the rationale behind procurement decisions.</p>  <p>Administrator Perspective</p> <p>\u201cMy initial reaction is it's bloody brilliant. I\u2019ve not seen anything like that and if something like that landed in my lap I would seize upon it.\u201d</p> <p>\u201cWhat I\u2019m looking for is a really quick way of working out whether this is something that\u2019s worth looking at or not\u201d</p> <p>\u201cPeople like me are not naturally going to be flowchart kind of system development people. So, something like that [the assurance methodology], that can help us ask those questions and take these people through their paces, because what we get is very slick presentations, would be very helpful\u201d</p> <p>\u201cI think it would be really helpful to have a standard set of expectations that, you know\u2026 a standard expectation of having to demonstrate what they were trying to achieve.\u201d</p> <p>Nevertheless, several challenges to the effective deployment of trustworthy assurance, as raised by university administrators and students, must first be addressed. These are set out below in order to inform conclusions in Section 3 for the future of the assurance methodology.</p>","location":"dmh-report/chapter-3/#methodological-challenges","tags":["university","duty-of-care","students","higher-education"]},{"title":"Challenge 7: Interpreting assurance cases","text":"<p>Trustworthy assurance cases (as set out in Chapter 2) can appear complex. This introduces two key challenges for administrators tasked with assessing whether a particular assurance case meets their requirements for the procurement of a new DMHT.</p> <p>First, technical skills are required to assess such cases and ensure evidence matches with property claims. Second, assurance cases cannot be treated in isolation and must often be evaluated alongside clinical standards. It is important to avoid treating clinical efficacy and ethical standards as equivalent. The assurance exercise is not intended to provide a universal stamp of approval that ethical concerns have been addressed. Instead, it will require contextual interpretation on whether the evidence provided by a developer is sufficient for deployment in a specific context.</p> <p>As a result, despite placing the greatest burden of responsibility on developers, this methodology does require administrators, as key decision-makers in the HE sector, to have capacity to effectively review assurance cases. Policy-makers must, therefore, take on responsibility for ensuring the necessary knowledge and organisational readiness to assess digital technologies is provided to universities. This could be offered through consultation with national bodies such as AMOSSHE, The Office for Students or Universities UK or through dedicating resources to capacity building within each higher education institution.</p>","location":"dmh-report/chapter-3/#challenge-7-interpreting-assurance-cases","tags":["university","duty-of-care","students","higher-education"]},{"title":"Challenge 8: Uptake by developers","text":"<p>To obtain necessary evidence and compile it into an assurance case requires time and resources on the side of developers. A key obstacle to the deployment of trustworthy assurance therefore rests upon the motivation of developers to do this due diligence. Currently, such motivation will be minimal as standards across the sector place few demands on developers.</p> <p>Nevertheless, the collective purchasing power of universities across the UK is significant and can be leveraged in order to place further requirements on developers. This may be done through standards setting at the national level, for example through networks such as AMOSSHE. Additional pressures can also be placed on university networks by organisations such as the National Union of Students whose collective voice can influence procurement decisions.</p> <p>In setting high standards for developers, there must also be efforts not to exclude smaller mental health providers from having the resources to produce assurance cases. For this reason, the burden placed on developers may be reduced by sharing best practices or building repositories of publicly accessible assurance cases and argument patterns which can be used as a starting point for developers as they embark on the ethical reflection process. These routes towards cross-sector best practices are discussed in greater depth in Chapter 5.</p>","location":"dmh-report/chapter-3/#challenge-8-uptake-by-developers","tags":["university","duty-of-care","students","higher-education"]},{"title":"Challenge 9: Use as a communication tool","text":"<p>Finally, in a university context, concerns were raised regarding the utility of trustworthy assurance in communicating to students that due diligence has been done and ethical implications have been considered.</p> <p>In conversation with the students, we discussed whether there was perceived value in administrators sharing assurance cases directly with the student body. Although there was no negative response, concerns were addressed about the complexity of assurance cases. Similar concerns were raised by administrators. In brief, stakeholders thought assurance cases represented too much information for effective communication, and that a summary would be preferable. Access to the full case could be made available for any students who were interested in order to facilitate both accessible communication and full transparency.</p>   <ol> <li> <p>Shackle, S. (2019. \u2018The way universities are run is making us ill\u2019: inside the student mental health crisis. The Guardian. https://www.theguardian.com/society/2019/sep/27/anxiety-mental-breakdowns-depression-uk-students \u21a9</p> </li> <li> <p>Kotouza, D., Callard, F., Garnett, P., &amp; Rocha, L. (2022). Mapping mental health and the UK university sector: Networks, markets, data. Critical Social. Policy, 42(3), pp.356-387 https://journals.sagepub.com/doi/full/10.1177/02610183211024820 \u21a9</p> </li> <li> <p>Williams, T. (2022, May 9). UK student mental health crisis \u2018may be worse than thought\u2019. Times Higher Education. https://www.timeshighereducation.com/news/uk-student-mental-health-crisis-may-be-worse-thought; https://www.if.org.uk/2021/03/11/new-ons-data-show-student-mental-health-crisis/ \u21a9</p> </li> <li> <p>Office for Students (2019) Mental Health: Are all students being properly supported? https://www.officeforstudents.org.uk/media/b3e6669e-5337-4caa-9553-049b3e8e7803/insight-brief-mental-health-are-all-students-being-properly-supported.pdf; Duffy, A., Saunders, K.E.A, Malhi, G.S., Patten S., Cipriani, A., McNevin, S. H., MacDonald, E., &amp; Geddes, J. (2019) Mental health care for students: A way forward? The Lancet Psychiatry, 6(11). https://www.thelancet.com/journals/lanpsy/article/PIIS2215-0366(19)30275-5/fulltext \u21a9</p> </li> <li> <p>Gunnell, D., Kidger, J., &amp; Elvidge, H. (2018). Adolescent mental health in crisis. BMJ 361. https://www.bmj.com/content/361/bmj.k2608.long \u21a9</p> </li> <li> <p>National Union of Students Northern Ireland. (2017). NUS-USI Student Wellbeing Research Report, https://nusdigital.s3-eu-west-1.amazonaws.com/document/documents/33436/59301ace47d6320274509b83e1bea53e/NUSUSI_Student_Wellbeing_Research_Report.pdf \u21a9</p> </li> <li> <p>Thorley, C., (2017). Not by degrees: Improving student mental health in the UK\u2019s universities, Universities UK (2020). Stepchange: Mentally healthy universities., Hughes, G., &amp; Spanner, L. (2019). The University Mental Health Charter.\u00a0\u21a9</p> </li> <li> <p>Cate, N. (2021, August 16). The role of digital mental health support tools and the importance of the student co-productin model in supporting their development. Office for Students. https://www.officeforstudents.org.uk/advice-and-guidance/student-wellbeing-and-protection/student-mental-health/the-role-of-digital-mental-health-support-tools-and-the-importance-of-the-student-co-production-model-in-supporting-their-development/ \u21a9</p> </li> <li> <p>NHS X (2019). Artificial Intelligence: How to get it right? https://www.nhsx.nhs.uk/media/documents/NHSX_AI_report.pdf \u21a9</p> </li> <li> <p>Cate, N. (2021, August 16). The role of digital mental health support tools and the importance of the student co-productin model in supporting their development. Office for Students. https://www.officeforstudents.org.uk/advice-and-guidance/student-wellbeing-and-protection/student-mental-health/the-role-of-digital-mental-health-support-tools-and-the-importance-of-the-student-co-production-model-in-supporting-their-development/ \u21a9</p> </li> <li> <p>In these cases, a university\u2019s recommendation of a service does not indicate that any formal relationship exists with the service in question. At times these recommendations for self-help apps are accompanied by a disclaimer. For example, the University of Durham notes that \u201cthe counselling service is happy to signpost this information/the availability of free access to these resources, in the hope they might prove useful to students and staff. However, please be aware that the Counselling Service does not have any relationship or affiliation with any of the providers, nor does it support, or endorse in any way the external information, advice, other services and/or resources that can be accessed via the links below (https://www.dur.ac.uk/counselling.service/self-help/). In other instances ,they are simply offered as resources for students to browse at their own discretion.\u00a0\u21a9</p> </li> <li> <p>Kotouza, D., Callard, F., Garnett, P., &amp; Rocha, L. (2022). Mapping mental health and the UK university sector: Networks, markets, data. Critical Social. Policy, 42(3), pp.356-387 https://journals.sagepub.com/doi/full/10.1177/02610183211024820 \u21a9</p> </li> <li> <p>Bennett, N. (2021, February 15) Rebuilding post-16 education around mental fitness. Fika community. https://www.fika.community/insight/rebuilding-post-16-education-around-mental-fitness \u21a9</p> </li> <li> <p>https://student.kooth.com \u21a9</p> </li> <li> <p>Nuffield Council on Bioethics. (2022). The role of technology in mental healthcare. https://www.nuffieldbioethics.org/assets/pdfs/The-role-of-technology-in-mental-healthcare.pdf \u21a9</p> </li> <li> <p>https://www.amosshe.org.uk/futures-duty-of-care-2015 \u21a9</p> </li> <li> <p>AMOSSHE. (2015). Where\u2019s the line? How far should universities go in providing duty of care for their students. https://www.amosshe.org.uk/futures-duty-of-care-2015 \u21a9</p> </li> <li> <p>There is a huge and varied literature on 'algorithmic bias', and so pinning down a single definition of the term is challenging. In other works we have instead focused on exploring three types of bias\u2014social, statistical, and cognitive, which can impact the design, development, and deployment of ML and AI technologies. See here for further information.\u00a0\u21a9</p> </li> </ol>","location":"dmh-report/chapter-3/#challenge-9-use-as-a-communication-tool","tags":["university","duty-of-care","students","higher-education"]},{"title":"Co-Designing Trustworthy Assurance\u2014Stakeholder Engagement","text":"<p></p>  <p>Chapter Overview</p> <p>This section introduces and analyses the findings of several stakeholder engagement events, which were conducted to a) identify participant's attitudes towards DMHTs, b) understand which ethical values and principles they view as significant, and c) explore how to use this information to construct trustworthy assurance cases and argument patterns for relevant ethical goals.</p> <p>First, we introduce the objectives, structure, and content of the workshops.</p> <p>Second, we analyse the findings of our workshops, drawing connections with the methodology of trustworthy assurance. These findings support the development of two argument patterns, presented in chapter 5, which serve to distill recurring themes from all of workshop participants that were deemed significant.</p> <p>Finally, we offer several recommendations for policy-makers, regulators, and developers, based on the preliminary results of the project.</p>","location":"dmh-report/chapter-4/","tags":["workshops","users","regulators","developers","researchers","participatory-design"]},{"title":"Workshop Information","text":"","location":"dmh-report/chapter-4/#workshop-information","tags":["workshops","users","regulators","developers","researchers","participatory-design"]},{"title":"Participants","text":"<p>In the previous chapter we discussed our engagement with university students and administrators. This work was treated as a sub-project because of the specific focus on the HE sector as a limiting context. In this chapter we address workshops with a wider range of stakeholders and from a broader perspective. While the objectives remain the same across these two chapters, the findings and analysis in this section are representative of a wider range of concerns.1</p> <p>The stakeholder groups we consider in this chapter are as follows:</p> <ol> <li>Policy-makers and regulators in healthcare</li> <li>Developers of DMHTs</li> <li>Researchers working in disciplines adjacent to digital mental healthcare</li> <li>Users with lived experience of DMHTs</li> </ol> <p>Representatives from the first three stakeholder groups were invited to participate in a series of two workshops, the first of which laid the foundation for a subsequent participatory design workshop.</p> <p>In contrast, users of DMHTs (4) were invited to a separate workshop (offered either online or in-person), which was organised with and facilitated by the McPin Foundation\u2014a mental health research charity that provide advice and support on research strategies to involve participation and expertise from individuals with lived experience of mental health issues. This decision was made to ensure that participants were fully supported by experts throughout the workshops, and that our analysis of the findings was further supported by domain experts.</p>","location":"dmh-report/chapter-4/#participants","tags":["workshops","users","regulators","developers","researchers","participatory-design"]},{"title":"Methodology and Activities","text":"<p>Full details of our methodology and activities are provided in Appendix 1. Summary information is included in Table 4.1.</p> <p>Table 4.1\u2014summary information about the two sets out workshops.</p>    Workshop Groups Purpose of workshop Main activities     1a <code>Policy-makers and regulators</code>, <code>Developers</code>, <code>Researchers</code> - To introduce participants to the methodology of trustworthy assurance - To identify key ethical values and principles that were salient or significant in the evaluation of digital mental healthcare - Introductory presentations on a) the current landscape of digital mental healthcare, including representative harms and benefits, and b) the methodology and purpose of trustworthy assurance - Group discussion exploring the ethical values and principles associated with the design, development, and deployment of DMHTs, using case studies developed by our team   1b <code>Policy-makers and regulators</code>, <code>Developers</code>, <code>Researchers</code> - To explore a set of illustrative case studies that were designed to support the development of trustworthy assurance cases - To build trustworthy assurance cases using a prototype platform developed for this purpose - A group discussion of the chosen case study (voted for by participants in the previous workshop) to ensure familiarity with the relevant details of the case study - A participatory design activity in which the participants collectively develop an assurance case for a specific ethical value or principle (e.g. health equity, explainable decisions)   2 <code>Users of DMHTs</code> (in-person; online) - To identify participants attitudes towards digital mental healthcare in general, and salient ethical issues more specifically. - Exploratory discussion on the possible harms and benefits of digital mental healthcare. - Identification of key ethical values and principles. - Evaluation of sample claims made by a hypothetical team about actions or decisions undertaken during the design, development, and deployment of DMHTs.","location":"dmh-report/chapter-4/#methodology-and-activities","tags":["workshops","users","regulators","developers","researchers","participatory-design"]},{"title":"Analysis","text":"<p>Key Findings</p> <ul> <li>All groups emphasised fairness as a key ethical principle, but the specifics of how fairness was understood differed between groups.</li> <li>Additional emphasis was placed on ethical priorities that could be captured by either the accountability, explainability, or data SAFE-D principles (e.g. informed consent, transparency).</li> <li>Goals that are not directly coupled to any specific ethical principle2, such as clinical efficacy, were nevertheless significant topics for consideration among regulators and developers.</li> <li>Ensuring sufficient understanding of the trustworthy assurance methodology proved to be challenging in the time available. This was the case even with the participants who attended two workshops, where the first included preliminary material on the methodology. </li> </ul>  <p>As with the workshops described in the previous section, we conducted thematic analysis on the findings from the workshops, with the goal of addressing the objectives set out in the Introduction.3</p> <p>In the following sections, we first discuss the specific themes for each set of workshops and then explore cross-cutting themes and differences, expanding on the <code>Key Findings</code> section above.</p>","location":"dmh-report/chapter-4/#analysis","tags":["workshops","users","regulators","developers","researchers","participatory-design"]},{"title":"Workshops (1a and 1b) with policy-makers and regulators, developers, and researchers","text":"<p>Summary</p> <ul> <li>Nearly all of the ethical issues raised could be easily captured by the SAFE-D principles and their core attributes. However, additional space and emphasis is needed to capture the following concepts: <code>choice</code>, <code>patient choice</code>, <code>self-determination</code>, <code>autonomy</code>.</li> <li>Fairness was prioritised by the majority of participants. The principles was strongly linked to considerations such as <code>access to services</code>, <code>unequal distribution of health outcomes across demographic groups</code>, <code>bias in algorithmic decision-making</code>, and <code>diverse and inclusive participation in service design</code>.</li> <li>Participants expressed positive sentiment towards the trustworthy assurance, noting its perceived value for processes such as transparent auditing, assessment, or procurement.</li> <li>Producing assurance cases was a challenging exercise for many, but there were no signs that these barriers could not be addressed with additional user guidance and familiarity.</li> </ul>","location":"dmh-report/chapter-4/#workshops-1a-and-1b-with-policy-makers-and-regulators-developers-and-researchers","tags":["workshops","users","regulators","developers","researchers","participatory-design"]},{"title":"Workshop 1a","text":"<p>The first workshop (1a) ensured that participants had sufficient information about the trustworthy assurance methodology, which was required for the second workshop. This information was provided while minimising the likelihood of priming the participants to evaluate our case studies with reference to specific ethical values or principles, such as the SAFE-D principles. Therefore, there were fewer findings from workshop 1a than with workshop 1b.</p>  <p>Note</p> <p>Our goal was to identify which ethical principles mattered most to them, so we were careful not to highlight that we had already developed an existing framework (SAFE-D principles) that could unduly influence their feedback.</p>  <p>However, one relevant activity from workshop 1a that is worth mentioning was the explicit request to identify and discuss ethical values and principles that were seen as salient or significant in the context of the design, development, and deployment of trustworthy DMHTs.</p> <p>The following word cloud shows participant answers for the question,</p>  <p>Question</p> <p>'What values and principles matter to you?'</p>  <p></p> <p>Figure 4.1: a word cloud displaying answers to the question, 'What values and principles matter to you?'</p> <p>Immediately, it can be seen that <code>transparency</code>, <code>privacy</code>, and <code>evidence-based</code> were clearly significant for our participants. And related concepts, such as <code>accountability</code>, <code>explainability</code>, and <code>clarity</code> are also salient.</p> <p>However, there is also a wide variety of terms here, and many are either synonymous or closely related. For instance, <code>self-determination</code>, <code>autonomy</code>, <code>informed consent</code>, <code>control of own data</code>, and <code>patient choice</code> could be clustered together. And so could <code>equity</code>, <code>fairness</code>, <code>equality</code>, and <code>equity of access</code>.</p> <p>Although intended as an exploratory and preliminary activity, the findings represent a useful source of contextual information that can help illuminate some of the themes that emerged during the workshop discussion and activities. For instance, the vast majority of the concepts map onto our existing framework, and overlap with the SAFE-D principles and corresponding attributes (see Table 3.2)4.</p> <p>Table 4.2\u2014mapping the word cloud concepts onto the SAFE-D principles.</p>    Principle Concept     Sustainability <code>evidence-based</code>, <code>fit for purpose</code>, <code>safeguarding</code>, <code>monitoring</code>, <code>cost effective</code>, <code>follow up</code>, <code>redress</code>, <code>safety</code>, <code>usefulness</code>, <code>impact</code>   Accountability <code>transparency</code>, <code>safeguarding</code>, <code>accountability</code>, <code>expert led</code>, <code>regulated</code>, <code>monitoring</code>, <code>honesty</code>, <code>redress</code>, <code>monitored closely</code>   Fairness <code>accessible/accessibility</code>, <code>fairness</code>, <code>co-designed</code>, <code>compassion</code>, <code>bias</code>, <code>equity</code>, <code>co-produced</code>, <code>diversity</code>, <code>equality</code>, <code>equality of access</code>   Explainability <code>transparency</code>, <code>evidence-based</code>, <code>clarity</code>, <code>accessible</code>, <code>honesty</code> <code>explainability</code>, <code>monitored closely</code>   Data Quality, Integrity, Privacy and Protection <code>privacy</code>, <code>control of own data</code>, <code>regulated</code>, <code>honesty</code>, <code>usefulness</code>, <code>safe and secure</code>, <code>confidentiality</code>    <p>Two considerations can be extracted from this mapping:</p> <ol> <li>The SAFE-D principles provide an informative starting point for ethical reflection and deliberation in digital mental healthcare, as they do in other domains, and would likely serve as useful normative goals for trustworthy assurance cases.</li> <li>There are gaps and nuances in the framework when applied to digital mental healthcare that need to be addressed.</li> </ol> <p>In terms of the second consideration, there are a few clarifications that need to be made.</p> <p>Firstly, the main gap relates to the ability for the principles to capture concepts such as,<code>choice</code>, <code>patient choice</code>, <code>self-determination</code>, and <code>autonomy</code>. The appearance of these concepts is not surprising. Patient autonomy, informed consent, and participatory decision-making in healthcare are longstanding ethical values, and are reflected in well-known bioethical principles.5</p> <p>In the original domain-general setting in which the SAFE-D principles were designed, informed consent and autonomous decision-making were captured under principles such as fairness and explainability (e.g. ensuring that information about an algorithmic decision is accessible and explainable to users). However, as we will see shortly, there are nuances in the design, development, and deployment of DMHTs that put pressure on the choice of subsuming these values into another principle. </p> <p>Secondly, there are other principles, such as <code>human-centred</code>, <code>human rights</code>, <code>honesty</code> and <code>closed loop systems</code> that are either ambiguous or do not fit cleanly into the existing framework. In the context of the first two, this is simply because they stand outside of the SAFE-D principles as meta-frameworks (e.g. human rights law). For instance, the SAFE-D principles have been put forward as a means for safeguarding human rights6. However, in the case of <code>honesty</code> and <code>closed loop systems</code>, there was simply insufficient information during discussion to determine whether these are merely outliers or express an existing attribute that fits within the framework.</p> <p>Fortunately, the activities and discussion from the second workshop help emphasise more salient topics that were deemed significant by the stakeholders.</p>","location":"dmh-report/chapter-4/#workshop-1a","tags":["workshops","users","regulators","developers","researchers","participatory-design"]},{"title":"Workshop 1b","text":"<p>The second workshop (1b) focused on a participatory design activity that was created to evaluate the trustworthy assurance methodology and attempt to operationalise some of the ethical principles explored in the first workshop.</p> <p>For the main activity, participants were asked to review and discuss the ethical issues related to a specific case study and then develop a hypothetical assurance case that communicated how a set of decisions or actions had been undertaken to justify the ethical goals and properties that they had discussed. The groups were free to choose the goal. And the case study, which had also been selected by participants, involved the use of a decision support system that offered tailored and real-time recommendations to a psychiatrist during consultation with a patient (e.g. during assessment)(see Appendix 1).</p> <p>Two breakout groups were formed and the (incomplete) assurance cases are depicted below.</p> <p></p> <p>Figure 4.2: the assurance case for breakout group 1, focusing on ensuring fair outcomes for patients.</p> <p></p> <p>Figure 4.3: the assurance case for breakout group 2, focusing on supporting the professional judgement of psychiatrists.</p> <p>As noted above, fairness was a significant focus for the participants, and so it is unsurprising that one of the groups chose to explore an assurance case related to this goal.</p> <p>The assurance cases are incomplete because a lot of time was spent in discussion. However, some of the statements made during discussion can help elucidate aspects of the case. For instance, the choice to emphasise a group that is underserved, was linked to aforementioned components of fairness such as unequal access:</p>  <p>\"There are clearly real benefits for many people being able to access digital technology but there are also many people who won't be able to access it. Prior to even thinking about the introduction of digital technology in mental health services, we were aware of very longstanding inequalities in access to an outcomes from mental health services. There's a fear, particularly during COVID and lockdown and the move to \"digital by default\" that some of those divisions will grow larger as a result.\"</p>  <p>Interestingly, the second breakout group chose to focus on the impact of the hypothetical decision support system upon the healthcare professionals.7</p> <p>As the image above shows, their goal was framed in terms of supporting the \"professional judgement\" of the psychiatrist. On its own, this goal would be underspecified, making it difficult to accurately link to any of the SAFE-D principles or core attributes. Fortunately, the discussion and property claims of the assurance case help clarify the intentions of this group, but some residual ambiguity remains due to the inclusion of two core themes:</p> <ol> <li>The long-term effects of the system on professional development and judgement</li> <li>The responsibility of early career psychiatrists, who may be less able to challenge or contest automated recommendations.</li> </ol> <p>These two themes emerged from discussion about the potential harmful impact of the system on the judgement of psychiatrists, such as the possibility of automation bias (i.e. the tendency for users to be unduly influenced by automated decision-making, even when their own judgement would be preferable or more accurate). For instance, when the group considered which forms of evidence they would expect to see included to provide assurance that this risk had been managed and mitigated, they added an evidential claim and artefact that communicated additional mentor support (from a senior healthcare professional) and positive self-evaluation from the user (see right of Figure 4.3).</p> <p>The inclusion of this segment in the assurance case cannot be divorced from consideration about the responsibility that a user has for their own decisions, as well as the institutional mechanisms of accountability that ought to be put in place to support users of decision support systems. Therefore, it remains unclear whether the goal of this assurance case could be captured by a single SAFE-D principle. On the one hand, <code>Sustainability</code> would be a good candidate for capturing the long-term impacts of the system on user autonomy and professional judgement. On the other hand, <code>Accountability</code> would be preferable for those attributes concerned with responsible decision-making and institutional accountability. These questions were not raised with the participants, nor is there sufficient information to infer their intentions.</p> <p>However, the following quotation from one of the (developer) participants is illuminating for the challenges involved in ensuring responsible decision-making in situations where multiple organisations are involved in a distributed project lifecycle:</p>  <p>\"Ultimately, once you hand over the software and you set them [procuring organisation] up, how they actually use it and why they've got it can be a bit of a mystery. Sometimes, organisations are looking for ways to support people but they don't have much resource and they think digital might be a good way of doing that. So, maybe it's still a good motivation but there's also this expectation that digital can do a lot more than it can, and it's a cheap way of ticking a box.\"</p>  <p>In general, it was challenging to develop a full assurance case for several reasons:</p> <ol> <li>Time limits imposed by workshop</li> <li>Lack of familiarity with the Trustworthy Assurance methodology</li> <li>Challenges of reconciling broad range of perspectives to co-create a shared goal and common understanding</li> </ol> <p>The first two barriers would be easy to reconcile. For instance, although we dedicated a significant portion of time to introducing and exploring the trustworthy assurance methodology, more hands-on experience with the tool could have helped the participatory activity of developing an assurance case for the hypothetical case studies.</p> <p>The third barrier, however, is harder to overcome. In our initial project planning meetings we considered running separate workshops for all of the stakeholder groups, but settled on mixed engagement events because of the perceived benefit of facilitating communication between different group\u2014a key benefit of the methodology itself. A portion of this barrier could be resolved with additional time, but the value-based and translational gap that typically exists between different groups (e.g. regulators and developers) will remain. Therefore, the following recommendation is proposed as a measure to address this challenge:</p>  <p>Readiness, skills, and training should be prioritised both within organisations (e.g. how to implement ethical considerations into the project lifecycle) and across organisations (e.g. how to develop and adopt best practices). In addition, common capacity building should be supported by regulators and industry representatives (e.g. shared risk mapping, regulatory gap analysis, and horizon scanning activities to help create and maintain a common pool of expertise).8</p>","location":"dmh-report/chapter-4/#workshop-1b","tags":["workshops","users","regulators","developers","researchers","participatory-design"]},{"title":"Feedback","text":"<p>Following the participatory activity, participants were asked to complete an anonymous survey, which was designed to elicit additional information about the perceived value of the trustworthy assurance methodology. Therefore, despite the small sample, it is worth analysing the responses before we turn to the final workshop with users of DMHTs.</p>  <p>Question 1</p> <p>To what extent do you agree/disagree with the following statement: \"The methodology of trustworthy assurance would be helpful in identifying potential ethical risks which arise while designing, developing and deploying a DMHT\".</p>     Option # Responses     Strongly Agree 4   Agree 9   Undecided 2   Disagree 0   Strongly Disagree 0    <p>This initial feedback is positive. The majority of respondents 'agree' or 'strongly agree' with the statement, indicating support for the methodology despite the challenges faced during the activity.</p> <p>Unfortunately, the sample size is too small to infer anything meaningful about the distribution of participants across these categories. For instance, whether developers were more positive than regulators or vice versa. 9</p>  <p>Question 2</p> <p>To what extent do you agree/disagree with the following statement: \"The methodology of trustworthy assurance would be a helpful means through which to communicate to other stakeholders that a DMHT is trustworthy\".</p>  <p>Similarly, the majority of respondents 'agree' or 'strongly agree' with the above statement, reinforcing our prior assumption about the communicative value of trustworthy assurance. However, as we will see shortly, there are some future areas for improvement, which likely explain the increased number of 'undecided' responses, which were primarily from policy-makers. 10 </p>    Option # Responses     Strongly Agree 4   Agree 8   Undecided 3   Disagree 0   Strongly Disagree 0     <p>Question 3</p> <p>What do you consider to be the primary advantages of the ethical assurance methodology?</p>  <p>The feedback from this question can be summarised as follows:</p> <ul> <li>The primary advantage is having a capacity to support a structured and end-to-end approach to project governance, facilitated by shared aims and objectives for broader normative goals (e.g. health equity).</li> </ul> <p>For instance, as one participant noted,</p> <ul> <li>\"Its [i.e. trustworthy assurance] emphasis on structure and evidence supporting claims which derive from an overarching goal. It really helps to be forced to think in these terms to keep an open mind about what requirements a certain system has at different stages of design, development and deployment. I love how flexible the system is, so that it can account for many technologies and contexts on the market.\" [Researcher]</li> </ul> <p>Although trustworthy assurance is a structured process, as this participant emphasises, flexibility is also maintained by enabling myriad goals, properties, and evidence to be selected to fit the context of a specific project. Where this flexibility is used to facilitate bidirectional decision-making about project aims and objectives, trustworthy assurance can (as another participant notes) support the development of</p> <ul> <li>\"A level playing field expectation from procurers, and a common improvement in practice.\" [Developer]</li> </ul> <p>And, in turn,</p> <ul> <li>\"will help developers, commissioners and service providers to know that equality/ethics has been considered.\" [Policy-maker]</li> </ul>  <p>Question 4</p> <p>What do you consider to be the primary disadvantages of the ethical assurance methodology? Please specify any aspects of the methodology which you believe require further refinement. </p>  <p>While it is encouraging to see many positive responses, it is also important to consider critical feedback, as it is here that potential gaps and barriers can be addressed. The first critical comment attenuates the positive feedback about the methodology's flexibility noted above:</p> <ul> <li>\"The open-ended nature of the methodology makes it difficult to know what to include in scope and may pose a difficulty for comparing assurance cases between manufacturers, for instance.\" [Developer]</li> </ul> <p>This is a valid concern, but can be addressed through the use of a) argument patterns and b) guidance about how ethical principles can be operationalised throughout the project lifecycle (see below and chapter 5).</p> <p>The second point relates to organisational culture and readiness, and the challenge of considering competing incentive or disincentive structures\u2014a theme also raised in the previous chapter:</p> <ul> <li>\"There are lots of examples of people producing equality impact assessments11 that are little more than a tickbox exercise. It's important they are produced, but it's even more important they are of a high quality.\" [Policy-maker]</li> </ul> <p>We have previously emphasised that trustworthy assurance should not be reduced to a mere checklist or compliance exercise. The model of the project lifecycle is one means for mitigating the risk of misuse in this manner, as it emphasises the iterative and dynamic process of building an assurance case over the course of the entire project lifecycle. As such, the act of building an assurance case is not rendered a checklist exercise that is carried out at the end of a project as an afterthought, but is rather approached as subject to ongoing review.</p> <p>However, this prescription is going to be in conflict with alternative interests, as one participant notes:</p> <ul> <li>\"Assurance may be not in the interest of profit\" [Researcher]</li> </ul> <p>At present, our methodology is not supported by a theory of change for how organisations can adopt the methodology into their own practices. One possibility would be to work with public sector organisations and regulators to establish a requirement for those responding to tenders to provide an assurance case for a relevant ethical goal (e.g. non-discrimination, explainability). Alternatively, we could investigate how specific goals in the context of healthcare could expand upon existing regulations (e.g. medical device approval).</p> <p>The following three questions were posed to the respective participants as a means of eliciting more specific feedback about perceived obstacles to the successful adoption and integration of trustworthy assurance into their respective practices.</p>  <p>Question 5a</p> <p>As a developer, are there any objections or external obstacles which would prevent you from producing an ethical assurance case during the design, development and deployment of a new DMHT?</p>  <p>The first two comments relate to a similar concern about organisational readiness and incentive/disincentive structures raised above:</p> <ul> <li>\"External obstacles are business needs and drivers that might cut down on the time needed to use a methodology like this properly. It can be hard to create and argue for time to give ethics proper consideration when there are business deadlines.\"</li> <li>\"To produce an ethical assurance case would require additional effort. It would ideally be integrated into other early design processes alongside clinical safety case, data protection impact assessment, etc. The process would need to be usable in an (agile) product development context, where an ethical assurance case would be updated as changes are made to the tool in an iterative fashion.\"</li> </ul> <p>Here, our analysis and response echoes some of the comments made earlier (e.g. conflict with profit incentives). However, one additional comment can be made: having tailored versions of the project lifecycle model, which reflect the unique needs and challenges of specific domains, would help developers identify which actions could be undertaken (and when) to meet the goal of an assurance case. This development would, furthermore, create space for the development of supporting standards, as acknowledge by the following participant:</p> <ul> <li>\"Clarity on expected standards would be the largest obstacle. We noticed the biggest upswing in GDPR policy uptake came when we started offering standard starter policies.\"</li> </ul>  <p>Question 5b</p> <p>As a researcher working on DMHTs, do you see any obstacles to the uptake of ethical assurance in the sector?</p>  <p>The responses from researchers were mostly positive in the sense that few obstacles were identified. However, there was a skepticism about the likelihood of the private sector adopting the practices of trustworthy assurance.</p> <ul> <li>\"Ethical regulation, in the private sector, is non-existent. Theories around mental health, in general, are too underdeveloped, the digital context and the methods to use for research unexplored, non-rigorous, and misaligned with gold standards. Ethical assurance has no tangible rewards for a designer with a purpose or aim, independently of the positive principles used to design the system.\"</li> </ul> <p>We can, again, note that targeting procurement practices in the public sector may be a positive first step, and, moreover, that complementarity with existing regulation could increase adoption. This latter point was highlighted by the responses from the policy-makers.</p>  <p>Question 5c</p> <p>As a policy-maker, do you consider the methodology of ethical assurance to be compatible with existing regulatory mechanisms in the sector? Please describe any obstacles to the adoption of ethical assurance in the digital mental healthcare sector.</p>  <ul> <li>\"I'd recommend that you align to existing regs where possible (e.g. goals could be 'conform to GDPR' or 'meet regulatory requirements' rather than more abstract items) and make it easy for non-experts to use.\"</li> </ul> <p>The response from another policy-maker, however, suggests that caution should still be exercised when seeking alignment, in order to avoid confusion:</p> <ul> <li>\"It [trustworthy assurance] is compatible but it may overlap considerably with other requirements eg. ESF, DTAC etc. creating confusion and overload for developers.\"</li> </ul> <p>The final section of the survey, asked participants to offer any remaining feedback. Here, the selection of responses serves as a useful summary on the above analysis. </p> <p>First, our analysis shows that while many recognise the value of the methodology, significant obstacles remain in the successful adoption of trustworthy assurance. Most notably, the friction presented by antagonistic incentive/disincentive structures. While our comments suggest possible avenues that may counteract some of the disincentives, a comment raised by one participant suggests that more work needs to be done to better communicate the positive value of using trustworthy assurance:</p> <ul> <li>\"How do we ensure developers of mental health tools use or are even aware of these types of methodologies? What is the incentive to use these methodologies?\" [Developer]</li> </ul> <p>We outlined many of the potential benefits of trustworthy assurance in the first section, but ensuring others are convinced of these values will take time. In alignment with the final two comments from our participants, a means for reducing the friction would be to build out additional case studies or examples of best practice, which can help serve as a point of orientation for other developers and organisations.</p> <ul> <li>\"The worked example made things easier to understand and think about.\" [Developer]</li> <li>\"Having a best practice example to learn from and emulate would help our practice.\" [Developer]</li> </ul> <p>This suggestion is a variation of the recommendation above about supporting readiness, skills and training, and common capacity building. However, in the next chapter we will build on this by setting out a clearer proposal and recommendation for the incorporation of argument patterns.</p>","location":"dmh-report/chapter-4/#feedback","tags":["workshops","users","regulators","developers","researchers","participatory-design"]},{"title":"Workshops with users of DMHTs","text":"<p>Summary</p> <p>The workshops with users exposed wide-ranging, nuanced, and interconnected attitudes, while contributing to practical and complementary recommendations for developers and regulators.</p> <p>Four central themes emerged from the workshops:</p> <ol> <li>Distrust as a barrier to accessing and using DMHTs</li> <li>Stakeholder and user engagement as a means for ensuring accountability</li> <li>Explainable technology and systems as a pre-requisite for informed choice</li> <li>Ensuring fairness by reducing digital exclusion, bias, and discrimination, and promoting social justice</li> </ol>  <p>While all of the themes are interconnected, the fourth theme especially is inseparable from the others.</p>","location":"dmh-report/chapter-4/#workshops-with-users-of-dmhts","tags":["workshops","users","regulators","developers","researchers","participatory-design"]},{"title":"Overview","text":"<p>The workshops with users of DMHTs were co-organised and facilitated by the McPin foundation\u2014a mental health research charity. This ensured an additional level of support from those with domain expertise, in addition to the participants' lived experience, and helped reduce interpreter bias in our analysis. </p> <p>We held two workshops (one in person and one online) to improve accessibility for participants (e.g. reducing geographic restrictions, supporting those who were uncomfortable/unable to attend in-person to still participate). The information that participants received and the activities that were carried out were identical across the workshops, except for the medium in which the activities were conducted (e.g. use of a collaborative whiteboard in the online setting).</p> <p>There were two activities that participants contributed to. Both were designed to maximise the ability of the feedback to shape and inform the design of our methodology and recommendations while minimising the need for prior reading (e.g. information about argument-based assurance). A talk preceded each of the activities to ensure that participants were equipped to contribute in a meaningful way.</p> <ul> <li>Activity 1: participants were asked to reflect on a range of possible use cases for DMHTs and evaluate possible harms and benefits by answering the following questions:<ul> <li>Which ethical values or principles matter to you in the context of digital mental healthcare?</li> <li>What are some positive use cases for DMHTs?</li> <li>What are some negative use cases for DMHTs?</li> </ul> </li> <li>Activity 2: participants were given a set of claims made by a fictional development team about one of the four case studies, and asked to evaluate the claim based on the following criteria:<ul> <li>Whether the claim appeared to be motivated by or support an ethical value or principle.</li> <li>Whether they found the claim reassuring or whether it raised concerns.</li> <li>What evidence, if any, they would expect to see to support or validate the claim.</li> </ul> </li> </ul> <p>In both activities, participants were encouraged to explore tangential points in dialogue with the group. The purpose of these activities was to provide a general scaffold for discussion, from which salient and significant themes could be identified with the participants. Therefore, in our analysis we do not differentiate between the findings from the two activities, but instead group them together and make specific recommendations linked to the relevant themes.</p> <p>However, one output from the first activity can be presented as a stand-alone output. Table 4.3 presents a summary of the positive and negative uses of DMHTs, as judged by the workshop participants. Although this feedback is incorporated into our own thematic analysis, the reader may find it illuminating to consider the responses prior to reviewing our subsequent analysis.</p> <p>Table 4.3\u2014participant's perceptions about the positive and negative uses of DMHTs.</p>    Positive Use Cases Negative Use Cases     Useful for opening dialogue with clinicians Predatory \"targeted marketing\", business/finance models that take advantage of people, using people\u2019s data to attain information and target them. Manufactured empathy is not empathy.   Tools for self-management and self-help Limitations of the technology leading to problems. Difficulty in determining when there is an emergency, not understanding tone of voice.   Useful for remaining anonymous Selling people\u2019s personal data   Could be useful for preserving continuity of care, and personalisation of care Privacy issues (young people especially)   Useful for dangerous or violent person where clinical contact is bad/not recommended Stalking, coercive control and abuse, people pretending to be identities that they are not.   Potential for a deeper understanding of mental health difficulties due to the amount of data that could be collected Infiltration in creepy ways into personal/sex life.   Accessibility - can do in your own time and from your own home. Useful for those who live remote areas where travel is not possible or expensive Constant monitoring by the device, increased paranoia, over-reliance on device.   Reducing the load on psychiatrists Increased loneliness. Social isolation can be exacerbated when you are talking to a chatbot, or you can become reliant on the chatbot.   Ability to share recovery (or other mental health) narratives digitally Misrepresentation of outcomes. When usage time is monitored, not using the device can indicate deep depression or apathy but could also indicate that there are other good things going on in the real world they are engaged with.    Tech use and being online has been problematic for some service users and giving it all up has been a good factor in their recovery    Some people don't all have access to privacy to use digital technologies in their own home.    Need to keep up-to-date hardware to access may impact those on lower incomes and increase the electronic waste problem.","location":"dmh-report/chapter-4/#overview","tags":["workshops","users","regulators","developers","researchers","participatory-design"]},{"title":"Thematic Analysis","text":"<p>The following themes were identified across the two workshops and activities. They have been co-developed by the users, the facilitators from the McPin foundation, and ourselves.</p>","location":"dmh-report/chapter-4/#thematic-analysis","tags":["workshops","users","regulators","developers","researchers","participatory-design"]},{"title":"Distrust as a barrier to access and use","text":"<p>There were high levels of distrust and skepticism within the group regarding the societal and individual benefits of digital mental health. However, the sources and targets of the distrust or skepticism were nuanced and wide-ranging.</p> <p>Several participants, for example, were keen to acknowledge that the issues with digital technologies should be set against the backdrop of the current issues facing mental health services (e.g. long wait lists, insufficient funding). This includes a recognition of the difficulty of getting face-to-face appointments and the biases of human healthcare professionals:</p>  <p>\u201cI didn\u2019t like it [online CBT], I was just desperate to have any form of counselling and because the waiting list was two years, I thought it was better than nothing.</p> <p>\u201cYou get people in the NHS who are as bad as chatbots. They may as well be robots.\u201d</p>  <p>Some participants linked this distrust to cultural attitudes they held:</p>  <p>\"My parents are [information redacted] and really value privacy and that is why I didn\u2019t use a smartphone for a long time. And a lot of my friends who are African or Caribbean or Asian don\u2019t have a smartphone because of privacy.\u201d</p>  <p>Whereas others linked the source of distrust to potential conflicts of interest:</p>  <p>\"Who is the advocate for the technology? Is it the psychiatrist pushing it because it makes their life easier?\"</p> <p>\"Who holds the purse strings?\"</p>  <p>For some participants, the distrust or skepticism was directed towards specific technologies such as chatbots:</p>  <p>\u201c[Chatbots are] good for customer service, but sometimes it feels like they are being used to replace humans\u201d</p> <p>\u201cHow is it ethical to develop a solution like that, and allow these technologies to exist on the marketplace when they are doing more harm than good?\u201d</p>  <p>But, again, the skepticism that participants held was typically nuanced and voiced with caveats:</p>  <p>\"I am very comfortable with tech. There are some circumstances where I trust technology more than people. I trust an iPad food ordering system than a human.\u201d</p> <p>In some instances, such as AI-assisted surgery, involvement of technology should optimise for safety. But in mental healthcare, human interaction and involvement will always be vital to a trustworthy and supportive relationship.</p>  <p>From these preliminary remarks, it is important to remember how we disentangled the concepts of 'trust' and 'trustworthiness' back in Chapter 1. To recall, 'trust' can refer to a belief or attitude that is directed towards an object, person, or proposition (among other things), whereas 'trustworthiness' refers to the perceived property or attribute which an individual uses to determine whether to place trust (e.g. whether to trust a news article based on its quoted sources). Differentiating these terms is helpful for evaluating whether there are reasonable (and unreasonable) grounds for placing trust. For example, a person may have reasonable grounds for their distrust in an organisation, where the organisation has a history of violating data protection and privacy laws. In contrast, another person may have unreasonable grounds for their skepticism about the clinical efficacy of a technology based on an accessible, well-validated, and reliable evidence base.</p> <p>Identifying the reasons for why users may trust or distrust a DMHT can help organisations assess and evaluate both the trustworthiness of their teams and services, and identify opportunities for intervention. Phrased as a recommendation:</p>  <p>Recommendation</p> <p>Organisations should consider both the trustworthiness of their products and services, but also the reasons why users may trust or distrust them.</p>  <p>Acting upon this recommendation requires organisations to engage stakeholders, which links to the next theme that emerged during discussion.</p>","location":"dmh-report/chapter-4/#distrust-as-a-barrier-to-access-and-use","tags":["workshops","users","regulators","developers","researchers","participatory-design"]},{"title":"Accountability through engagement","text":"<p>In her BBC Reith Lectures, 'A Question of Trust', moral philosopher Onora O\u2019Neill argues that, \"we need more intelligent forms of accountability, and that we need to focus less on grandiose ideals of transparency and rather more on limiting deception.\"12</p> <p>O'Neill's prescription captures many of the concerns and aspirations of the participants. For instance, several participants voiced concerns with the deceptive practices of some organisations to exploit vulnerable users through social media marketing (e.g. adolescents). Other participants viewed the over-reliance on privacy policies to be an instance of deceptive practice, as such policies rarely provide sufficient information to address a user's concern, such as data use:</p>  <p>\"people should know how data is being used, who has access to it.\"</p>  <p>In contrast, participants were keen to express their desire for genuine forms of accountability and responsibility exercised through the life cycle of a DMHT, achieved through meaningful engagement and participation of stakeholders. The slogan, \"nothing about us without us\" comes to mind here13. And, there are also close ties between this theme and the subsequent one (i.e. explainability):</p>  <p>\"No transparency without accountability and explainability\"</p>  <p>This emphasis on engagement and meaningful forms of participation will be returned to, as it was a recurring and cross-cutting theme. However, several practical recommendations can be offered here in connection with the theme of accountability:</p>  <p>Recommendations</p> <ul> <li>Accountability should be built into all stages of the project lifecycle, and requires both stakeholder engagement and also diversity within the project team (especially neurodiversity).</li> <li>Where there is a risk of harm to users, organisations should be transparent about how these risks were identified (e.g. who was involved in the risk assessment), how they were mitigated, and what mechanisms for redress are available to impacted individuals. </li> </ul>","location":"dmh-report/chapter-4/#accountability-through-engagement","tags":["workshops","users","regulators","developers","researchers","participatory-design"]},{"title":"Explainability as a pre-requisite for informed choice","text":"<p>As has already been noted, there was a strong dislike and distrust towards the perceived over-reliance on privacy policies. As several participants noted:</p>  <p>\"The culture of small-print is pervasive, but the culture of consent is not built into a business model\u201d</p> <p>\u201cit is not consent because you are not informed\u201d</p> <p>\"Pooled permissions are a risk to privacy and there should be more modular options to accept or deny the Terms and Conditions. Usually, you must \u2018accept all\u2019 for an app to work.\"</p>  <p>However, the following question from one participant inverts the perspective and shifts focus onto the user:</p>  <p>\u201cHow do we communicate our privacy policy\u201d</p>  <p>Traditional frameworks in biomedical ethics link informed consent to values such as patient autonomy. In short, without sufficient knowledge about how a service operates or the risks associated with a medical intervention, a patient has little to no meaningful choice about whether to engage or accept a recommendation from a healthcare professional.</p> <p>The above quotation is a succinct and cogent way of capturing the ethical importance of these values. But its emphasis on a more active form of communication goes beyond the practical goal of informed and autonomous decision-making, and reiterates the importance of stakeholder participation as a form of meaningful input and influence.</p> <p>To understand why this shift in framing matters, consider the fact that for many users there may be no practical choice about whether to engage if there is only a single option available to them. This may be because of long wait times, limited provision of services, or perhaps because a user is only choosing to engage at a point of crisis. </p>  <p>\u201cI didn\u2019t like it [online CBT], I was just desperate to have any form of counselling and because the waiting list was two years, I thought it was better than nothing. It will keep me from having suicidal ideation.\u201d</p>  <p>Capturing these themes and building on the previous sections recommendations, we can add the following recommendation.</p>  <p>Recommendation</p> <p>Information that is necessary to and supportive of informed choice should not be hidden within obscure privacy policies; it should be made accessible to users as explanations of how a system was designed, developed, and deployed. In doing so, organisations should be clear about how they define and operationalise key terms, such as 'mental health' or 'well-being' and how their understanding of the terms may have impacted the design, development, and evaluation of a service.</p>  <p>But the shift from choice to active involvement is not just about improving explanations to support participation, it is also about improving access more generally. As one participant acknowledges, this is fundamentally a matter of fairness.</p>  <p>\"if everything is moving towards digital, who is going to be excluded. Is it going to be harder to access face-to-face care\".</p>  <p>This brings us to our final theme.</p>","location":"dmh-report/chapter-4/#explainability-as-a-pre-requisite-for-informed-choice","tags":["workshops","users","regulators","developers","researchers","participatory-design"]},{"title":"Fairness: reducing digital exclusion, bias and discrimination, and promoting social justice","text":"<p>Despite being left until the end of the chapter, this final theme stood out as one of the most significant and resonates with many aspects of the themes above and also with the other workshops.</p> <p>In a similar manner to the other workshops, the plurality of concepts referenced in this theme's heading reflects the breadth, nuance, and interconnectedness of the ideas that were raised. For instance, the participants' understanding of what we call 'fairness' in the SAFE-D principle framework was nuanced and multifaceted. Although we are unable to capture all of the comments raised, the topics discussed touched upon centuries-old forms of sociocultural and structural discrimination or oppression, historical abuses of vulnerable groups by scientific research groups and institutions, epistemic injustices, and power imbalances that disproportionately affect marginalised communities.</p> <p>Digging deeper into some of these topics, concerns about the negative impacts of digital exclusion and the widening digital divide, exacerbated by growing socioeconomic inequalities, were highlighted frequently. Some participants linked their concerns to gaps in current regulation and legislation:</p>  <p>\"Ensuring inclusion and accessibility requires going beyond protected characteristics: disability &amp; class and access to technology.\"14</p>  <p>While others emphasised structural forms of exclusion in technology design:</p>  <p>\"When designing based on AI and machine learning, we look at what works for the mass and the smaller minority communities and the rare types of people/personality are excluded by design.\"</p>  <p>Similarly, some participants raised questions about the possibility of algorithmic discrimination due to varying levels of efficacy across demographic groups:</p>  <p>\"How will it [the hypothetical NLP algorithm in one of our case studies] account for regional words and dialect? Slang terms? Cultural terms? Accessibility in different languages.\"</p>  <p>These critical comments and considerations should not be isolated from the remarks outlined in previous themes or the following recommendations raised by participants:</p> <ul> <li>Distrust as a barrier to access and use: historic forms of oppression, injustice, and discrimination partially explain why some individuals and groups have low levels of trust towards these technologies and the organisations that design, develop, and deploy them. </li> <li>Accountability through engagement: the risks of harm and the likely benefits associated with DMHTs may not be shared equally by all groups. Inclusive stakeholder engagement is one mechanism by which oversight and accountability in the risk management process can be achieved. To paraphrase one participant, 'those on a design team should be a diverse, invested group, and diversity should not be tokenistic'.</li> <li>Explainability as a pre-requisite for informed choice: as a bioethical principle, ensuring informed consent is often linked to the ethical value of self-determination. The significance of the principle is understood by many to arise from a universal right to autonomous decision-making in matters relating to one's health and well-being. While the domain of mental healthcare places restrictions on this right when it conflicts with other duties (e.g. protecting others from harm), these are limiting cases for which there are existing norms and guidelines in place15. In general, ensuring that an individual has sufficient access to the explanations needed about how a technology operates, in order to make an informed choice about whether to use the technology, has already been acknowledged as a vital ethical goal. However, there are many barriers in place to achieving this goal, and where they disproportionately affect certain groups (e.g. those with low levels of digital literacy or access to support services) this goal connects with the related goal of promoting social justice.</li> </ul> <p>Improving health equity is already a key priority across many organisations in the UK.16 However, the longstanding impacts and challenges of COVID-19 are still affecting society, often in a disproportionate and unjust manner, and many lessons still need to be learned and adopted, as highlighted in 'Build Back Fairer: The Covid-19 Marmot Review', from the Institute of Health Equity and Health Foundation.</p> <p>Unlike the other themes, we do not offer any specific recommendations on this topic beyond the reiteration of the importance of stakeholder engagement and meaningful participation. This is partially because there is already a wealth of extant research produced by organisations across the public and third sectors offer evidence-based policy recommendations. However, it is also because we pick up on this theme directly in the next chapter and present an argument pattern to help promote the goal of fairness in digital mental health.</p>   <ol> <li> <p>There is, of course, a trade-off here between the narrower and wider perspectives, which we discuss in Appendix 1.\u00a0\u21a9</p> </li> <li> <p>This is not to say that clinical efficacy is not a key component of ethical decision-making. For instance, the bioethical principle of beneficence clearly requires sufficient levels of clinical efficacy.\u00a0\u21a9</p> </li> <li> <p>These objectives were as follows: 1) To explore whether and how the methodology of argument-based assurance could be extended to address ethical issues in the context of digital mental healthcare. 2) To evaluate how an extension of the methodology could support stakeholder co-design and engagement, in order to build a more trustworthy and responsible ecosystem of digital mental healthcare. 3) To lay the theoretical and practical foundations for scaling the ethical assurance methodology to new domains, while integrating wider regulatory guidance (e.g., technical standards).\u00a0\u21a9</p> </li> <li> <p>The attentive reader will see significant overlap between the concepts that are mapped onto the principles, and subsequently the principles themselves (e.g. explainability and accountability). Because the principles were not designed to be mutually exclusive and collectively exhaustive, this overlap is to be expected.\u00a0\u21a9</p> </li> <li> <p>Beauchamp, T. L., &amp; Childress, J. F. (2013). Principles of biomedical ethics (7th ed.). Oxford University Press.\u00a0\u21a9</p> </li> <li> <p>Leslie, D. et al. (2022). Human rights, democracy, and the rule of law assurance framework for AI systems: A proposal. Accessed: https://rm.coe.int/huderaf-coe-final-1-2752-6741-5300-v-1/1680a3f688 \u21a9</p> </li> <li> <p>Our case studies included a section on 'Affected individuals, groups, and other stakeholders'. For this case study, 'psychiatrists' were included (see case study 3.\u00a0\u21a9</p> </li> <li> <p>See our report on developing 'Common Regulatory Capacity for AI' for more on these topics.\u00a0\u21a9</p> </li> <li> <p>For transparency, the distribution of responses by stakeholder group is as follows: Strongly agree: policy-maker x2, researcher x1, developer x1; Agree: policy-maker x1, researcher x4, developer x2; Undecided: policy-maker x2\u00a0\u21a9</p> </li> <li> <p>Again, these results should be treated with caution due to the small sample size. Strongly agree: policy-maker x1, researcher x1, developer x2; Agree: policy-maker x2, researcher x4, developer x4; Undecided: policy-maker x2, developer x1\u00a0\u21a9</p> </li> <li> <p>See Equality and Human Rights Commission. (2014, August 1). Technical Guidance on the Public Sector Equality Duty: England | Equality and Human Rights Commission. Technical Guidance on the Public Sector Equality Duty: England. https://www.equalityhumanrights.com/en/publication-download/technical-guidance-public-sector-equality-duty-england \u21a9</p> </li> <li> <p>O\u2019Neill, O. (2002). A Question of Trust. Cambridge University Press, pp. 77\u201378.\u00a0\u21a9</p> </li> <li> <p>Charlton, J. I. (1998). Nothing about us without us: Disability Oppression and Empowerment. University of California Press.\u00a0\u21a9</p> </li> <li> <p>This theme is built into an argument pattern in the next chapter. The pattern urges reflection upon and consideration of deep patterns of discrimination, marginalisation, and minoritisation, which can exacerbate mental health issues, but which fall beyond protected characteristics that lie outside the scope of the Equality Act 2010 (e.g. poverty, housing, employment).\u00a0\u21a9</p> </li> <li> <p>On these points it is worth noting that the Mental Health Act, which sets out the legislation that is used to determine when it is appropriate to place restrictions on people,  has been subject to proposed reform in recent months. Readers may find the \"new guiding principles\" of particular interest in the context of this report (see here).\u00a0\u21a9</p> </li> <li> <p>For instance, the goal of increasing health equality is incorporated into a recent discussion paper from the Department of Health and Social Care's Mental health and wellbeing plan, the 'Advancing mental health equalities' strategy from NHS England, the Welsh Government's 'Together for Mental Health' delivery plan, and an ongoing consideration for the Scottish Government following the publication of an equality impact assessment of their mental health strategy back in 2017.\u00a0\u21a9</p> </li> </ol>","location":"dmh-report/chapter-4/#fairness-reducing-digital-exclusion-bias-and-discrimination-and-promoting-social-justice","tags":["workshops","users","regulators","developers","researchers","participatory-design"]},{"title":"Developing Trustworthy Assurance\u2014Argument Patterns for fairness and Explainability","text":"<p></p>  <p>Chapter Overview</p> <p>In this chapter we present two argument patterns (i.e. starting templates for building assurance cases) that identify the types of claims, or the sets of reasons that need to be established to justify the associated top-level normative goal.</p> <p>The first pattern is for assurance cases that aim to justify the fairness of a DMHT. The second is for cases that address the explainability of systems.</p> <p>We also discuss relevant legislation, regulation, and best practice guidance that support and motivate the development of these patterns.</p>","location":"dmh-report/chapter-5/","tags":["fairness","explainability","argument-patterns"]},{"title":"Co-designing argument patterns","text":"<p>While the assurance methodology is a tool in its own right, there is a missing component that was highlighted in Chapter 2: argument patterns.</p> <p>You may recall that argument patterns are reusable structures that serve as starting templates for building assurance cases. They identify the types of claims (or, the sets of reasons) that need to be established to justify the associated top-level normative goal. And, in doing so, they set useful constraints on both the deliberative process and evidence-generating and evidence-gathering exercises. We say more about the evidential generation and selection process towards the end of this section.1</p> <p>Before this, we present and explain two argument patterns for use in the assurance of DMHTs. The first is for assurance cases that address and justify the fairness of a DMHT; the second is for cases that address and justify the explainability of systems.</p>  <p>Why \u2018fairness\u2019 and \u2018explainability\u2019?</p> <p>Our decision to focus on these two patterns is motivated primarily by the desire to capture the significant themes raised in the workshops. As such, the inclusion of a pattern for fairness is an obvious choice based on the discussions in the previous chapter. However, the inclusion of one for the goal of explainability requires additional clarification.</p> <p>Many participants in our workshops focused on ethical issues that could map onto multiple SAFE-D principles. For instance, considerations around <code>transparency</code> were sometimes raised in connection with mechanisms for holding organisations <code>accountable</code> and at other times raised in connection with a requirement to ensure users had access to information to support informed consent and decision-making\u2014captured either by <code>sustainability</code> or <code>explainability</code>. Our second argument pattern is framed in terms of <code>explainability</code> as an attempt to be maximally inclusive of these wide-ranging considerations. </p> <p>To recall, argument patterns in trustworthy assurance are always starting points for participatory forms of reflection and deliberation. They provide greater specificity than the top-level goal would on its own, and help operationalise ethical principles within the project lifecycle model. But they are no substitute for embedded processes of inclusive stakeholder engagement\u2014in fact, they depend upon stakeholder engagement processes for their completion. Furthermore, they should not be used as a mere checklist for compliance. </p> <p>Finally, our focus on 'fairness' and 'explainability' should not suggest that other patterns are not desirable or important. Rather, the co-design and development of additional patterns, including those that go beyond the SAFE-D principles are left for future research (see Conclusion).</p>","location":"dmh-report/chapter-5/#co-designing-argument-patterns","tags":["fairness","explainability","argument-patterns"]},{"title":"Fairness","text":"<p></p> <p>In the context of data-driven technologies, a core attribute of fairness is the equal distribution of risk and benefit across all groups of affected users. For instance, a technology that was highly accurate for users aged between 18-30 but became decreasingly accurate for older individuals would be unfair to those from higher age brackets.</p> <p>The differentiation between risk and benefit leads to a subsequent distinction between corresponding duties or obligations (e.g. legal duties). Where there is a duty to ensure that a particular group of users are not exposed to disproportionate risks of harm, this can set up a so-called \u201cnegative duty\u201d (e.g. a duty for a developer to not discriminate). However, negative duties often set only the minimal ethical standards. In other words, one can build a non-discriminatory service that does not harm anyone, but similarly benefits no one or benefits only a small portion of users.</p> <p>Therefore, corresponding \u201cpositive duties\u201d also exist to promote beneficial outcomes, sometimes focusing on those who are most disadvantaged\u2014a so-called \u201cprioritarian\u201d approach to ethics (i.e. prioritising those who need the most support). A good example of this duality is the Public Sector Equality Duty, which sets the following objectives for all public authorities:</p> <ul> <li>eliminate discrimination, harassment, victimisation and any other conduct that is prohibited by or under the Equality Act 2010;</li> <li>advance equality of opportunity between persons who share a relevant protected characteristic and persons who do not share it;</li> <li>foster good relations between persons who share a relevant protected characteristic and persons who do not share it.</li> </ul> <p>Notice that these objectives can be seen as having both negative and positive aspects to them (e.g. \u2018eliminate discrimination\u2019 as \u2018opposed to advance equality\u2019).2</p> <p>In the context of mental healthcare, where stigmatisation prevents many vulnerable individuals from accessing care and discrimination exacerbates symptoms for already marginalised or minoritised groups, both types of duty are absolutely crucial. However, acting upon positive duties is not without its challenges.  </p> <p>In our workshops, for instance, a key concern that was emphasised was the degree to which the delineation of \"good\" or \"desirable\" mental health outcomes, for the purpose of evaluating the impacts of a system, could adequately take into account the subjective nature of mental health and well-being. For instance, while there may be widespread agreement about what constitutes undesirable outcomes (e.g. chronic stress, suicide), the range of positive outcomes for mental health (or well-being) are varied and multitudinous (to echo back to Whitman\u2019s quote that started this report), and the experience and process of recovery can also mean many different things to people3. A failure to account for such issues can introduce a further source of bias into the design of a system (e.g. how the problem it seeks to address is formulated) which in turn could lead to unfair outcomes that only benefit a small group of users with aligned goals.</p> <p>The pattern that has been developed, through participation of stakeholders and users, attempts to address both negative and positive duties, while also making room for core attributes such as user autonomy. In the context of mental healthcare, this inclusion of autonomy can be problematic in the most severe cases where it is simply not viable (e.g. severe forms of psychosis). However, recall that a pattern is a starting point or scaffold; it is not a checklist of jointly sufficient claims and supporting evidence. Therefore, if a particular type of claim is inappropriate due to contextual factors that are determined during project scoping or stakeholder engagement, it can be adjusted as necessary.</p>  <p>Why does this pattern matter?</p> <p>In recent years, many tools for improving and supporting the trustworthy and responsible development of DMHTs have been proposed. One key advancement is the Digital Technology Assessment Criteria for Health and Social Care (DTAC). This form provides guidance on assessing four technical components (in addition to a contextual component):</p> <ol> <li>Clinical safety</li> <li>Data protection</li> <li>Technical security</li> <li>Interoperability criteria</li> </ol> <p>While the DTAC is intended to supplement existing regulatory guidance, as well as sitting alongside current and developing legislation or compliance duties (e.g. MHRA medical device registration, Equality Act 2010, NICE's Evidential Standards Framework), there is (at present) nothing in the DTAC about broader ethical issues such as the fair distribution of risk and benefits, or the requirement of explainable outcomes to support informed decision-making. As such, tools such as the DTAC serve a valuable but limited role in the assessment of fair DMHTs.</p> <p>In the last few years, however, many public authorities across the UK have released statements and policies calling for greater health equity. For instance, in October 2020, NHS England released their 'Advancing mental health equalities' strategy, which also fed into a recent consultation on the Mental health and wellbeing plan by the Department for Health and Social Care.</p> <p>The second of these publications makes reference to the UK Government's Levelling Up Strategy, which opens with the following statement:</p>  <p>\"not everyone shares equally in the UK\u2019s success. While talent is spread equally across our country, opportunity is not. Levelling up is a mission to challenge, and change, that unfairness. Levelling up means giving everyone the opportunity to flourish. It means people everywhere living longer and more fulflling lives, and beneftting from sustained rises in living standards and well-being.\"</p>  <p>Beyond people who share protected characteristics as set out in the Equality Act 2010, there are other groups who require specific consideration4:</p>  <p>Risks of mental ill-health are also higher for people who are unemployed, people in problem debt, people who have experienced displacement, including refugees and asylum seekers, people who have experienced trauma as the result of violence or abuse, children in care and care leavers, people in contact with the criminal justice system (both victims and offenders), people who sleep rough or are homeless, people with substance misuse or gambling problems, people who live alone, and unpaid carers.</p>  <p>And, finally, a recent publication by the UK's Office for AI has also called for regulators to \"embed considerations of fairness into AI\", but have further specified this principle as follows:</p>  <ul> <li>interpret and articulate \u2018fairness\u2019 as relevant to their sector or domain,</li> <li>decide in which contexts and specific instances fairness is important and relevant (which it may not always be), and</li> <li>design, implement and enforce appropriate governance requirements for \u2018fairness\u2019 as applicable to the entities that they regulate.</li> </ul>  <p>Therefore, there is clear regulatory appetite and industry need for both domain-specific and cross-cutting guidance on how to embed considerations of fairness and equality into the design, development, and deployment of digital technologies.</p>","location":"dmh-report/chapter-5/#fairness","tags":["fairness","explainability","argument-patterns"]},{"title":"Fairness Pattern","text":"<p></p> <p>Figure 5.1: A pattern for designing, developing, and deploying fair digital mental health technologies ( click image to expand;  download here)</p> <p>The goal claim in this argument pattern addresses distributional concepts of fairness (e.g. whether harms and benefits are shared equally), while also acknowledging broader conceptions of social justice (e.g. representational harms to marginalised groups). </p> <p>Unlike traditional safety cases, which often include  <code>System Description</code> and <code>Context of Use</code> placeholders, this pattern also includes a <code>Stakeholder</code> component to emphasise the importance of engagement. Here, the term 'stakeholder' should be treated in as inclusive a manner as possible, and not only the direct users of the technology.</p> <p>The goal is broken into four higher-level property claims and their respective sub-claims, which we group according to the following core attributes of the Fairness principle as specified and operationalised in the context of digital mental healthcare:</p> <ul> <li>Argument over <code>bias mitigation</code></li> <li>Argument over <code>non-exclusion</code></li> <li>Argument over <code>non-discrimination</code></li> <li>Argument over <code>equitable impact</code></li> </ul>","location":"dmh-report/chapter-5/#fairness-pattern","tags":["fairness","explainability","argument-patterns"]},{"title":"Argument over bias mitigation","text":"<p>This argument emphasises identification, evaluation, and mitigation activities. As there are too many biases to incorporate into a single pattern, this argument instead draws attention to transparent and accountable methods that allow stakeholders to determine if the set of biases that have been addressed are sufficient to address their concerns. This argument is supported by additional tools, such as a bias self-assessment tool that outlines social, statistical, and cognitive biases that can affect the lifecycle of a machine learning or AI system project.5</p>","location":"dmh-report/chapter-5/#argument-over-bias-mitigation","tags":["fairness","explainability","argument-patterns"]},{"title":"Argument over non-exclusion","text":"<p>This argument sets out another negative duty to consider wider social impacts of digital technologies, and prompts developers to ensure they are not contributing to growing socioeconomic inequalities (i.e. the digital divide) by overlooking important social determinants (e.g. education, poverty). In some instances, this may require nothing more than ensuring that UI/UX design choices do not exclude those who have additional accessibility requirements. However, the argument also sets up a duty to consider how a system being developed for use within a public healthcare system, for example, does not create an unsustainable multi-tiered approach, where some users are excluded from accessing a better performing technological service and forced to use comparatively inferior options.</p>","location":"dmh-report/chapter-5/#argument-over-non-exclusion","tags":["fairness","explainability","argument-patterns"]},{"title":"Argument over non-discrimination","text":"<p>This argument addresses the better known obligations, such as ensuring that members of protected groups are not discriminated against. Much of the FairML literature, which has provided useful categorisations of formal criteria (i.e. independence, sufficiency, and separation) and the respective (statistical) notions of individual and group level fairness (e.g. demographic parity, equalised opportunities, counterfactual fairness) are relevant here. And, indeed, a requirement to explain the choice of any statistical measures used during the development of a predictive model (e.g. classifier) is included. Again, there are useful tools and taxonomies that offer further guidance on these decisions. However, our pattern also urges project teams to reflect upon other patterns of discrimination, marginalisation, and minoritisation, which can exacerbate mental health issues, but which fall beyond protected characteristics that lie outside the scope of the Equality Act 2010 (e.g. housing, employment, social support). Doing so will typically require engagement of domain experts where such expertise does not exist within teams.</p>","location":"dmh-report/chapter-5/#argument-over-non-discrimination","tags":["fairness","explainability","argument-patterns"]},{"title":"Argument over equitable impact","text":"<p>Finally, this argument references positive duties that matter in the context of digital mental healthcare specifically. Key to this is the consideration of ethical values such as autonomy and self-determination, and the prioritarian weighting that was mentioned previously. However, there is also an aspect of our Sustainability principle that trickles into this argument. This is important in the context of digital mental healthcare because of associated risks that a) positive effects diminish over time (e.g. behavioural nudges or habit forming techniques that become ineffective over time)6 and b) negative impacts worsen and compound (e.g. prolonged use of social media worsening anxiety or depression)7. Studies have already criticised the evidence base of mental health apps and services8, especially for a lack of reliable longitudinal evidence, so drawing attention to sustainable impacts and setting up requirements for continuous monitoring is vital to maintain trust and also ensure that specific users are not locked in to services or technologies that degrade in quality or efficacy over time (e.g. apps that start by offering free services to leverage network effects only to force subscriptions at a later date).</p>","location":"dmh-report/chapter-5/#argument-over-equitable-impact","tags":["fairness","explainability","argument-patterns"]},{"title":"Explainability","text":"<p></p> <p>Much like fairness, explainability has become a popular and thriving area of research (e.g. so-called XAI). And, also like fairness, it is a normative goal that encompasses a range of significant concerns and salient ethical values.</p> <p>The most obvious conceptual distinction is between <code>interpretability</code> as a core component of <code>explainability</code>. Whereas the former is to often treated as the ability for developers or users to understand the inner workings of algorithms (or inability in the case of complex, non-linear techniques), the latter refers to an interpersonal ability to communicate knowledge in a manner that is accessible to those who may be asking questions about a system (e.g. patients asking for explanations from a clinician). Although statistical techniques help significantly in the case of the former, they are more limited in their ability to support the latter where a more diverse range of users are likely.</p>  <p>Why does this pattern matter?</p> <p>Explainable AI has received a lot of attention over the last several years.9 Computer scientists have developed new tools and methods to improve the interpretability of otherwise opaque algorithms, such as neural networks.10 Researchers in psychology and human-computer interaction have explored how different components of the user experience can help support more intentional interactions with intelligent software agents.11 And, regulators, auditors, and journalists have investigated how to make systems more transparent to support objectives related to accountability and informed decision-making.12</p> <p>Much of this attention arises from the recognition that data-driven technologies have the potential to automate decision-making to varying degrees and, therefore, affect key ethical values and principles such as autonomy, accountability, responsibility, and informed consent. On the one hand, decision support systems can offer recommendations to users but are not responsible for enacting a decision directly. And, on the other hand, you have fully automated-decision making systems, which once set up require no human involvement.</p> <p>This distinction is admittedly coarse grained, but it will suffice for our purposes because it helps identify two illustrative cases where explainability matters. In the former case, although a human user is responsible for the decision, their judgement may be influenced and biased by the decision support system, potentially in ways that are problematic (e.g. leading to differential treatment for certain groups of users). In the latter case, no human is involved, but because the automated systems cannot be held morally or legally accountable for their decisions, if something goes wrong, a human will need to be able to identify the reason why the problem occurred and perhaps communicate this to other affected stakeholders.</p> <p>In both of the above cases, extracting a valid and accurate explanation is necessary to enable post hoc forms of accountability or transparency. But prioritising \u2018explainability\u2019 from the start of a project also allows project teams to have better oversight of what their systems do and why, leading to more responsible forms of project governance. And, at the other end of the lifecycle, clear and accessible explanations can help ensure users and affected stakeholders are better informed and empowered to make autonomous decisions regarding their interactions with DMHTs. Therefore, having an argument pattern for \u2018explainability\u2019 helps capture many of the key considerations that were raised during our workshops.</p> <p>While the themes and values expressed in the following pattern are based primarily on the engagement with stakeholders, we have also drawn upon two other documents. First, we have drawn from prior regulatory guidance that we co-designed with the Information Commissioner\u2019s Office. This guide, titled 'Explaining Decisions Made with AI', details best practices for explainable AI in domain-general settings, and was also informed by stakeholder engagement. The regulatory ecosystem around explainability is less developer than fairness and equality, but as this report acknowledges there are still legislative and regulatory considerations that organisations need to consider, such as the wide-range of rights established by the General Data Protection Regulation and implemented in the UK's Data Protection Act 2018, such as the need to uphold individuals rights to be informed or to object to automated decisions.13</p> <p>Second, we have incorporated some elements of an existing pattern for interpretable machine learning14, which is motivated by a similar need for addressing a range of questions and concerns, such as the following:</p>  <p>\u201cwho needs to understand the system, what they need to understand, what types of interpretations are appropriate, and when do these interpretations need to be provided\u201d (Ward and Habli, 2020).</p>","location":"dmh-report/chapter-5/#explainability","tags":["fairness","explainability","argument-patterns"]},{"title":"Explainability Pattern","text":"<p></p> <p>Figure 5.2: A pattern for designing, developing, and deploying explainable digital mental health technologies ( click image to expand,  download here)</p> <p>In previous guidance16, we have distinguished between two sub-categories of explanations:</p> <ol> <li>Process-based explanations of AI systems are about demonstrating that you have followed good governance processes and best practices throughout your design and use.</li> <li>Outcome-based explanations of AI systems are about clarifying the results of a specific decision. They involve explaining the reasoning behind a particular algorithmically-generated outcome in plain, easily understandable, and everyday language.</li> </ol> <p>These categories are reflected in our pattern, where they form 'intermediate arguments' that help refine the goal claim and also serve as scaffolding for the main arguments.</p> <p>As with the fairness pattern, placeholders for <code>System Description</code> and <code>Context of Use</code> and <code>Stakeholder</code> are also included.</p> <p>The intermediate arguments are then broken into four higher-level property claims and their respective sub-claims, which we group according to the following core attributes of the Explainability principle as specified and operationalised in the context of digital mental healthcare:</p> <ul> <li>Argument over <code>transparency and accountability</code></li> <li>Argument over <code>responsible project governance</code></li> <li>Argument over <code>informed and autonomous decision-making</code></li> <li>Argument over <code>sustainable impact</code></li> </ul>","location":"dmh-report/chapter-5/#explainability-pattern","tags":["fairness","explainability","argument-patterns"]},{"title":"Argument over transparency and accountability","text":"<p>This argument addresses the processes and mechanisms that have been undertaken throughout the project lifecycle to establish sufficient forms of transparency and accountability. This includes documentation relevant to the identification of responsible project members, as well as choices made about data (e.g. why certain data types were included or excluded). Importantly, this argument also recommends the inclusion of a statement about sources of funding and conflicts of interest, which was an important matter for trustworthiness that arose during our engagement with participants with lived experience of DMHTs.</p>","location":"dmh-report/chapter-5/#argument-over-transparency-and-accountability","tags":["fairness","explainability","argument-patterns"]},{"title":"Argument over responsible project governance","text":"<p>This argument is more comprehensive than the others, and so is further split into three sub-arguments:</p> <ol> <li>Sub-argument over meaningful engagement: here, meaningful engagement can be seen to include participation in decisions about the formulation of the problem that a DMHT is expected to address, as well as issues of data usage\u2014both of which affect later stages of the project lifecycle and the final behaviour of the deployed system.</li> <li>Sub-argument over interpretability: sufficient levels of accuracy and the potential trade-off with interpretability can require high levels of technical and data literacy. Therefore, this argument focuses on the requisite information that is needed to support explainability (recall earlier distinction between <code>interpretability</code> and <code>explainability</code>).</li> <li>Sub-argument over accessible communication: the previous sub-argument feeds into this sub-argument, which focuses on how 'accessible' forms of communication will achieved, and the challenges of communicating probabilistic information. Ultimately, this sub-argument will depend on decisions reached and evidence obtained through consultation with intended users, as well as on the basis of knowledge about any time-constraints presented by the context of use (e.g. urgency in high-risk care environments).</li> </ol>","location":"dmh-report/chapter-5/#argument-over-responsible-project-governance","tags":["fairness","explainability","argument-patterns"]},{"title":"Argument over informed and autonomous decision-making","text":"<p>The core attribute motivating this argument is shared with our fairness pattern. Here, the argument emphasises the importance of explanations that refer to the observed behaviours or outcomes of the system. For instance, one of the claims is intended to ensure that explanations are \"sufficiently expressive\", without overwhelming the user with unnecessary or overly-complex information. This will depend on the intended user and context of use. However, to supplement this claim, emphasis is also placed on the ability for user to challenge outcomes, rather than just having them explained without an option to contest.</p>","location":"dmh-report/chapter-5/#argument-over-informed-and-autonomous-decision-making","tags":["fairness","explainability","argument-patterns"]},{"title":"Argument over sustainable impact","text":"<p>The final argument also follows the theme of the fairness pattern, but rather than addressing equitable impact, it focuses on sustainable impact15. This is important because explanations are sometimes used as a means to justify why a specific norm was transgressed (e.g. why you were late). However, over time, if the same explanation is provided without a change to the offending behaviour, the explanation loses its validity. A similar risk is present in the automated delivery of explanations by algorithmic systems. For example, if an AI chatbot continues to offer the same inaccurate and irrelevant explanations, it is likely to lose the trust of a user. Therefore, assessments about the impact of explainable AI need to account for longer-term dynamics to ensure that the relevant systems are sustainable over time.</p> <p>The inclusion of clinical safety and efficacy should not suggest that these goals are not significant in their own right. In fact, we would advocate for a separate assurance case (and corresponding pattern) on these goals specifically. Instead, reference is simply made to the need to ensure that some form of explanation is provided to stakeholders.</p>","location":"dmh-report/chapter-5/#argument-over-sustainable-impact","tags":["fairness","explainability","argument-patterns"]},{"title":"Evidential Considerations","text":"<p>Neither of the patterns above include prescriptions about specific evidential artefacts that could be used to ground the assurance case. There are two reasons for this intentional omission:</p> <ol> <li>Prescribing specific forms of evidence is too difficult outside of highly constrained contexts where there are clear details about a) the intended use context, b) the type of ML/AI technique being used, and c) the intended users and target audience.</li> <li>Developers and regulators should be free to determine the appropriate forms of evidence, based on developing best practices and standards, many of which do not exist at present.</li> </ol> <p>However, there are a couple of general remarks that can be made, as well as some suggestions for further resources.</p> <p>First, as we have argued elsewhere17, the generation, evaluation, and selection of evidence can be guided by the following considerations:</p> <ol> <li>Is the evidential artefact/claim relevant to the parent claim?</li> <li>Is the evidential artefact/claim (or set of artefacts/claims) sufficient to justify the parent claim?</li> <li>Is there sufficient probative value in the overall assurance case to justify the top-level normative goal?</li> </ol> <p>Outside of the regulatory considerations already mentioned above, there have been several developments in recent years to help organisations address these considerations. Some examples (among many) include:</p> <ul> <li>Model Cards for Model Reporting: templates for model documentation, which include ethical considerations alongside statistical information to support reuse.</li> <li>Responsible AI Licensing: licenses that help developers restrict the use of their AI technology in order to prevent irresponsible and harmful applications.</li> <li>Data Hazards: a set of labels that enable project members to make decisions about the risks of data-driven technologies using a shared vocabulary</li> <li>Assurance of Machine Learning in Autonomous Systems (AMLAS): a methodology for assuring the safety of ML systems, with systematic means for evaluating processes such as model testing or verification.</li> <li>Algorithmic Transparency Standard: a template for organisations to use when choosing to publish information about how they are using algorithmic systems to aid decision-making.</li> </ul> <p>Typically, these tools, methods, or templates exist to help organisations and project teams address a specific challenge (e.g. licensing, model documentation). The framework and methodology we have presented in this report is designed to work alongside these tools, but it also goes further by helping teams organise them according to a particular goal or objective (e.g. fairness). As such, our framework and methodology is broader in scope and offers a systematic means for choosing when to use specific tools throughout a project's lifecycle and how to bring the documented evidence together to create a trustworthy and justifiable assurance case.</p>   <ol> <li> <p>In a previous article we also explore several considerations about the evidence generation and selection process, including whether evidential artefacts are permissible, sufficient, and relevant. See Burr, C., &amp; Leslie, D. (2022). Ethical assurance: A practical approach to the responsible design, development, and deployment of data-driven technologies. AI and Ethics. https://doi.org/10.1007/s43681-022-00178-0 \u21a9</p> </li> <li> <p>We are not suggesting that this is the sole correct way of interpreting the public sector equality duty. Others may view the associated duties as entirely positive if they are viewed from a human rights lens that treats actively protecting people from risks of harm that are known about, or should have been known about, as a positive duty. The status of this duty will likely depend on which party it falls on, and how they are expected to discharge the duty.\u00a0\u21a9</p> </li> <li> <p>For instance, a patient experiencing depression may be fully informed by their psychiatrist about their mental health and the options available to them in terms of recovery, but nevertheless, autonomously decide to forego any treatment because their condition may be an important part of their self-identity. This acknowledgment is part of the recovery approach, which views recovery as \u201ca deeply personal, unique process of changing one\u2019s attitudes, values, feelings, goals, skills, and/or roles. It is a way of living a satisfying, hopeful, and contributing life even with limitations caused by illness. Recovery involves the development of new meaning and purpose in one\u2019s life as one grows beyond the catastrophic effects of mental illness.\u201d W. A. Anthony, \u201cRecovery from mental illness: The guiding vision of the mental health service system in the 1990s,\u201d Psychosoc. Rehabil. J., vol. 16, no. 4, p. 527, 1993, doi: 10.1037/h0095655.\u00a0\u21a9</p> </li> <li> <p>This is further problematised in the context of ML and AI, where novel forms of discrimination, perhaps as a result of so-called \"affinity profiling\". See Wachter, S. (2021). Affinity Profiling and Discrimination by Association in Online Behavioural Advertising. Berkeley Technology Law Journal, 35(2).\u00a0\u21a9</p> </li> <li> <p>See our course on responsible research and innovation for more details about social, statistical, and cognitive biases: https://alan-turing-institute.github.io/turing-commons/rri/ \u21a9</p> </li> <li> <p>See Maier et al. (2022) No evidence for nudging after adjusting for publication bias. PNAS. https://doi.org/10.1073/pnas.2200300119 \u21a9</p> </li> <li> <p>For a review published prior to the onset of the COVID-19 pandemic see Keles (2019) A systematic review: the influence of social media on depression, anxiety and psychological distress in adolescents. Adolescence and Youth. https://doi.org/10.1080/02673843.2019.1590851. For a range of studies that focus on the impacts of COVID-19 on mental health, including several that explore social media, see the COVID-MINDS repository.\u00a0\u21a9</p> </li> <li> <p>For example, see Torous et al. (2018). Clinical review of user engagement with mental health smartphone apps: Evidence, theory and improvements. Evidence Based Mental Health, 21(3), 116\u2013119. https://doi.org/10.1136/eb-2018-102891; and the consensus statement that followed: Torous et al. (2019). Towards a consensus around standards for smartphone apps and digital mental healthcare: Towards a consensus around standards for smartphone apps and digital mental healthcare. World Psychiatry, 18(1), 97\u201398. https://doi.org/10.1002/wps.20592 \u21a9</p> </li> <li> <p>See the following notable publications: Phillips et al. (2021). Four Principles of Explainable Artificial Intelligence. National Institute of Standards and Technology. https://doi.org/10.6028/NIST.IR.8312;  Miller, T. (2019). Explanation in artificial intelligence: Insights from the social sciences. Artificial Intelligence, 267, 1\u201338. https://doi.org/10.1016/j.artint.2018.07.007; Diakopoulos, N. (2015).\u00a0\u21a9</p> </li> <li> <p>For a recent review of methods, see Linardatos, P., Papastefanopoulos, V., &amp; Kotsiantis, S. (2020). Explainable AI: A Review of Machine Learning Interpretability Methods. Entropy, 23(1), 18. https://doi.org/10.3390/e23010018 \u21a9</p> </li> <li> <p>Ferreira, J. J., &amp; Monteiro, M. S. (2020). What Are People Doing About XAI User Experience? A Survey on AI Explainability Research and Practice. In A. Marcus &amp; E. Rosenzweig (Eds.), Design, User Experience, and Usability. Design for Contemporary Interactive Environments (Vol. 12201, pp. 56\u201373). Springer International Publishing. https://doi.org/10.1007/978-3-030-49760-6_4 \u21a9</p> </li> <li> <p>Information Commisioner's Office &amp; Alan Turing Institute. (2020). Explaining decisions made with AI. https://ico.org.uk/media/for-organisations/guide-to-data-protection/key-data-protection-themes/explaining-decisions-made-with-artificial-intelligence-1-0.pdf; Algorithmic Accountability: Journalistic investigation of computational power structures. Digital Journalism, 3(3), 398\u2013415. https://doi.org/10.1080/21670811.2014.976411.\u00a0\u21a9</p> </li> <li> <p>Information Commisioner's Office &amp; Alan Turing Institute. (2020). Explaining decisions made with AI. https://ico.org.uk/media/for-organisations/guide-to-data-protection/key-data-protection-themes/explaining-decisions-made-with-artificial-intelligence-1-0.pdf \u21a9</p> </li> <li> <p>Ward, F. R., &amp; Habli, I. (2020). An Assurance Case Pattern for the Interpretability of Machine Learning in Safety-Critical Systems. In A. Casimiro, F. Ortmeier, E. Schoitsch, F. Bitsch, &amp; P. Ferreira (Eds.), Computer Safety, Reliability, and Security. SAFECOMP 2020 Workshops (Vol. 12235, pp. 395\u2013407). Springer International Publishing. https://doi.org/10.1007/978-3-030-55583-2_30 \u21a9</p> </li> <li> <p>Not to be confused with the related SAFE-D principle, 'Sustainability'.\u00a0\u21a9</p> </li> <li> <p>ICO and Alan Turing Institute. Explaining decisions made with AI. Technical Report, Information Comissioners Office, May 2020. URL: https://ico.org.uk/media/for-organisations/guide-to-data-protection/key-data-protection-themes/explaining-decisions-made-with-artificial-intelligence-1-0.pdf (visited on 2020-10-26).\u00a0\u21a9</p> </li> <li> <p>Christopher Burr and David Leslie. Ethical assurance: a practical approach to the responsible design, development, and deployment of data-driven technologies. AI and Ethics, June 2022. URL: https://link.springer.com/10.1007/s43681-022-00178-0 (visited on 2022-08-01), doi:10.1007/s43681-022-00178-0.\u00a0\u21a9</p> </li> </ol>","location":"dmh-report/chapter-5/#evidential-considerations","tags":["fairness","explainability","argument-patterns"]},{"title":"Conclusion","text":"","location":"dmh-report/conclusion/"},{"title":"Co-Creating a Culture of Trust","text":"<p></p>  <p>Uncertainty breeds distrust.</p>  <p>When the consequences of potential actions or interventions are hard to identify or evaluate, inaction or inertia can follow. And, such uncertainty and deliberative inertia is only exacerbated in the context of mental health (e.g. challenges that are faced by those with depression or anxiety disorders, such as catastrophising, when attempting to evaluate actions).</p> <p>As we have seen over the course of this report, vague privacy policies, poorly-specified objectives, pervasive and invasive data extraction, and dubious claims about user safety and clinical efficiency of services, are all sources of uncertainty.</p> <p>This report has laid out a methodological proposal for how we can begin to address the current culture of distrust that casts a shadow over the digital mental healthcare landscape. While we believe that the methodology and recommendations will support this goal, and have presented tentative evidence to justify this belief, the development of a trustworthy ecosystem of digital mental healthcare requires a more collaborative effort.</p> <p>Therefore, in addition to our methodological proposal of trustworthy assurance and the initial argument patterns, we have also made a series of supporting recommendations (summarised in the Executive Summary). But what comes next?</p>","location":"dmh-report/conclusion/#co-creating-a-culture-of-trust"},{"title":"Next Steps","text":"<p>In addition to the individual recommendations presented in the previous chapters, this report can also be viewed as a general recommendation itself\u2014one that calls for the more widespread adoption of the trustworthy assurance methodology. We can even formulate this recommendation as an argument in a pseudo assurance case:</p> <p></p> <p>Ultimately, the veracity of the goal claim depends on how and whether the methodology is adopted. This report is not a user guide, however, so more work needs to be done to ensure the adoption by organisations is made as straightforward as possible (e.g. alignment with existing regulation and current practices, including complementary quality assurance procedures).</p> <p>A key next step will be to develop user guidance that can help with this objective. This has already commenced, and we currently have a) a full-length article that goes into further detail about the methodology1, in relation to domain-general ethical principles, and b) a prototype platform that can enable the production of assurance cases2. However, these proposals have hitherto not been connected directly with the more formal work undertaken by those working in safety assurance. This is primarily because we endeavoured to make the trustworthy assurance methodology simple and accessible for the purpose of our stakeholder engagement. But, the adoption of the methodology by developers and engineers would likely benefit from closer integration with standardisation efforts, such as the Goal Structuring Notation (GSN) and the GSN Standard Working Group.</p> <p>These efforts will need to remain receptive to ongoing developments in this domain, whether technological (e.g. development of new computational techniques or devices), legislative (e.g. reforms to UK legislation such as the Draft Mental Health Bill, 2022), regulatory (e.g. report on the Public Sector Equality Duty by the Equality and Human Rights Commission, the formation of the Multi Agency Advice Service; proposal of the Digital Technology Assessment Criteria (DTAC), or societal (e.g. changing perceptions or attitudes of users towards mental health and well-being services, including data-driven technologies). </p> <p>We hope that the proposal and recommendations set out in this report offers some clarity, structure, and positive direction to help navigate this complex (and multitudinous) space. And, more importantly, we hope that it can (indirectly) improve mental health services for those who need them.</p> <p></p>   <ol> <li> <p>Burr, C., and Leslie, D. (2022). Ethical assurance: a practical approach to the responsible design, development, and deployment of data-driven technologies. AI Ethics. https://doi.org/10.1007/s43681-022-00178-0 \u21a9</p> </li> <li> <p>Details of the platform and the code is available through the following GitHub repository, and we welcome contributions to its ongoing development from any open-source developers: https://github.com/alan-turing-institute/AssurancePlatform \u21a9</p> </li> </ol>","location":"dmh-report/conclusion/#next-steps"},{"title":"Executive Summary","text":"<p></p> <p>There is a culture of distrust surrounding the development and use of digital mental health technologies (DMHTs).</p> <p>As many organisations continue to grapple with the long-term impacts on mental health and well-being from the COVID-19 pandemic, a growing number are turning to digital technologies to increase their capacity and try to meet the growing need for mental health services. Prior to the pandemic, we had already called for greater attention to the ethical challenges of using digital technologies in the domain of psychiatry or mental healthcare3. Since then, the urgency for meeting this call has only grown.</p> <p>In this report, we argue that clearer assurance for how ethical principles have been considered and implemented in the design, development, and deployment of DMHTs is necessary to help build a more trustworthy and responsible ecosystem. To address this need, we set out a positive proposal for a framework and methodology we call 'Trustworthy Assurance'.</p> <p>To support the development and evaluation of Trustworthy Assurance, we conducted a series of participatory stakeholder engagement events with students, University administrators, regulators and policy-makers, developers, researchers, and users of DMHTs. Our objectives were</p> <ul> <li>to identify and explore how stakeholders understood and interpreted relevant ethical objectives for DMHTs,</li> <li>to evaluate and co-design the trustworthy assurance framework and methodology, and</li> <li>solicit feedback on the possible reasons for distrust in digital mental healthcare.</li> </ul> <p>Based on these objectives, the following 'key findings' and 'recommendations' are presented.</p>  <p>Key Findings</p> <ol> <li>The current landscape of digital mental healthcare is characterised by significant uncertainty, a lack of transparency or accountability, and a rising demand that outpaces trusted services and resources. This contributes to a culture of distrust, which may prevent vulnerable users from accessing support.</li> <li> <p>Concerns raised by stakeholders suggest that there are a wide range of challenges to be addressed, which may broadly be grouped into concerns surrounding a dearth of trustworthy innovation and concerns surrounding a lack of transparent communication between groups:</p> <ol> <li>For developers, key concerns focused on <ol> <li>the lack of clear guidance and structure through which to present evidence of trustworthy innovation,</li> <li>the lack of integration of ethics within existing workflows, and </li> <li>the challenges posed by what is viewed as burdensome regulation.</li> </ol> </li> <li>For policymakers, key concerns focused on <ol> <li>the lack of clarity surrounding standards for medical devices versus services for \"well-being\" that are widely available on digital platforms (e.g. app stores), </li> <li>the lack of integration or harmonisation between existing examples of legislation and standards in this space.</li> </ol> </li> <li>For those with lived experiences of using these tools, key concerns focused on <ol> <li>the lack of clear and meaningful consent procedures and the insufficiency of data privacy policies, </li> <li>the perceived erosion of in-person care by digital technologies and services, </li> <li>a perceived lack of diversity and representation in development teams, and </li> <li>the varying quality and accessibility of services across society (e.g. the digital divide).</li> </ol> </li> </ol> </li> <li> <p>Trustworthy Assurance is a framework and methodology that can support the design, development, and deployment of data-driven technologies and also create a more responsible and trustworthy ecosystem of digital mental healthcare.</p> </li> </ol>   <p>Recommendations</p> <ol> <li>Organisations that are involved in the design, development, and deployment of DMHTs should adopt and use trustworthy assurance methodology to demonstrate and justify how they have embedded core ethical principles into their systems. In doing so, the methodology can also help provide assurance for how key legislative or regulatory duties and obligations have been met.</li> <li>Standards can be co-developed within and among organisations by sharing best practices related to trustworthy assurance. This can help ease the burden associated with relevant responsibilities (e.g. compliance, deliberation).</li> <li>Common capacities should be developed across the digital mental healthcare landscape, such as initiatives aimed at improving data and digital literacy, in order to support and foster trustworthy and responsible innovation through shared best practices and standards.</li> <li>Research should be undertaken to identify how organisations and product managers could ease the time burden on developers through embedding and integrating the trustworthy assurance methodology into key stages of the project lifecycle, rather than the methodology being treated as a post hoc compliance exercise.</li> <li>Organisations involved in the design, development, or deployment of DMHTs should identify opportunities and processes to support the transformative and inclusive engagement and participation of affected users within the project lifecycle.</li> </ol>  <p>In each subsequent chapter, these key findings of our project are further contextualised and refined with reference to the specific topics under discussion.</p>","location":"dmh-report/executive-summary/","tags":["executive summary","findings","recommendations","digital-mental-healthcare"]},{"title":"Report Overview","text":"<p>Our report is structured as follows:</p> <ul> <li>Chapter 1 (Introduction) establishes the background context and conceptual foundations for the report, while also outlining the many challenges that exist for researchers, developers, and policy-makers/regulators working in the domain of digital mental healthcare.</li> <li>Chapter 2 (Presenting Trustworthy Assurance) introduces the framework and methodology of 'Trustworthy Assurance'. The framework includes a model of a typical project lifecycle involving a data-driven technology (e.g. health and well-being app), and a discussion of several ethical principles, known as the SAFE-D principles. The framework serves as a guide to our methodology for developing an assurance case that promotes trustworthy goals associated with DMHTs. Finally, this chapter also includes an important discussion about 'argument patterns', which supports the material presented in Chapter 5.</li> <li>Chapter 3 (Applying Trustworthy Assurance) presents findings from a research sub-project conducted with students and administrators from UK Universities. These engagement events explored the application of trustworthy assurance to the procurement of DMHTs for use in the higher education (HE) sector, as well as general attitudes and perceptions towards the use of data-driven technologies in higher education. A series of recommendations accompany our thematic analysis.</li> <li>Chapter 4 (Co-Designing Trustworthy Assurance) broadens the scope from the previous chapter to present research findings from a series of stakeholder engagement events carried out with regulators and policy-makers, developers, researchers, and users with lived experience of DMHTs. As with the previous chapter, a set of recommendations accompanies our thematic analysis.</li> <li>Chapter 5 (Developing Trustworthy Assurance) introduces, motivates, and explains two argument patterns that are intended to help project teams meet objectives for fair and explainable DMHTs. This chapter also connects the argument patterns to existing and relevant legislation and regulation (e.g. Equality Act 2010).</li> </ul>  <p>Examples</p> <p>If you are new to the topics covered in this report, you can also find a set of illustrative examples of DMHTs available on this page.</p>","location":"dmh-report/executive-summary/#report-overview","tags":["executive summary","findings","recommendations","digital-mental-healthcare"]},{"title":"About the Report","text":"<p>The following summarises what this report is and what it is not:</p> <p>\u2705 An introduction to 'Trustworthy Assurance'\u2014a framework and methodology for enabling a more trustworthy ecosystem of digital mental healthcare through the responsible and ethical design, development, and deployment of digital technologies.</p> <p>\u274c A comprehensive user guide for 'Trustworthy Assurance' or argument-based assurance\u2014though links and further resources are provided.1</p> <p>\u2705 A summary of findings from research conducted on the application of trustworthy assurance to the procurement of DMHTs for use in the higher education (HE) sector.</p> <p>\u274c Findings from a sociological study or series of generalisable results from scientific experiments.</p> <p>\u2705 A summary of findings from a series of more general stakeholder engagements, exploring the ethics of digital mental healthcare and attitudes towards trustworthy and untrustworthy technologies.</p> <p>\u274c A report with a strong international or multi-national focus. While we make reference to non-UK developments in this domain, our primary focus is on the UK. However, the methodology we present and many of the findings we discuss have value beyond the UK.</p> <p>\u2705 An explanation and discussion of two argument patterns exploring the goals of fairness and explainability in the design, development, and deployment of DMHTs.</p> <p>\u274c A critical examination of argument-based assurance.2</p> <p>\u2705 A series of recommendations, targeted at different stakeholders, for how to enable a more responsible and trustworthy ecosystem of digital mental healthcare.</p> <p>\u274c A review of the current legislative or regulatory publications that are relevant to digital mental healthcare.</p>","location":"dmh-report/executive-summary/#about-the-report","tags":["executive summary","findings","recommendations","digital-mental-healthcare"]},{"title":"Who is this report for?","text":"<p>This report is primarily targeted at the following groups:</p>   <p></p> <p>Policy-makers and Regulators</p> <p>Policy-makers and regulators will find the recommendations and guidance we set out of specific interest, and will find value in the methodology that we set out in Chapter 2 because it is framed in procedural terms, and with links to process-based forms of governance. We also link our two argument patterns, which are presented in Chapter 5, to specific legislative and regulatory developments in healthcare.</p>  <p></p> <p>Senior Decision-Makers</p> <p>Senior Decision-Makers, like policy-makers and regulators, will likely benefit from our methodology of Trustworthy Assurance. Specifically, from exploring its procedural underpinnings that are discussed in the section on a typical ML or AI lifecycle (see Chapter 2).</p>  <p></p> <p>Developers and Product Managers</p> <p>Although our framework and methodology are not set out using formal syntax and schemas for argument-based assurance, Trustworthy Assurance is, nevertheless, primarily aimed at developers and product managers. For instance, one of its key values is as a reflective and deliberative aid for demonstrating how ethical principles and decisions have been undertaken and establish through a project's lifecycle, with easy means for justifying the relevant claims by linking them to evidence. Therefore, developers and product managers are a key stakeholder group that we have targeted in this report and research project.</p>  <p></p> <p>Researchers</p> <p>Researchers may find less practical value in our framework and methodology than the above groups. However, our report highlights and emphasises significant knowledge gaps and research opportunities for improving our collective understanding about the individual and social impacts of DMHTs. Therefore, our report can also be seen as a call for further research into specific areas, including the evaluation and validation of the Trustworthy Assurance framework and methodology.</p>   <p>Users of DMHTs may also find value in the report, but it has not been produced with members of the public as primary target audience. In general, the responsibility for utilising the methodology and implementing and acting upon the recommendations is for the groups above; they are not the responsibility of the user!</p>   <ol> <li> <p>Work is underway to develop an user guide, and this will be added to the online version of our report when ready. The guide will also include instructions on how to use our tool for producing assurance cases.\u00a0\u21a9</p> </li> <li> <p>The following documents provide a more critical examination for those interested: (Sujan and Habli, 2021); (Burr and Leslie, 2022).\u00a0\u21a9</p> </li> <li> <p>Christopher Burr, J. Morley, M. Taddeo, and L. Floridi. Digital Psychiatry: Risks and Opportunities for Public Health and Wellbeing. IEEE Transactions on Technology and Society, 1(1):21\u201333, March 2020. doi:10.1109/TTS.2020.2977059.\u00a0\u21a9</p> </li> </ol>","location":"dmh-report/executive-summary/#who-is-this-report-for","tags":["executive summary","findings","recommendations","digital-mental-healthcare"]},{"title":"Foreword","text":"<p></p> <p>In a society where mental health need far outstrips the supply of services, it is perhaps inevitable that policy-makers and regulators, healthcare professionals and service providers, and employers and educators are looking for scalable solutions. At the same time as digital and data-driven technologies have become mainstream in so many areas of our lives, from banking to dating, people seeking support are also turning to the internet and app stores to find ways to get help. And commercial organisations haven\u2019t been slow to spot this trend and provide solutions in exchange for your money or your data. For example, Calm and Headspace Health\u2014both leading providers of mindfulness apps\u2014were valued at $2bn and $3bn, and made an estimated $200m and $150m respectively in 2020.1</p> <p>In a fast moving and increasingly crowded marketplace, there is an urgent need to help those purchasing or procuring digital mental health services, for themselves or on behalf of others, to be confident that the products they are buying are not only safe and clinically effective, but also promote key ethical values, such as data protection, health equity, and sustainability.  In other words, are trustworthy.</p> <p>At Mind, we are actively exploring how to use digital technology to increase the reach and impact of all aspects of our work, from fundraising to campaigning and service delivery. While we recognise that digital mental health services can increase choice and reach, they are not a panacea and cannot replace the localised, personal touch that is core to our service.  Our survey in 2021 of almost 2000 people revealed that more than one in three (35 per cent) found support from NHS mental health services, given over the phone or online, difficult to use; almost two in three (63 per cent) said they would have preferred to have been given face-to-face support; and one in four (23 per cent) say their mental health actually got worse as a result of using this support. However, two in three (69 per cent) appreciated not having to travel; almost one in two (47 per cent) were grateful for greater flexibility over appointment times; and two in five (40 per cent) said that waiting times were shorter. It is within this context that we are taking a test-and-learn approach to understand when and how digital mental health technologies can be used to augment and complement in person support.</p> <p>As such, we welcome the work The Alan Turing Institute are doing to provide a framework to determine which digital mental health technologies are trustworthy.</p> <p>Existing regulatory frameworks such as DTAC, NICE, MHRA, CQC exist, but do not provide sufficient coverage. Kooth, Togetherall and Silvercloud, for example, are all reputable, large companies delivering services to the NHS that fall outside the scope for CQC registration because the activities of those organisations are not deemed a \u2018regulated activity\u2019. Many healthcare apps also fall outside of the current definition of medical devices, and as such are outside of the scope of MHRA. And although NICE have recently updated their evidence standards framework for digital health technologies, it is not clear how evaluations will keep pace with the regularity of app updates.</p> <p>This report raises questions that continue to concern providers of mental health support:</p> <ul> <li>Can digital healthcare provide a route to support people who are not already in contact or not well served by formal healthcare services?</li> <li>How can we ensure access to those who are digitally excluded or in data poverty, who may also be socially excluded and at increased risk of mental health challenges?</li> <li>Can structured scrutiny, as described/proposed in this report, encourage developers to improve the transparency and trustworthiness of their products?</li> <li>Are users (particularly at a time of increased vulnerability) adequately informed and protected by the practice of \u2018opting in\u2019 to terms and conditions which are often pages long and written for lawyers not end users?</li> <li>Can digital mental healthcare replace the \u2018human connection\u2019 or is it safest and most effective when used to supplement and complement in person support?</li> </ul> <p>The research and trustworthy assurance framework proposed in this report by The Alan Turing Institute\u2019s Public Policy Programme provides a key stepping stone towards addressing these questions, as we strive to meet the growing need for mental health support in a way that is both responsible and trustworthy.</p> <p>Dr Cath Biddle</p> <p>Head of Digital, Mind</p>   <ol> <li> <p>https://www.businessofapps.com/data/ \u21a9</p> </li> </ol>","location":"dmh-report/foreword/"}]}