---
title: "Appendix: Project Methodology"
authors:
  - Christopher Burr
  - Rosamund Powell
tags:
  - methodology
  - workshops
---

# Appendix: Project Methodology

This annexe offers additional information about the structure and methodology for our workshops. First, we begin by providing an overview of the three case studies that were used across the workshops. Next, we detail the general methodology for the workshopsm, which is split into three sections:

1. University administrators and students
2. Regulators, developers, and researchers 
3. Users of digital mental health technology

## Case Study Information 

No one ever said ethics was easy. Moral deliberation requires deep and wide-ranging consideration of issues and challenges such as conflicting values, resource limitations, and the needs and interests of diverse people and groups. 

One way to facilitate ethical deliberation, therefore, is through the use of case studies, which hold certain details fixed to support reflection. 

Our workshops were supported by four case studies, all of which were related to hypothetical projects involving the development or use of a digital mental health technology. Although hypothetical, all the case studies were based on real-world examples, and were selected because of their potential for supporting ethical reflection and deliberation.

- **Risk assessment and peer-to-peer support:** a platform for users to discuss their mental health with others in an anonymous environment, but where a machine-learning algorithm is identifying markers of risk and alerting trained professionals if positive instances are detected.

![](https://raw.githubusercontent.com/alan-turing-institute/ethics-and-rri-resources/main/images/illustrations/peer-support.png)

- **App limits:** an automated service on a smartphone that learns to detect signs of problematic usage of specific apps (e.g. gambling, social media) and prevents users from accessing these apps for a specified period.

![](https://raw.githubusercontent.com/alan-turing-institute/ethics-and-rri-resources/main/images/illustrations/limit-reached.png)

- **Clinical decision support system:** an AI system that supports a psychiatrist with the assessment and diagnosis of a patient by analysing the patient's speech and making recommendations for follow-up questions or possible follow-up actions.

![](https://raw.githubusercontent.com/alan-turing-institute/ethics-and-rri-resources/main/images/illustrations/nlp-decision-support.png)

- **Virtual reality for therapeutic support:** a virtual reality (VR) system that immerses patients in a virtual environment that is designed to expose them to challenging situations in a way that is monitored and controlled (e.g. social encounters for individuals suffering from social anxiety).

![](https://raw.githubusercontent.com/alan-turing-institute/ethics-and-rri-resources/main/images/illustrations/vr-therapy.png)

For each case study, the following information was presented:

- Overview: a summary of the case study with relevant information about the system and use context
- Key consideration: a question that was designed to elicit reflection on the ethically salient features of the case study
- Deliberative prompts: additional questions to help structure group discussion
- Datasheet: a table of information about the data available to the project team (e.g. input data, training data) and the techniques used in the project (e.g. natural language processing, artificial neural network)
- Affected users, groups, and stakeholders: a list of those who are likely to be impacted by the system or who may impact the system's use and adoption.

For all the case studies, this information was presented as a *starting point* but the participants were given sufficient flexibility to build upon the case studies where necessary. For instance, if the participant's thought that a property of the use context was relevant to their deliberation, but it was not explicitly stated in the document, they were encouraged to include it their discussion. 

> The full case studies can be accessed here: *insert GH repo link**

With this preliminary information specified, we now turn to our analysis of the workshops. 

## Methodology for Section 2

### Semi-structured interviews with university administrators 

#### Recruitment and participant details 

Interviews with administrators across 10 UK universities were conducted between January and March 2022, each lasting one hour and facilitated by two project members from the Turing. In all instances, interviews were conducted remotely via Zoom. Participants were selected from the top 20 UK universities according to all metrics, based upon the Times Higher Education Survey 2021.[^times]

[^times]: https://www.timeshighereducation.com/world-university-rankings/2021/world-ranking#!/page/0/length/25/locations/GBR/sort_by/rank/sort_order/asc/cols/stats Desk research was used to determine different methods for filtering universities. For example, based upon the best and worst student satisfaction. The top 20 according to Times HE Survey were selected based upon the broad range of digital offerings available at these institutions.  

Relevant representatives were identified and invited to interview. Individuals invited to interview worked within Student Services departments with the majority serving as Director of Student Welfare or Wellbeing. Some instead served as Head of Disability, Deputy Director of Wellbeing or Head of Student Services. 

#### Interview design 

In advance of interviews, all participants were sent a consent form, a list of questions and briefing document on the digital mental healthcare landscape. At the start of each interview, one of the facilitators began by introducing the project and the project members on the call. Following this, verbal consent was requested, and participants were given the opportunity to ask questions on the informed consent form that had been pre-circulated. The informed consent statement explained that their answers would be fully anonymised, in order to allow the participants to feel comfortable expressing their opinions and beliefs about potentially sensitive topics.  

Once consent was obtained, the audio recording began. A semi-structured format was selected for several reasons:  

- It was suitable for an exploratory stage interview 
- It encourages exploration of tangential issues that participants feel are relevant 
- It is more likely to promote a relaxed interview and hones feedback. 

The interviews were split into three main sections. Section 1 focused on attitudes to the advantages and disadvantages of digital mental health technologies and current procurement practices at UK universities. Section 2 focused on how current procurement processes align with duty of care. Section 3 focused on collecting feedback on the ethical assurance methodology. 

At the end of interviews, the audio recording was stopped, and the participant was asked if they had any further questions for the interviewers. The participants were thanked for their time and told they would receive a copy of this report.  

#### Analysis 

To facilitate qualitative analysis, audio recordings were transcribed by members of the Turing project team. Two project team members then analysed these transcriptions to identify salient themes, grouped into two sections: 

1. Contextual challenges to the ethical deployment of digital mental healthcare 
2. Administrator feedback on trustworthy assurance 

Once themes were identified, key conclusions were summarised and quote from interviews extracted. This preliminary analysis was then set aside so that student workshops could be completed. 

### Workshops with university students  

#### Recruitment and participant details 

Workshops with students from UK universities were conducted between February and March 2022. Two six-hour workshops were completed and facilitated by two project members from the Turing. In all instances, workshops were conducted remotely via Zoom. Participants were selected from across all UK universities. 

An open call for applications was published by the Turing for any students currently enrolled in an undergraduate or postgraduate course. In order to apply, students completed an application form asking a series of optional EDI questions alongside the below three project questions: 

1. Why are you interested in participating in this workshop? 
2. How do you understand the aims of this research in your own words? 
3. Do you have any prior understanding about the ethics of digital mental healthcare? If so, please provide details. 

Participants were selected from a total of 45 applications and a total of 25 students joined the final workshop sessions.  

#### Workshop design 

In advance of interviews, all participants were sent a consent form and briefing document on the digital mental healthcare landscape. At the start of each workshop, one of the facilitators began by introducing the project and the project members on the call. Following this, verbal consent was requested, and participants were given the opportunity to ask questions on the informed consent form that had been pre-circulated. The informed consent statement explained that their answers would be fully anonymised, in order to allow the participants to feel comfortable expressing their opinions and beliefs about potentially sensitive topics.  

Once consent was obtained, the audio recording began.  

The workshops were split into two sections. Section 1 focused on attitudes to the advantages and disadvantages of digital mental health technologies and on identifying which values and principles mattered to students. Section 2 then saw students evaluate two illustrative case studies designed by the Turing team in order to identify possible ethical issues which might arise if they were deployed in a university context.  

During the workshop sessions, participants were also asked to complete an online survey seeking individual feedback on the ethics of digital mental healthcare in a university context. 

At the end of workshops, the audio recording was stopped, and the participants were asked if they had any further questions for the facilitators. The participants were thanked for their time and told they would receive a copy of this report.  

#### Analysis 

To facilitate qualitative analysis, audio recordings were saved and notes were taken by members of the Turing team in order to identify key themes and quotations. In addition, the completed surveys were analysed. Finally, the notes taken online during the workshop activities were analysed as participants had been encouraged to take structured notes on the case studies during the workshop sessions.  

Each of these elements were taken into account in identifying key themes. Once themes were identified, key conclusions were summarised and quotes from workshops extracted. This preliminary analysis was then set aside, to be combined with pre-existing analysis form administrator interviews.  

### Final Analysis 

For students and administrators, key themes which fell into the category of “contextual challenges to the deployment of digital mental health technologies” were compared in order to identify cross-cutting themes. During analysis, researchers were careful to maintain differentiation between administrator and student perspectives such that agreements and disagreements could be identified. For all of the six themes identified, student and administrator feedback was relevant and so all themes were cross-cutting. However, in many cases, students and administrators contributed to the development of these themes in contrasting ways.  

In addition to contextual challenges, the final report identifies a series of methodological challenges. These are largely drawn from administrator interviews where more time was dedicated to the evaluation of trustworthy assurance. Nevertheless, where relevant, student perspectives are used to supplement this analysis.

## Methodology for Section 2
 
#### Workshops with regulators, policy-makers, developers, and researchers

We expected the participants of the first workshop to have a higher level of knowledge and understanding regarding digital mental healthcare than the user group. Therefore, we took the decision to provide fewer explanatory material and prompts in the first workshop in an attempt to elicit feedback that was not steered by our own framework (i.e. the SAFE-D principles).

[...]

#### Workshops with users of digital mental health technology

Whereas the workshops with regulators, policy-makers, developers, and reseacrhers required participants to directly evaluate the trustworthy assurance methodology, the workshops with users focused on the moral attitudes of the participants and only indirectly explored trustworthy assurance. 

This choice was influenced a) by the feedback from the students gathered during the sub-project (see section 2), and b) preliminary discussions with our workshop facilitators—McPin—about the accessibility of the material within the time constraints.

[...]

